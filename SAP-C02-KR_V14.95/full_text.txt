QUESTION NO: 1 AWS 계정이 여러 개인 회사에서 AWS Organizations를 사용하고 있습니다. 회사의 AWS 계정은 VPC, Amazon EC2 인스턴스 및 컨테이너를 호스팅합니다. 회사의 규정 준수 팀은 회사가 배포한 각 VPC에 보안 도구를 배포했습니다. 보안 도구는 EC2 인스턴스에서 실행되며 규정 준수 팀 전용 AWS 계정으로 정보를 보냅니다. 회사는 "costCenter" 키와 값 또는 "compliance"를 사용하여 모든 규정 준수 관련 리소스에 태그를 지정했습니다. 회사는 규정 준수 팀의 AWS 계정에 비용을 청구할 수 있도록 EC2 인스턴스에서 실행되는 보안 도구의 비용을 식별하려고 합니다. 비용 계산은 가능한 한 정확해야 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까? A. 조직의 마스터 계정에서 costCenter 사용자 정의 태그를 활성화합니다. 월별 AWS 비용 및 사용 보고서를 구성하여 마스터 계정의 Amazon S3 버킷에 저장합니다. 보고서의 태그 분류를 사용하여 costCenter 태그가 지정된 리소스에 대한 총 비용을 얻으십시오. B. 조직의 회원 계정에서 costCenter 사용자 정의 태그를 활성화합니다. 월별 AWS 비용 및 사용 보고서를 구성하여 마스터 계정의 Amazon S3 버킷에 저장합니다. 월별 AWS Lambda 함수를 예약하여 보고서를 검색하고 costCenter 태그가 지정된 리소스의 총 비용을 계산합니다. C. 조직의 구성원 계정에서 costCenter 사용자 정의 태그를 활성화합니다. 마스터 계정에서 월별 AWS 비용 및 사용 보고서를 예약합니다. 보고서의 태그 분석을 사용하여 costCenter 태그가 지정된 리소스의 총 비용을 계산합니다. D. AWS Trusted Advisor의 조직 보기에서 사용자 지정 보고서를 생성합니다. 규정 준수 팀의 AWS 계정에서 costCenter 태그가 지정된 리소스에 대한 월별 청구 요약을 생성하도록 보고서를 구성합니다. Answer: A Explanation: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/custom-tags.html https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/configurecostallocreport.htmlSQSSQSSQSSQSSQS
QUESTION NO: 2 SaaS(Software as a Service) 기반 회사는 솔루션의 A3 부분을 고객에게 사례 관리 솔루션을 제공합니다. 이 회사는 독립형 SMTP(Simple Mail Transfer Protocol) 서버를 사용하여 애플리케이션에서 이메일 메시지를 보냅니다. 애플리케이션은 또한 고객에게 이메일 메시지를 보내기 전에 고객 데이터를 채우는 확인 이메일 메시지에 대한 이메일 템플릿을 저장합니다. 회사는 이 메시징 기능을 AWS 클라우드로 마이그레이션할 계획이며 운영 오버헤드를 최소화해야 합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까? A. AWS Marketplace의 AMI를 사용하여 Amazon EC2 인스턴스에 SMTP 서버를 설정합니다. 이메일 템플릿을 Amazon S3 버킷에 저장합니다. S3 버킷에서 템플릿을 검색하고 애플리케이션의 고객 데이터를 템플릿과 병합하는 AWS Lambda 함수를 생성합니다. Lambda 함수에서 SDK를 사용하여 이메일 메시지를 보냅니다. B. 이메일 메시지를 보내도록 Amazon Simple Email Service(Amazon SES)를 설정합니다. 이메일 템플릿을 Amazon S3 버킷에 저장합니다. S3 버킷에서 템플릿을 검색하고 애플리케이션의 고객 데이터를 템플릿과 병합하는 AWS Lambda 함수를 생성합니다. Lambda 함수에서 SDK를 사용하여 이메일 메시지를 보냅니다. C. AWS Marketplace의 AMI를 사용하여 Amazon EC2 인스턴스에 SMTP 서버를 설정합니다. 고객 데이터에 대한 파라미터를 사용하여 이메일 템플릿을 Amazon Simple Email Service(Amazon SES)에 저장합니다. SES 템플릿을 호출하고 고객 데이터를 전달하여 파라미터를 대체하는 AWS Lambda 함수를 생성합니다. AWS Marketplace SMTP 서버를 사용하여 이메일 메시지를 보냅니다. D. 이메일 메시지를 보내도록 Amazon Simple Email Service(Amazon SES)를 설정합니다. 고객 데이터에 대한 파라미터를 사용하여 Amazon SES에 이메일 템플릿을 저장합니다. SendTemplatedEmail API 작업을 호출하고 고객 데이터를 전달하여 파라미터와 이메일 대상을 대체하는 AWS Lambda 함수를 생성합니다. Answer: D Explanation: In this solution, the company can use Amazon SES to send email messages, which will minimize operational overhead as SES is a fully managed service that handles sending and receiving email messages. The company can store the email template on Amazon SES with parameters for the customer data and use an AWS Lambda function to call the SendTemplatedEmail API operation, passing in the customer data to replace the parameters and the email destination. This solution eliminates the need to set up and manage an SMTP server on EC2 instances, which can be costly and time-consuming.
QUESTION NO: 3 엔터프라이즈 회사는 개발자가 AWS Marketplace를 통해 타사 소프트웨어를 구매할 수 있도록 허용하려고 합니다. 이 회사는 모든 기능이 활성화된 AWS Organizations 계정 구조를 사용하고 조달 관리자가 사용할 각 조직 단위(OU)에 공유 서비스 계정을 가지고 있습니다. 조달 팀의 정책에 따르면 개발자는 승인된 목록에서만 타사 소프트웨어를 얻을 수 있어야 하며 AWS Marketplace에서 Private Marketplace를 사용하여 이 요구 사항을 충족해야 합니다. 조달 팀은 Private Marketplace의 관리가 조달 관리자가 맡을 수 있는 procurementmanager-role이라는 역할로 제한되기를 원합니다. 기타 IAM 사용자 그룹, 역할, A. 조직의 모든 AWS 계정에서 procurement-manager-role이라는 IAM 역할을 생성합니다. PowerUserAccess 관리형 정책을 역할에 추가합니다. 인라인 정책을 모든 AWS 계정의 모든 IAM 사용자 및 역할에 적용하여 AWSPrivateMarketplaceAdminFullAccess 관리형에 대한 권한을 거부합니다. 수단. B. 조직의 모든 AWS 계정에서 procurement-manager-role이라는 IAM 역할을 생성합니다. AdministratorAccess 관리형 정책을 역할에 추가합니다. AWSPrivateMarketplaceAdminFullAccess 관리형 정책으로 권한 경계를 정의하고 모든 개발자 역할에 연결합니다. C. 조직의 모든 공유 서비스 계정에 procurement-manager-role이라는 IAM 역할을 생성합니다. 역할에 AWSPrivateMarketplaceAdminFullAccess 관리형 정책을 추가합니다. 조직 루트 수준 SCP를 생성하여 역할을 제외한 모든 사람에게 Private Marketplace를 관리할 수 있는 권한을 거부합니다. procurement-manager-role이라는 조직의 모든 사람에게 procurement-manager-role이라는 IAM 역할을 생성할 수 있는 권한을 거부하려면 다른 조직 루트 수준 SCP를 생성합니다. D. 개발자가 사용할 모든 AWS 계정에서 procurement-manager-role이라는 IAM 역할을 생성합니다. 역할에 AWSPrivateMarketplaceAdminFullAccess 관리형 정책을 추가합니다. 조직에서 SCP를 생성하여 procurement-manager-role이라는 역할을 제외한 모든 사람에게 Private Marketplace를 관리할 수 있는 권한을 거부합니다. 조직의 모든 공유 서비스 계정에 SCP를 적용합니다. Answer: C Explanation: SCP to deny permissions to administer Private Marketplace to everyone except the role named procurement-manager-role. https://aws.amazon.com/blogs/awsmarketplace/controlling-access-to-a-well-architectedprivate-marketplace-using-iam-and-aws-organizations/ This approach allows the procurement managers to assume the procurement-manager-role in shared services accounts, which have the AWSPrivateMarketplaceAdminFullAccess managed policy attached to it and can then manage the Private Marketplace. The organization root-level SCP denies the permission to administer Private Marketplace to everyone except the role named procurement-manager-role and another SCP denies the permission to create an IAM role named procurement-manager-role to everyone in the organization, ensuring that only the procurement team can assume the role and manage the Private Marketplace. This approach provides a centralized way to manage and restrict access to Private Marketplace while maintaining a high level of security.
QUESTION NO: 4 솔루션 설계자는 회사가 Amazon Workspaces에서 새 세션을 설정할 수 없는 문제를 조사하고 있습니다. 초기 분석 결과 문제에 사용자 프로필이 관련되어 있음이 나타납니다. Amazon Workspaces 환경은 Amazon FSx for Windows File Server를 프로필 공유 스토리지로 사용하도록 구성됩니다. FSx for Windows File Server 파일 시스템은 10TB 스토리지로 구성됩니다. 솔루션 설계자는 파일 시스템이 최대 용량에 도달했음을 발견합니다. 솔루션 설계자는 사용자가 다시 액세스할 수 있는지 확인해야 합니다. 솔루션은 또한 문제가 다시 발생하지 않도록 방지해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 이전 사용자 프로필을 제거하여 공간을 만듭니다. 사용자 프로필을 Amazon FSx for Lustre 파일 시스템으로 마이그레이션합니다. B. update-file-system 명령을 사용하여 용량을 늘립니다. 여유 공간을 모니터링하는 Amazon CloudWatch 지표를 구현합니다. Amazon EventBridge를 사용하여 AWS Lambda 함수를 호출하여 필요에 따라 용량을 늘립니다. C. Amazon CloudWatch에서 FreeStorageCapacity 지표를 사용하여 파일 시스템을 모니터링합니다. AWS Step Functions를 사용하여 필요에 따라 용량을 늘립니다. D. 이전 사용자 프로필을 제거하여 공간을 만듭니다. Windows 파일 서버 파일 시스템용 추가 FSx를 만듭니다. 사용자의 50%가 새 파일 시스템을 사용하도록 사용자 프로필 리디렉션을 업데이트합니다. Answer: B Explanation: It can prevent the issue from happening again by monitoring the file system with the FreeStorageCapacity metric in Amazon CloudWatch and using Amazon EventBridge to invoke an AWS Lambda function to increase the capacity as required. This ensures that the file system always has enough free space to store user profiles and avoids reaching maximum capacity.
QUESTION NO: 5 회사에는 이벤트 지속성을 위해 PostgreSQL 데이터베이스를 사용하는 온프레미스 모니터링 솔루션이 있습니다. 과도한 수집으로 인해 데이터베이스를 확장할 수 없으며 스토리지가 자주 부족합니다. 이 회사는 하이브리드 솔루션을 만들고자 하며 이미 네트워크와 AWS 간에 VPN 연결을 설정했습니다. 솔루션에는 다음 속성이 포함되어야 합니다. * 운영 복잡성을 최소화하는 관리형 AWS 서비스 * 데이터 처리량에 맞게 자동으로 확장되며 지속적인 관리가 필요하지 않은 버퍼입니다. * 거의 실시간으로 이벤트를 관찰하기 위해 대시보드를 생성하는 시각화 도구입니다. * 반구조화된 JSON 데이터 및 동적 스키마 지원. 이러한 요구 사항을 충족하는 모니터링 솔루션을 만들 수 있는 구성 요소 조합은 무엇입니까?''(2개 선택) A. Amazon Kinesis Data Firehose를 사용하여 이벤트 버퍼링 AWS Lambda 함수 생성 10 이벤트 처리 및 변환 B. 이벤트를 버퍼링할 Amazon Kinesis 데이터 스트림 생성 이벤트를 처리하고 변환하는 AWS Lambda 함수 생성 C. 이벤트를 수신하도록 Amazon Aurora PostgreSQL DB 클러스터 구성 Amazon Quick Sight를 사용하여 데이터베이스에서 읽고 실시간에 가까운 시각화 및 대시보드 생성 D. 이벤트를 수신하도록 Amazon Elasticsearch Service(Amazon ES) 구성 Amazon ES와 함께 배포된 Kibana 엔드포인트를 사용하여 실시간에 가까운 시각화 및 대시보드를 생성합니다. E. 이벤트를 수신하도록 Amazon Neptune 0 DB 인스턴스 구성 Amazon QuickSight를 사용하여 데이터베이스에서 읽고 거의 실시간에 가까운 시각화 및 대시보드 생성 Answer: A,D Explanation: https://aws.amazon.com/kinesis/data-firehose/faqs/
QUESTION NO: 6 회사에서 AWS Elastic Beanstalk에 애플리케이션을 배포했습니다. 애플리케이션은 데이터베이스 계층에 Amazon Aurora를 사용합니다. Amazon CloudFront 배포는 웹 요청을 처리하고 Elastic Beanstalk 도메인 이름을 오리진 서버로 포함합니다. 배포는 방문자가 애플리케이션에 액세스할 때 사용하는 대체 도메인 이름으로 구성됩니다. 매주 회사는 일상적인 유지 관리를 위해 응용 프로그램을 서비스에서 제외합니다. 애플리케이션을 사용할 수 없는 시간 동안 회사는 방문자가 CloudFront 오류 메시지 대신 정보 메시지를 받기를 원합니다. 솔루션 설계자는 프로세스의 첫 번째 단계로 Amazon S3 버킷을 생성합니다. 요구 사항을 충족하기 위해 솔루션 설계자가 다음으로 수행해야 하는 단계 조합은 무엇입니까? (3개를 선택하세요.) A. 정적 정보 콘텐츠를 S3 버킷에 업로드합니다. B. 새 CloudFront 배포를 만듭니다. S3 버킷을 오리진으로 설정합니다. C. S3 버킷을 원래 CloudFront 배포에서 두 번째 오리진으로 설정합니다. 원본 액세스 ID(OAI)를 사용하도록 배포 및 S3 버킷을 구성합니다. D. 주간 유지 관리 중에 S3 오리진을 사용하도록 기본 캐시 동작을 편집합니다. 유지 관리가 완료되면 변경 사항을 되돌립니다. E. 주간 유지 관리 중에 새 배포에서 S3 오리진에 대한 캐시 동작을 생성합니다. 경로 패턴을 \로 설정합니다. 우선 순위를 0으로 설정합니다. 유지 관리가 완료되면 캐시 동작을 삭제합니다. F. 주간 유지 관리 중에 S3 버킷의 트래픽을 처리하도록 Elastic Beanstalk를 구성합니다. Answer: A,C,D Explanation: The company wants to serve static content from an S3 bucket during the maintenance period. To do this, the following steps are required: Upload static informational content to the S3 bucket. This will provide the source of the content that will be served to the visitors. Set the S3 bucket as a second origin in the original CloudFront distribution. Configure the distribution and the S3 bucket to use an origin access identity (OAI). This will allow CloudFront to access the S3 bucket securely and prevent public access to the bucket. During the weekly maintenance, edit the default cache behavior to use the S3 origin. Revert the change when the maintenance is complete. This will redirect all web requests to the S3 bucket instead of the Elastic Beanstalk domain name. The other options are not correct because: Creating a new CloudFront distribution is not necessary and would require changing the alternate domain name configuration. Creating a cache behavior for the S3 origin on a new distribution would not work because the visitors would still access the original distribution using the alternate domain name. Configuring Elastic Beanstalk to serve traffic from the S3 bucket is not possible and would not achieve the desired result. Reference: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3An dCustomOrigins.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-contentrestricting-access-to-s3.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/distribution-webvalues-specify.html#DownloadDistValuesPathPattern
QUESTION NO: 7 회사는 회사의 데이터 센터에 있는 VM에 복잡한 종속성이 있는 Java 애플리케이션을 실행합니다. 응용 프로그램이 안정적입니다. 하지만 회사는 기술 스택을 현대화하고자 합니다. 회사는 애플리케이션을 AWS로 마이그레이션하고 서버를 유지 관리하기 위한 관리 오버헤드를 최소화하려고 합니다. 최소한의 코드 변경으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS App2Container를 사용하여 애플리케이션을 AWS Fargate의 Amazon Elastic Container Service(Amazon ECS)로 마이그레이션합니다. Amazon Elastic Container Registry(Amazon ECR)에 컨테이너 이미지를 저장합니다. ECS 작업 실행 역할 권한 10에 ECR 이미지 리포지토리 액세스 권한을 부여합니다. Application Load Balancer(ALB)를 사용하도록 Amazon ECS를 구성합니다. ALB를 사용하여 애플리케이션과 상호 작용합니다. B. 애플리케이션 코드를 AWS Lambda에서 실행되는 컨테이너로 마이그레이션합니다. Lambda 통합으로 Amazon API Gateway REST API를 구축합니다. API 게이트웨이를 사용하여 애플리케이션과 상호 작용합니다. C. AWS App2Container를 사용하여 EKS 관리형 노드 그룹의 Amazon Elastic Kubernetes Service(Amazon EKS)로 애플리케이션을 마이그레이션합니다. Amazon Elastic Container Registry(Amazon ECR)에 컨테이너 이미지를 저장합니다. ECR 이미지 리포지토리에 액세스할 수 있는 권한을 EKS 노드에 부여합니다. Amazon API Gateway를 사용하여 애플리케이션과 상호 작용합니다. D. 애플리케이션 코드를 AWS Lambda에서 실행되는 컨테이너로 마이그레이션합니다. Application Load Balancer(ALB)를 사용하도록 Lambda를 구성합니다. ALB를 사용하여 애플리케이션과 상호 작용합니다. Answer: B Explanation: By using AWS App2Container to migrate the application to Amazon ECS, the company can make the migration process easier. Additionally, using Amazon ECR to store the container images and granting the ECS task execution role permission to access the ECR image repository will minimize the administrative overhead to maintain the servers. Finally, configuring Amazon ECS to use an ALB and using the ALB to interact with the application will reduce the amount of code changes needed. This solution will allow the company to modernize their technology stack while minimizing the amount of code changes needed. You can refer to the AWS App2Container documentation for more information on how to use this service: https://aws.amazon.com/app2container/ You can refer to the AWS Fargate documentation for more information on how to use this service: https://aws.amazon.com/fargate/ You can refer to the AWS Elastic Container Service documentation for more information on how to use this service: https://aws.amazon.com/ecs/ You can refer to the Amazon Elastic Container Registry documentation for more information on how to use this service: https://aws.amazon.com/ecr/ You can refer to the Application Load Balancer documentation for more information on how to use this service: https://aws.amazon.com/elasticloadbalancing/applicationloadbalancer/
QUESTION NO: 8 회사에서 Amazon API Gateway와 AWS Lambd를 사용하여 새로운 서버리스 API를 개발하고 있습니다. 이 회사는 Lambda 함수를 API Gateway와 통합하여 여러 공유 라이브러리와 사용자 지정 클래스를 사용했습니다. 솔루션 설계자는 솔루션 배포를 단순화하고 코드 재사용을 최적화해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 공유 라이브러리 및 사용자 지정 클래스를 Docker 이미지에 배포합니다. 이미지를 S3 버킷에 저장합니다. Docker 이미지를 소스로 사용하는 Lambda 계층을 생성합니다. API의 Lambda 함수를 Zip 패키지로 배포합니다. Lambda 계층을 사용하도록 패키지를 구성합니다. B. 공유 라이브러리 및 사용자 지정 클래스를 Docker 이미지에 배포합니다. 이미지를 Amazon Elastic Container Registry(Amazon ECR)에 업로드합니다. Docker 이미지를 소스로 사용하는 Lambda 계층을 생성합니다. API의 Lambda 함수를 Zip 패키지로 배포합니다. Lambda 계층을 사용하도록 패키지를 구성합니다. C. AWS Fargate 시작 유형을 사용하여 Amazon Elastic Container Service(Amazon ECS)의 Docker 컨테이너에 공유 라이브러리 및 사용자 지정 클래스를 배포합니다. API의 Lambda 함수를 Zip 패키지로 배포합니다. 배포된 컨테이너를 Lambda 계층으로 사용하도록 패키지를 구성합니다. D. API의 Lambda 함수에 대한 공유 라이브러리, 사용자 지정 클래스 및 코드를 Docker 이미지에 배포합니다. 이미지를 Amazon Elastic Container Registry(Amazon ECR)에 업로드합니다. Docker 이미지를 배포 패키지로 사용하도록 API의 Lambda 함수를 구성합니다. Answer: B Explanation: Deploying the shared libraries and custom classes to a Docker image and uploading the image to Amazon Elastic Container Registry (Amazon ECR) and creating a Lambda layer that uses the Docker image as the source. Then, deploying the API's Lambda functions as Zip packages and configuring the packages to use the Lambda layer would meet the requirements for simplifying the deployment and optimizing for code reuse. A Lambda layer is a distribution mechanism for libraries, custom runtimes, and other function dependencies. It allows you to manage your in-development function code separately from your dependencies, this way you can easily update your dependencies without having to update your entire function code. By deploying the shared libraries and custom classes to a Docker image and uploading the image to Amazon Elastic Container Registry (ECR), it makes it easy to manage and version the dependencies. This way, the company can use the same version of the dependencies across different Lambda functions. By creating a Lambda layer that uses the Docker image as the source, the company can configure the API's Lambda functions to use the layer, reducing the need to include the dependencies in each function package, and making it easy to update the dependencies across all functions at once. Reference: AWS Lambda Layers documentation: https://docs.aws.amazon.com/lambda/latest/dg/configuration-layers.html AWS Elastic Container Registry (ECR) documentation: https://aws.amazon.com/ecr/ Building Lambda Layers with Docker documentation: https://aws.amazon.com/blogs/compute/building-lambdalayers-with-docker/
QUESTION NO: 9 배송 회사는 타사 경로 계획 애플리케이션을 AWS로 마이그레이션해야 합니다. 타사는 공용 레지스트리에서 지원되는 Docker 이미지를 제공합니다. 경로 맵을 생성하는 데 필요한 만큼의 컨테이너에서 이미지를 실행할 수 있습니다. 회사는 배송 기사가 허브에서 고객까지 최단 거리를 이동할 수 있도록 배송 영역을 공급 허브가 있는 구역으로 구분했습니다. 경로 맵을 생성하는 데 필요한 시간을 줄이기 위해 각 섹션은 섹션 영역에서만 주문을 처리하는 사용자 지정 구성과 함께 고유한 Docker 컨테이너 집합을 사용합니다. 회사는 실행 중인 컨테이너 수에 따라 리소스를 비용 효율적으로 할당할 수 있는 기능이 필요합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Amazon EC2에서 Amazon Elastic Kubernetes Service(Amazon EKS) 클러스터를 생성합니다. Amazon EKS CLI를 사용하여 -tags 옵션을 사용하여 포드에 사용자 지정 태그를 할당함으로써 포드에서 계획 애플리케이션을 시작합니다. B. AWS Fargate에서 Amazon Elastic Kubernetes Service(Amazon EKS) 클러스터를 생성합니다. Amazon EKS CLI를 사용하여 계획 애플리케이션을 시작합니다. AWS CLI tag-resource API 호출을 사용하여 포드에 사용자 지정 태그를 할당합니다. C. Amazon EC2에서 Amazon Elastic Container Service(Amazon ECS) 클러스터를 생성합니다. run-tasks가 true로 설정된 AWS CLI를 사용하여 -tags 옵션을 사용하여 작업에 사용자 지정 태그를 할당함으로써 계획 애플리케이션을 시작합니다. D. AWS Fargate에서 Amazon Elastic Container Service(Amazon ECS) 클러스터를 생성합니다. AWS CLI run-task 명령을 사용하고 enableECSManagedTags를 true로 설정하여 계획 애플리케이션을 시작합니다. --tags 옵션을 사용하여 작업에 사용자 지정 태그를 할당합니다. Answer: D Explanation: Amazon Elastic Container Service (ECS) on AWS Fargate is a fully managed service that allows you to run containers without having to manage the underlying infrastructure. When you launch tasks on Fargate, resources are automatically allocated based on the number of tasks running, which reduces the operational overhead. Using ECS on Fargate allows you to assign custom tags to tasks using the --tags option in the run-task command, as described in the documentation: https://docs.aws.amazon.com/cli/latest/reference/ecs/run-task.html You can also set enableECSManagedTags to true, which allows the service to automatically add the cluster name and service name as tags. https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placementconstraints.html#tag-based-scheduling
QUESTION NO: 10 회사는 Amazon S3를 사용하여 다양한 스토리지 클래스에 파일과 이미지를 저장합니다. 회사의 S3 비용은 작년에 크게 증가했습니다. 솔루션 설계자는 지난 12개월 동안의 데이터 추세를 검토하고 개체에 적합한 스토리지 클래스를 식별해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 지난 12개월간의 S3 사용에 대한 AWS 비용 및 사용 보고서를 다운로드하십시오. 비용 절감을 위한 AWS Trusted Advisor 권장 사항을 검토합니다. B. S3 스토리지 클래스 분석을 사용합니다. 데이터 추세를 Amazon QuickSight 대시보드로 가져와 스토리지 추세를 분석합니다. C. Amazon S3 Storage Lens를 사용합니다. 스토리지 경향에 대한 고급 지표를 포함하도록 기본 대시보드를 업그레이드하십시오. D. S3용 액세스 분석기를 사용합니다. 지난 12개월 동안의 Access Analyzer for S3 보고서를 다운로드하십시오. csvfile을 Amazon QuickSight 대시보드로 가져옵니다. Answer: B Explanation: https://docs.aws.amazon.com/AmazonS3/latest/userguide/storage_lens.html
QUESTION NO: 11 회사는 Amazon Elastic File System(Amazon EFS)의 백업 비용을 최적화해야 합니다. 솔루션 설계자는 EFS 백업을 위해 AWS Backup에서 이미 백업 계획을 구성했습니다. 백업 계획에는 7일 후에 EFS 백업을 콜드 스토리지로 전환하고 추가 90일 동안 백업을 유지하는 수명 주기 구성 규칙이 포함되어 있습니다. 한 달 후 회사는 EFS 스토리지 비용을 검토하고 EFS 백업 비용이 증가했음을 확인합니다. EFS 백업 콜드 스토리지는 EFS 웜 백업 스토리지 비용의 거의 두 배를 생성합니다. 솔루션 설계자는 비용을 최적화하기 위해 무엇을 해야 합니까? A. 1일 후 EFS 백업을 콜드 스토리지로 이동하도록 백업 규칙의 수명 주기 구성을 수정합니다. 백업 보존 기간을 30일로 설정합니다. B. 8일 후에 EFS 백업을 콜드 스토리지로 이동하도록 백업 규칙의 수명 주기 구성을 수정합니다. 백업 보존 기간을 30일로 설정합니다. C. 1일 후 EFS 백업을 콜드 스토리지로 이동하도록 백업 규칙의 수명 주기 구성을 수정합니다 . 백업 보존 기간을 90일로 설정합니다. D. 8일 후에 EFS 백업을 콜드 스토리지로 이동하도록 백업 규칙의 수명 주기 구성을 수정합니다. 백업 보존 기간을 98일로 설정합니다. Answer: A Explanation: The cost of EFS backup cold storage is $0.01 per GB-month, whereas the cost of EFS backup warm storage is $0.05 per GB-month1. Therefore, moving the backups to cold storage as soon as possible will reduce the storage cost. However, cold storage backups must be retained for a minimum of 90 days2, otherwise they incur a pro-rated charge equal to the storage charge for the remaining days1. Therefore, setting the backup retention period to 30 days will incur a penalty of 60 days of cold storage cost for each backup deleted. This penalty will still be lower than keeping the backups in warm storage for 7 days and then in cold storage for 83 days, which is the current configuration. Therefore, option A is the most cost-effective solution.
QUESTION NO: 12 회사의 솔루션 아키텍트가 샌드박스 AWS 계정에서 내부적으로 개발된 새로운 애플리케이션을 검토하고 있습니다. 애플리케이션은 IAM 인스턴스 프로필이 연결된 Amazon EC2 인스턴스의 AWS Auto Scaling 그룹을 사용합니다. 애플리케이션 로직의 일부가 AWS Secrets Manager에서 비밀을 생성하고 액세스합니다. 회사 기능을 테스트하기 위해 애플리케이션 API를 호출하는 AWS Lambda 함수가 있습니다. 회사는 또한 계정에 AWS CloudTrail 추적을 생성했습니다. 애플리케이션'개발자가 SecretsManagerReadWrite AWS 관리형 IAM 정책을 IAM 역할에 연결했습니다. IAM 역할이 EC2 인스턴스에 연결된 인스턴스 프로파일과 연결되었습니다. 솔루션 설계자가 테스트를 위해 Lambda 함수를 호출했습니다. 솔루션 설계자가 SecretsManagerReadWrite 정책을 애플리케이션에 필요한 Secrets Manager 작업에 대한 최소 권한 액세스를 제공하는 새로운 정책 이러한 요구 사항을 충족하는 운영 효율성이 가장 높은 솔루션은 무엇입니까? A. IAM 역할에 대한 CloudTrail 이벤트를 기반으로 정책 생성 생성된 정책 출력을 사용하여 새 IAM 정책 생성 새로 생성된 IAM 정책을 사용하여 IAM 역할에 연결된 SecretsManagerReadWrite 정책 교체 B. AWS Identity and Access Management Access Analyzer에서 분석기 생성 IAM 역할의 Access Advisor 조사 결과를 사용하여 새 IAM 정책 생성 새로 생성된 IAM 정책을 사용하여 IAM 역할에 연결된 SecretsManagerReadWrite 정책 교체 C. aws cloudtrail lookup-events AWS CLI 명령을 사용하여 Secrets Manager와 관련된 CloudTrail 이벤트를 필터링하고 내보냅니다. CloudTrail의 작업이 포함된 새 IAM 정책을 사용하여 IAM 역할에 연결된 SecretsManagerReadWrite 정책을 바꿉니다. D. IAM 정책 시뮬레이터를 사용하여 IAM 역할에 대한 IAM 정책 생성 새로 생성된 IAM 정책을 사용하여 IAM 역할에 연결된 SecretsManagerReadWrite 정책 교체 Answer: B Explanation: The IAM policy simulator will generate a policy that contains only the necessary permissions for the application to access Secrets Manager, providing the least privilege necessary to get the job done. This is the most efficient solution as it will not require additional steps such as analyzing CloudTrail events or manually creating and testing an IAM policy. You can use the IAM policy simulator to generate an IAM policy for an IAM role by specifying the role and the API actions and resources that the application or service requires. The simulator will then generate an IAM policy that grants the least privilege access to those actions and resources. Once you have generated an IAM policy using the simulator, you can replace the existing SecretsManagerReadWrite policy that is attached to the IAM role with the newly generated policy. This will ensure that the application or service has the least privilege access to the Secrets Manager actions that it requires. You can access the IAM policy simulator through the IAM console, AWS CLI, and AWS SDKs. Here is the link for more information: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_simulator.html
QUESTION NO: 13 AWS Organizations를 사용하는 회사는 개발자가 AWS를 실험할 수 있도록 합니다. 회사에서 배포한 랜딩 존의 일부로 개발자는 회사 이메일 주소를 사용하여 계정을 요청합니다. 회사는 개발자가 비용이 많이 드는 서비스를 시작하거나 불필요하게 서비스를 실행하지 않도록 하기를 원합니다. 회사는 개발자에게 AWS 비용을 제한하기 위해 고정된 월 예산을 제공해야 합니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (3개를 선택하세요.) A. 고정된 월 계정 사용 한도를 설정하기 위해 SCP를 생성합니다. 개발자 계정에 SCP를 적용합니다. B. AWS 예산을 사용하여 계정 생성 프로세스의 일부로 각 개발자 계정에 대해 고정된 월 예산을 생성합니다. C. 값비싼 서비스 및 구성 요소에 대한 액세스를 거부하는 SCP를 만듭니다. 개발자 계정에 SCP를 적용합니다. D. 비용이 많이 드는 서비스 및 구성 요소에 대한 액세스를 거부하는 IAM 정책을 만듭니다. 개발자 계정에 IAM 정책을 적용합니다. E. 예산 금액에 도달하면 서비스를 종료하는 AWS 예산 알림 작업을 생성합니다. 모든 서비스를 종료하도록 조치를 구성하십시오. F. 예산 금액에 도달하면 Amazon Simple Notification Service(Amazon SNS) 알림을 보내는 AWS 예산 알림 작업을 생성합니다. 모든 서비스를 종료하려면 AWS Lambda 함수를 호출하십시오. Answer: B,C,F Explanation: Option A is incorrect because creating an SCP to set a fixed monthly account usage limit is not possible. SCPs are policies that specify the services and actions that users and roles can use in the member accounts of an AWS Organization. SCPs cannot enforce budget limits or prevent users from launching costly services or running services unnecessarily1 Option B is correct because using AWS Budgets to create a fixed monthly budget for each developer's account as part of the account creation process meets the requirement of giving developers a fixed monthly budget to limit their AWS costs. AWS Budgets allows you to plan your service usage, service costs, and instance reservations. You can create budgets that alert you when your costs or usage exceed (or are forecasted to exceed) your budgeted amount2 Option C is correct because creating an SCP to deny access to costly services and components meets the requirement of ensuring that developers are not launching costly services or running services unnecessarily. SCPs can restrict access to certain AWS services or actions based on conditions such as region, resource tags, or request time. For example, an SCP can deny access to Amazon Redshift clusters or Amazon EC2 instances with certain instance types1 Option D is incorrect because creating an IAM policy to deny access to costly services and components is not sufficient to meet the requirement of ensuring that developers are not launching costly services or running services unnecessarily. IAM policies can only control access to resources within a single AWS account. If developers have multiple accounts or can create new accounts, they can bypass the IAM policy restrictions. SCPs can apply across multiple accounts within an AWS Organization and prevent users from creating new accounts that do not comply with the SCP rules3 Option E is incorrect because creating an AWS Budgets alert action to terminate services when the budgeted amount is reached is not possible. AWS Budgets alert actions can only perform one of the following actions: apply an IAM policy, apply an SCP, or send a notification through Amazon SNS. AWS Budgets alert actions cannot terminate services directly. Option F is correct because creating an AWS Budgets alert action to send an Amazon SNS notification when the budgeted amount is reached and invoking an AWS Lambda function to terminate all services meets the requirement of giving developers a fixed monthly budget to limit their AWS costs. AWS Budgets alert actions can send notifications through Amazon SNS when a budget threshold is breached. Amazon SNS can trigger an AWS Lambda function that can perform custom logic such as terminating all services in the developer's account. This way, developers cannot exceed their budget limit and incur additional costs.
QUESTION NO: 14 금융 회사는 웹 애플리케이션을 온프레미스에서 AWS로 마이그레이션할 계획입니다. 회사는 타사 보안 도구를 사용하여 애플리케이션에 대한 인바운드 트래픽을 모니터링합니다. 이 회사는 지난 15년 동안 이 보안 도구를 사용해 왔으며 이 도구에는 공급업체에서 제공하는 클라우드 솔루션이 없습니다. 회사의 보안 팀은 보안 도구를 AWS 기술과 통합하는 방법에 대해 우려하고 있습니다. 이 회사는 Amazon EC2 인스턴스에서 AWS로의 애플리케이션 마이그레이션을 배포할 계획입니다. EC2 인스턴스는 전용 VPC의 Auto Scaling 그룹에서 실행됩니다. 회사는 보안 도구를 사용하여 VPC로 들어오고 나가는 모든 패킷을 검사해야 합니다. 이 검사는 실시간으로 이루어져야 하며 애플리케이션의 성능에 영향을 미치지 않아야 합니다. 솔루션 설계자는 AWS 리전 내에서 가용성이 높은 대상 아키텍처를 AWS에서 설계해야 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까? (2개를 선택하세요.) A. 기존 VPC의 새 Auto Scaling 그룹에 있는 EC2 인스턴스에 보안 도구를 배포합니다. B. Network Load Balancer 뒤에 웹 애플리케이션을 배포합니다. C. 보안 도구 인스턴스 앞에 Application Load Balancer를 배포합니다. D. 각 가용 영역에 대한 게이트웨이 로드 밸런서를 프로비저닝하여 트래픽을 보안 도구로 리디렉션합니다. E. VPC 간의 통신을 용이하게 하기 위해 전송 게이트웨이를 프로비저닝합니다. Answer: A,D Explanation: Option A, Deploy the security tool on EC2 instances in a new Auto Scaling group in the existing VPC, allows the company to use its existing security tool while still running it within the AWS environment. This ensures that all packets coming in and out of the VPC are inspected by the security tool in real time. Option D, Provision a Gateway Load Balancer for each Availability Zone to redirect the traffic to the security tool, allows for high availability within an AWS Region. By provisioning a Gateway Load Balancer for each Availability Zone, the traffic is redirected to the security tool in the event of any failures or outages. This ensures that the security tool is always available to inspect the traffic, even in the event of a failure.
QUESTION NO: 15 회사는 eu-west-1 리전에서 애플리케이션을 실행하고 각 환경 개발, 테스트 및 프로덕션에 대해 하나의 계정을 가지고 있습니다. 모든 환경은 상태 저장 Amazon EC2 인스턴스 및 Amazon RDS를 사용하여 연중무휴로 실행됩니다. for MySQL 데이터베이스 데이터베이스 크기는 500GB에서 800GB 사이입니다. 개발 팀과 테스트 팀은 영업일 업무 시간 중에 작업하지만 프로덕션 환경은 하루 24시간 운영됩니다. 주 7일. 회사는 비용 절감을 원합니다. AH 리소스에는 개발, 테스트 또는 프로덕션을 키로 하는 환경 태그가 지정됩니다. 최소한의 운영 노력으로 비용을 절감하기 위해 솔루션 설계자는 무엇을 해야 합니까? A. 매일 한 번 실행되는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. 태그 요일 및 시간에 따라 인스턴스를 시작하거나 중지하는 하나의 AWS Lambda 함수를 호출하도록 규칙을 구성합니다. B. 매일 저녁에 실행되는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 만듭니다. 태그를 기반으로 인스턴스를 중지하는 AWS Lambda 함수를 호출하도록 규칙을 구성합니다. 매일 아침에 실행되는 두 번째 EventBridge(CloudWatch 이벤트) 규칙을 생성합니다. 태그를 기반으로 인스턴스를 시작하는 다른 Lambda 함수를 호출하도록 두 번째 규칙을 구성합니다. C. 매일 저녁에 실행되는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. 태그를 기반으로 인스턴스를 종료하는 AWS Lambda 함수를 호출하도록 규칙을 구성합니다. 매일 실행되는 두 번째 EventBridge(CloudWatch Events) 규칙을 생성합니다. 영업일 오전 태그를 기반으로 마지막 백업에서 인스턴스를 복원하는 다른 Lambda 함수를 호출하도록 두 번째 규칙을 구성합니다. D. 매시간 실행되는 Amazon EventBridge 규칙을 생성합니다. 태그를 기반으로 마지막 백업에서 인스턴스를 종료하거나 복원하는 하나의 AWS Lambda 함수를 호출하도록 규칙을 구성합니다. 요일, 시간. Answer: B Explanation: Creating an Amazon EventBridge rule that runs every business day in the evening to stop instances and another rule that runs every business day in the morning to start instances based on the tag will reduce costs with the least operational effort. This approach allows for instances to be stopped during non-business hours when they are not in use, reducing the costs associated with running them. It also allows for instances to be started again in the morning when the development and testing teams need to use them.
QUESTION NO: 16 글로벌 사무소가 있는 회사에는 단일 AWS 리전에 대한 단일 1Gbps AWS Direct Connect 연결이 있습니다. 회사의 온프레미스 네트워크는 연결을 사용하여 AWS 클라우드에 있는 회사 리소스와 통신합니다. 연결에는 단일 VPC에 연결되는 단일 프라이빗 가상 인터페이스가 있습니다. 솔루션 설계자는 동일한 리전에서 중복 Direct Connect 연결을 추가하는 솔루션을 구현해야 합니다. 솔루션은 또한 회사가 다른 지역으로 확장함에 따라 동일한 Direct Connect 연결 쌍을 통해 다른 지역에 대한 연결을 제공해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Direct Connect 게이트웨이를 프로비저닝합니다. 기존 연결에서 기존 프라이빗 가상 인터페이스를 삭제합니다. 두 번째 Direct Connect 연결을 만듭니다. 각 연결에서 새 프라이빗 가상 인터레이스를 생성하고 두 프라이빗 가상 인터페이스를 Direct Connect 게이트웨이에 연결합니다. Direct Connect 게이트웨이를 단일 VPC에 연결합니다. B. 기존 개인 가상 인터페이스를 유지합니다. 두 번째 Direct Connect 연결을 만듭니다. 새 연결에서 새 프라이빗 가상 인터페이스를 생성하고 새 프라이빗 가상 인터페이스를 단일 VPC에 연결합니다. C. 기존 프라이빗 가상 인터페이스를 유지합니다. 두 번째 Direct Connect 연결을 만듭니다. 새 연결에서 새 퍼블릭 가상 인터페이스를 생성하고 새 퍼블릭 가상 인터페이스를 단일 VPC에 연결합니다. D. 전송 게이트웨이를 프로비저닝합니다. 기존 연결에서 기존 프라이빗 가상 인터페이스를 삭제합니다. 두 번째 Direct Connect 연결을 만듭니다. 각 연결에서 새 프라이빗 가상 인터페이스를 생성하고 두 프라이빗 가상 인터페이스를 전송 게이트웨이에 연결합니다. 전송 게이트웨이를 단일 VPC와 연결합니다. Answer: A Explanation: A Direct Connect gateway is a globally available resource. You can create the Direct Connect gateway in any Region and access it from all other Regions. The following describe scenarios where you can use a Direct Connect gateway. https://docs.aws.amazon.com/directconnect/latest/UserGuide/direct-connect-gatewaysintro.html
QUESTION NO: 17 한 회사에 수백 개의 AWS 계정이 있습니다. 이 회사는 최근 새로운 예약 인스턴스를 구매하고 기존 예약 인스턴스를 수정하기 위한 중앙 집중식 내부 프로세스를 구현했습니다. 이 프로세스에서는 예약 인스턴스를 구매하거나 수정하려는 모든 사업부가 조달을 위해 전담 팀에 요청을 제출해야 합니다. 이전에는 사업부가 각자의 AWS 계정에서 자율적으로 예약 인스턴스를 직접 구매하거나 수정했습니다. 솔루션 설계자는 가능한 한 가장 안전한 방법으로 새 프로세스를 시행해야 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까? (2개 선택하세요.) A. 모든 AWS 계정이 모든 기능이 활성화된 AWS Organizations의 조직에 속해 있는지 확인하십시오. B. AWS Config를 사용하여 ec2:PurchaseReservedInstancesOffering 작업 및 ec2:ModifyReservedInstances 작업에 대한 액세스를 거부하는 IAM 정책의 연결에 대해 보고합니다. C. 각 AWS 계정에서 ec2:PurchaseReservedInstancesOffering 작업 및 ec2:ModifyReservedInstances 작업을 거부하는 IAM 정책을 생성합니다. D. ec2:PurchaseReservedInstancesOffering 작업 및 ec2:ModifyReservedInstances 작업을 거부하는 SCP를 생성합니다. SCP를 조직의 각 OU에 연결합니다. E. 모든 AWS 계정이 통합 결제 기능을 사용하는 AWS Organizations의 조직에 속해 있는지 확인하십시오. Answer: A,D Explanation: All features - The default feature set that is available to AWS Organizations. It includes all the functionality of consolidated billing, plus advanced features that give you more control over accounts in your organization. For example, when all features are enabled the management account of the organization has full control over what member accounts can do. The management account can apply SCPs to restrict the services and actions that users (including the root user) and roles in an account can access. https://docs.aws.amazon.com/organizations/latest/userguide/orgs_gettingstarted_ concepts.html#feature-set
QUESTION NO: 18 한 회사에 AWS Organizations에 AWS 계정이 많은 조직이 있습니다. AWS 계정 중 하나가 전송 계정으로 지정되고 다른 모든 AWS 계정과 공유되는 전송 게이트웨이가 있습니다. 회사의 모든 글로벌 사무소와 전송 계정 간에 AWS Site-to-Site VPN 연결이 구성됩니다. 모든 계정에서 AWS Config가 활성화되었습니다. 회사의 네트워킹 팀은 글로벌 사무소에 속하는 내부 IP 주소 범위 목록을 중앙에서 관리해야 합니다. 개발자는 이 목록을 참조하여 응용 프로그램에 안전하게 액세스할 수 있습니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Amazon S3에서 호스팅되고 모든 내부 IP 주소 범위를 나열하는 JSON 파일을 생성합니다. JSON 파일이 업데이트될 때 관련될 수 있는 각 계정에서 Amazon Simple Notification Service(Amazon SNS) 주제를 구성합니다. . SNS 주제에 대한 AWS Lambda 함수를 구독하여 Vie 업데이트된 IP 주소 범위로 모든 관련 보안 그룹 규칙을 업데이트합니다. B. 모든 내부 IP 주소 범위를 포함하는 새 AWS Config 관리형 규칙을 생성합니다. 이 규칙을 사용하여 각 계정의 보안 그룹을 확인하여 IP 주소 범위 목록을 준수하는지 확인합니다. 탐지된 규정 미준수 보안 그룹을 자동으로 수정하도록 규칙을 구성합니다. C. 전송 계정에서 모든 내부 IP 주소 범위로 VPC 접두사 목록을 생성합니다. AWS Resource Access Manager를 사용하여 접두사 목록을 다른 모든 계정과 공유하십시오. 공유 접두사 목록을 사용하여 다른 계정인 보안 그룹 규칙을 구성합니다. D. 전송 계정에서 모든 내부 IP 주소 범위로 보안 그룹을 생성합니다. 대중 교통 계정의 보안을 참조하도록 내 다른 계정의 보안 그룹 구성 Answer: C group by using a nested security group reference of *<transit-account-id>./sg-1a2b3c4d". Explanation: Customer-managed prefix lists - Sets of IP address ranges that you define and manage. You can share your prefix list with other AWS accounts, enabling those accounts to reference the prefix list in their own resources. https://docs.aws.amazon.com/vpc/latest/userguide/managed-prefix-lists.html a VPC prefix list is created in the transit account with all of the internal IP address ranges, and then shared to all of the other accounts using AWS Resource Access Manager. This allows for central management of the IP address ranges, and eliminates the need for manual updates to security group rules in each account. This solution also allows for compliance checks to be run using AWS Config and for any non-compliant security groups to be automatically remediated.
QUESTION NO: 19 통신 회사는 AWS에서 애플리케이션을 실행하고 있습니다. 회사는 회사의 온프레미스 데이터 센터와 AWS 간에 AWS Direct Connect 연결을 설정했습니다. 회사는 내부 Application Load Balancer(ALB) 뒤에 있는 여러 가용 영역의 Amazon EC2 인스턴스에 애플리케이션을 배포했습니다. 회사의 클라이언트는 HTTPS를 사용하여 온프레미스 네트워크에서 연결합니다. TLS는 ALB에서 종료됩니다. 회사에는 여러 대상 그룹이 있으며 경로 기반 라우팅을 사용하여 URL 경로를 기반으로 요청을 전달합니다. 회사는 IP 주소를 기반으로 하는 허용 목록을 사용하여 온프레미스 방화벽 어플라이언스를 배포할 계획입니다. 솔루션 설계자는 클라이언트가 애플리케이션에 계속 액세스할 수 있도록 온프레미스 네트워크에서 AWS로의 트래픽 흐름을 허용하는 솔루션을 개발해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 고정 IP 주소를 사용하도록 기존 ALB를 구성합니다. 여러 가용 영역의 IP 주소를 ALB에 할당합니다. 방화벽 어플라이언스에 ALB IP 주소를 추가합니다. B. NLB(Network Load Balancer)를 생성합니다. NLB를 여러 가용 영역에 있는 하나의 정적 IP 주소와 연결합니다. NLB에 대한 ALB 유형 대상 그룹을 생성하고 방화벽 어플라이언스에 기존 ALAdd the NLB IP 주소를 추가합니다. NLB에 연결하도록 클라이언트를 업데이트합니다. C. NLB(Network Load Balancer)를 생성합니다. LNB를 여러 가용 영역에 있는 하나의 고정 IP 주소와 연결합니다. 기존 대상 그룹을 NLB에 추가하십시오. NLB에 연결하도록 클라이언트를 업데이트합니다. ALB 삭제 방화벽 어플라이언스에 NLB IP 주소를 추가합니다. D. 게이트웨이 로드 밸런서(GWLB)를 생성합니다. 여러 가용 영역에서 GWLB에 고정 IP 주소를 할당합니다. GWLB에 대한 ALB 유형 대상 그룹을 생성하고 기존 ALB를 추가합니다. 방화벽 어플라이언스에 GWLB IP 주소를 추가합니다. GWLB에 연결하도록 클라이언트를 업데이트합니다. Answer: B Explanation: The company should create a Network Load Balancer (NLB) and associate it with one static IP address in multiple Availability Zones. The company should also create an ALB-type target group for the NLB and add the existing ALB. The company should add the NLB IP addresses to the firewall appliance and update the clients to connect to the NLB. This solution will allow traffic flow to AWS from the on-premises network by using static IP addresses that can be added to the firewall appliance's allow list. The NLB will forward requests to the ALB, which will use path-based routing to forward requests to the target groups.
QUESTION NO: 20 회사에서 Amazon RDS for MySQL 데이터베이스를 사용하여 데이터를 저장하는 중요한 애플리케이션을 실행하고 있습니다. RDS DB 인스턴스는 다중 AZ 모드로 배포됩니다. 최근 RDS 데이터베이스 장애 조치 테스트로 인해 애플리케이션이 40초 중단되었습니다. 솔루션 설계자는 중단 시간을 20초 미만으로 줄이기 위한 솔루션을 설계해야 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까? (3개를 선택하세요.) A. 데이터베이스 앞에서 Memcached용 Amazon ElastiCache 사용 B. 데이터베이스 앞에서 Redis용 Amazon ElastiCache를 사용합니다. C. 데이터베이스 앞에서 RDS 프록시 사용 D. 데이터베이스를 Amazon Aurora MySQL로 마이그레이션 E. Amazon Aurora 복제본 생성 F. MySQL용 RDS 읽기 전용 복제본 생성 Answer: C,D,E Explanation: Migrate the database to Amazon Aurora MySQL. - Create an Amazon Aurora Replica. - Us e RDS Proxy in front of the database. - These options are correct because they address the requirement of reducing the failover time to less than 20 seconds. Migrating to Amazon Aurora MySQL and creating an Aurora replica can reduce the failover time to less than 20 seconds. Aurora has a built-in, fault-tolerant storage system that can automatically detect and repair failures. Additionally, Aurora has a feature called "Aurora Global Database" which allows you to create read-only replicas across multiple AWS regions which can further help to reduce the failover time. Creating an Aurora replica can also help to reduce the failover time as it can take over as the primary DB instance in case of a failure. Using RDS proxy can also help to reduce the failover time as it can route the queries to the healthy DB instance, it also helps to balance the load across multiple DB instances.
QUESTION NO: 21 생명 과학 회사는 오픈 소스 도구의 조합을 사용하여 데이터 분석 워크플로를 관리하고 온프레미스 데이터 센터의 서버에서 실행되는 Docker 컨테이너를 사용하여 게놈 데이터를 처리합니다. 시퀀싱 데이터가 생성되어 로컬 SAN(Storage Area Network)에 저장됩니다. 그런 다음 데이터가 처리됩니다. 연구 및 개발 팀은 용량 문제에 직면해 워크로드 수요에 따라 확장하고 처리 시간을 몇 주에서 며칠로 단축하기 위해 AWS에서 게놈 분석 플랫폼을 재설계하기로 결정했습니다. 회사에는 고속 AWS Direct Connect 연결 시퀀서가 있습니다. 각 게놈에 대해 약 200GB의 데이터를 생성하며 개별 작업은 이상적인 컴퓨팅 용량으로 데이터를 처리하는 데 몇 시간이 걸릴 수 있습니다. 최종 결과는 Amazon S3에 저장됩니다. A. 정기적으로 예약된 AWS Snowball Edge 장치를 사용하여 시퀀싱 데이터를 AWS로 전송합니다. AWS가 Snowball Edge 장치를 수신하고 데이터가 Amazon S3에 로드되면 S3 이벤트를 사용하여 데이터를 처리하는 AWS Lambda 함수를 트리거합니다. B. AWS Data Pipeline을 사용하여 시퀀싱 데이터를 Amazon S3로 전송합니다. S3 이벤트를 사용하여 Amazon EC2 Auto Scaling 그룹을 트리거하여 Docker 컨테이너를 실행하는 사용자 지정 AMI EC2 인스턴스를 시작하여 데이터를 처리합니다. C. AWS DataSync를 사용하여 시퀀싱 데이터를 Amazon S3로 전송합니다. S3 이벤트를 사용하여 AWS Step Functions 워크플로를 시작하는 AWS Lambda 함수를 트리거합니다. Docker 이미지를 Amazon Elastic Container Registry(Amazon ECR)에 저장하고 AWS Batch를 실행하도록 트리거합니다. 컨테이너 및 시퀀싱 데이터 처리 D. AWS Storage Gateway 파일 게이트웨이를 사용하여 시퀀싱 데이터를 Amazon S3로 전송합니다. S3 이벤트를 사용하여 Docker 컨테이너를 실행하는 Amazon EC2 인스턴스에서 실행되는 AWS Batch 작업을 트리거하여 데이터를 처리합니다. Answer: C Explanation: AWS DataSync can be used to transfer the sequencing data to Amazon S3, which is a more efficient and faster method than using Snowball Edge devices. Once the data is in S3, S3 events can trigger an AWS Lambda function that starts an AWS Step Functions workflow. The Docker images can be stored in Amazon Elastic Container Registry (Amazon ECR) and AWS Batch can be used to run the container and process the sequencing data.
QUESTION NO: 22 한 회사가 양식 처리 애플리케이션을 AWS로 마이그레이션했습니다. 사용자가 애플리케이션과 상호 작용할 때 웹 애플리케이션을 통해 스캔한 양식을 파일로 업로드합니다. 데이터베이스는 사용자 메타데이터와 Amazon S3에 저장된 파일에 대한 참조를 저장합니다. 웹 애플리케이션은 Amazon EC2 인스턴스와 Amazon RDS for PostgreSQL 데이터베이스에서 실행됩니다. 양식이 업로드되면 애플리케이션은 Amazon Simple Notification Service(Amazon SNS)를 통해 팀에 알림을 보냅니다. 그러면 팀원이 로그인하여 각 양식을 처리합니다. 팀원은 API를 사용하는 다른 시스템에 정보를 입력하기 전에 양식에서 데이터 유효성 검사를 수행하고 관련 데이터를 추출합니다. 솔루션 설계자는 양식의 수동 처리를 자동화해야 합니다. 이 솔루션은 정확한 양식 추출을 제공하고 시장 출시 시간을 최소화하며 장기적인 운영 오버헤드를 최소화해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 양식에서 광학 문자 인식(OCR)을 수행하기 위한 맞춤형 라이브러리를 개발하십시오. 라이브러리를 Amazon Elastic Kubernetes Service(Amazon EKS) 클러스터에 애플리케이션 계층으로 배포합니다. 양식이 업로드될 때 이 계층을 사용하여 양식을 처리하십시오. 출력을 Amazon S3에 저장합니다. 데이터를 Amazon DynamoDB 테이블로 추출하여 이 출력을 구문 분석합니다. 대상 시스템의 API에 데이터를 제출합니다. EC2 인스턴스에서 새 애플리케이션 계층을 호스팅합니다. B. AWS Step Functions 및 AWS Lambda를 사용하는 애플리케이션 계층으로 시스템을 확장합니다. 양식이 업로드될 때 양식에서 OCR(광학 문자 인식)을 수행하기 위해 EC2 인스턴스에서 훈련되고 호스팅되는 인공 지능 및 기계 학습(AI/ML) 모델을 사용하도록 이 계층을 구성합니다. 출력을 Amazon S3에 저장합니다. 애플리케이션 계층 내에서 필요한 데이터를 추출하여 이 출력을 구문 분석하십시오. 대상 시스템의 API에 데이터를 제출하십시오. C. EC2 인스턴스에서 새 애플리케이션 계층을 호스팅합니다. 양식에서 광학 문자 인식(OCR)을 수행하기 위해 Amazon SageMaker에서 훈련 및 호스팅되는 인공 지능 및 기계 학습(Al/ML) 모델을 호스팅하는 엔드포인트를 호출하려면 이 계층을 사용하십시오. 출력을 Amazon ElastiCache에 저장합니다. 애플리케이션 계층 내에서 필요한 데이터를 추출하여 이 출력을 구문 분석하십시오. 대상 시스템의 API에 데이터를 제출하십시오. D. AWS Step Functions 및 AWS Lambda를 사용하는 애플리케이션 계층으로 시스템을 확장합니다. 양식이 업로드될 때 Amazon Textract 및 Amazon Comprehend를 사용하여 양식에서 OCR(광학 문자 인식)을 수행하도록 이 계층을 구성합니다. 출력을 Amazon S3에 저장합니다. 애플리케이션 계층 내에서 필요한 데이터를 추출하여 이 출력을 구문 분석하십시오. 대상 시스템의 API에 데이터를 제출하십시오. Answer: D Explanation: Extend the system with an application tier that uses AWS Step Functions and AWS Lambda. Configure this tier to use Amazon Textract and Amazon Comprehend to perform optical character recognition (OCR) on the forms when forms are uploaded. Store the output in Amazon S3. Parse this output by extracting the data that is required within the application tier. Submit the data to the target system's API. This solution meets the requirements of accurate form extraction, minimal time to market, and minimal long-term operational overhead. Amazon Textract and Amazon Comprehend are fully managed and serverless services that can perform OCR and extract relevant data from the forms, which eliminates the need to develop custom libraries or train and host models. Using AWS Step Functions and Lambda allows for easy automation of the process and the ability to scale as needed.
QUESTION NO: 23 회사에는 AWS Organizations에 조직이 있습니다. 이 회사는 AWS Control Tower를 사용하여 조직의 랜딩 존을 배포하고 있습니다. 회사는 거버넌스 및 정책 집행을 구현하려고 합니다. 회사는 회사의 프로덕션 OU에서 유휴 상태에서 암호화되지 않은 Amazon RDS DB 인스턴스를 감지하는 정책을 구현해야 합니다. 이 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS Control Tower에서 필수 가드레일을 켭니다. 프로덕션 OU에 필수 가드레일을 적용합니다. B. AWS Control Tower의 강력 권장 가드레일 목록에서 적절한 가드레일을 활성화합니다. 프로덕션 OU에 가드레일을 적용합니다. C. AWS Config를 사용하여 새로운 필수 가드레일을 생성합니다. 프로덕션 OU의 모든 계정에 규칙을 적용합니다. D. AWS Control Tower에서 사용자 지정 SCP를 생성합니다. 프로덕션 OU에 SCP를 적용합니다. Answer: B Explanation: AWS Control Tower provides a set of "strongly recommended guardrails" that can be enabled to implement governance and policy enforcement. One of these guardrails is "Encrypt Amazon RDS instances" which will detect RDS DB instances that are not encrypted at rest. By enabling this guardrail and applying it to the production OU, the company will be able to enforce encryption for RDS instances in the production environment.
QUESTION NO: 24 솔루션 설계자는 회사의 온프레미스 데이터 센터를 AWS 클라우드로 마이그레이션하기 위한 비즈니스 사례를 만들어야 합니다. 솔루션 설계자는 모든 회사 서버의 구성 관리 데이터베이스(CMDB) 내보내기를 사용하여 사례를 생성합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까? A. 분석을 수행하고 권장 사항을 생성하기 위해 AWS Well-Architected Tool을 사용하여 CMDB 데이터를 가져옵니다. B. Migration Evaluator를 사용하여 분석을 수행합니다. 데이터 가져오기 템플릿을 사용하여 CMDB 내보내기에서 데이터를 업로드합니다. C. 리소스 일치 규칙을 구현합니다. CMDB 내보내기 및 AWS Price List Bulk API를 사용하여 AWS 서비스에 대해 CMDB 데이터를 대량으로 쿼리합니다. D. 분석을 수행하기 위해 AWS Application Discovery Service를 사용하여 CMDB 데이터를 가져옵니다. Answer: B Explanation: https://aws.amazon.com/blogs/architecture/accelerating-your-migration-to-aws/ Build a business case with AWS Migration Evaluator The foundation for a successful migration starts with a defined business objective (for example, growth or new offerings). In order to enable the business drivers, the established business case must then be aligned to a technical capability (increased security and elasticity). AWS Migration Evaluator (formerly known as TSO Logic) can help you meet these objectives. To get started, you can choose to upload exports from third-party tools such as Configuration Management Database (CMDB) or install a collector agent to monitor. You will receive an assessment after data collection, which includes a projected cost estimate and savings of running your on-premises workloads in the AWS Cloud. This estimate will provide a summary of the projected costs to re-host on AWS based on usage patterns. It will show the breakdown of costs by infrastructure and software licenses. With this information, you can make the business case and plan next steps.
QUESTION NO: 25 회사가 AWS 클라우드에서 애플리케이션을 실행하고 있습니다. 핵심 비즈니스 로직은 Auto Scaling 그룹의 Amazon EC2 인스턴스 세트에서 실행됩니다. ALB(Application Load Balancer)는 EC2 인스턴스에 트래픽을 분산합니다. Amazon Route 53 레코드 api.example.com이 ALB를 가리키고 있습니다. 회사의 개발 팀은 비즈니스 논리를 크게 업데이트합니다. 회사에는 변경 사항이 배포되면 테스트 기간 동안 고객의 10%만 새로운 논리를 받을 수 있다는 규칙이 있습니다. 고객은 테스트 기간 동안 동일한 버전의 비즈니스 로직을 사용해야 합니다. 회사는 이러한 요구 사항을 충족하기 위해 업데이트를 어떻게 배포해야 합니까? A. 두 번째 ALB를 생성하고 새 Auto Scaling 그룹의 EC2 인스턴스 집합에 새 논리를 배포합니다. EC2 인스턴스에 트래픽을 분산하도록 ALB를 구성합니다. 가중치 기반 라우팅을 사용하도록 Route 53 레코드를 업데이트하고 레코드가 두 ALB를 가리키도록 합니다. B. ALB에서 참조하는 두 번째 대상 그룹을 생성합니다. 이 새 대상 그룹의 EC2 인스턴스에 새 논리를 배포합니다. 가중 대상 그룹을 사용하도록 ALB 리스너 규칙을 업데이트합니다. ALB 대상 그룹 고정성을 구성합니다. C. Auto Scaling 그룹에 대한 새 시작 구성을 생성합니다. AutoScaIingRoIIingUpdate 정책을 사용하도록 시작 구성을 지정하고 MaxBatchSize 옵션을 10으로 설정합니다. Auto Scaling 그룹에서 시작 구성을 교체합니다. 변경 사항을 배포합니다. D. ALB에서 참조하는 두 번째 Auto Scaling 그룹을 생성합니다. 이 새 Auto Scaling 그룹의 EC2 인스턴스 집합에 새 논리를 배포합니다. ALB 라우팅 알고리즘을 최소 미해결 요청(LOR)으로 변경합니다. ALB 세션 고정성을 구성합니다. Answer: B Explanation: The company should create a second target group that is referenced by the ALB. The company should deploy the new logic to EC2 instances in this new target group. The company should update the ALB listener rule to use weighted target groups. The company should configure ALB target group stickiness. This solution will meet the requirements because weighted target groups are a feature that enables you to distribute traffic across multiple target groups using a single listener rule. You can specify a weight for each target group, which determines the percentage of requests that are routed to that target group. For example, if you specify two target groups, each with a weight of 10, each target group receives half the requests1. By creating a second target group and deploying the new logic to EC2 instances in this new target group, the company can have two versions of its business logic running in parallel. By updating the ALB listener rule to use weighted target groups, the company can control how much traffic is sent to each version. By configuring ALB target group stickiness, the company can ensure that a customer uses the same version of the business logic during the testing window. Target group stickiness is a feature that enables you to bind a user's session to a specific target within a target group for the duration of the session2. The other options are not correct because: Creating a second ALB and deploying the new logic to a set of EC2 instances in a new Auto Scaling group would not be as cost-effective or simple as using weighted target groups. A second ALB would incur additional charges and require more configuration and management. Updating the Route 53 record to use weighted routing would not ensure that a customer uses the same version of the business logic during the testing window, as DNS caching could affect how requests are routed. Creating a new launch configuration for the Auto Scaling group and replacing it on the Auto Scaling group would not allow for gradual traffic shifting between versions. A launch configuration is a template that an Auto Scaling group uses to launch EC2 instances. You can specify information such as the AMI ID, instance type, key pair, security groups, and block device mapping for your instances3. However, replacing the launch configuration on an Auto Scaling group would affect all instances in that group, not just 10% of customers. Creating a second Auto Scaling group and changing the ALB routing algorithm to least outstanding requests (LOR) would not allow for controlled traffic shifting between versions. A second Auto Scaling group would require more configuration and management. The LOR routing algorithm is a feature that enables you to route traffic based on how quickly targets respond to requests. The load balancer selects a target from the target group with the fewest outstanding requests4. However, this algorithm does not take into account customer sessions or weights. Reference: https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancerlisteners.html#listener-rules-weighted-target-groups https://docs.aws.amazon.com/elasticloadbalancing/latest/application/sticky-sessions.html https://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html https://docs.aws.amazon.com/elasticloadbalancing/latest/application/load-balancer-target-groups.html#routing-algorithm
QUESTION NO: 26 한 회사에 여러 AWS 계정이 있습니다. 개발 팀은 클라우드 거버넌스 및 문제 해결 프로세스를 위한 자동화 프레임워크를 구축하고 있습니다. 자동화 프레임워크는 중앙 집중식 계정에서 AWS Lambda 함수를 사용합니다. 솔루션 설계자는 Lambda 함수가 회사의 각 AWS 계정에서 실행될 수 있도록 허용하는 최소 권한 정책을 구현해야 합니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개 선택하세요.) A. 중앙 집중식 계정에서 Lambda 서비스를 신뢰할 수 있는 엔터티로 포함하는 IAM 역할을 생성합니다. 다른 AWS 계정의 역할을 맡도록 인라인 정책을 추가합니다. B. 다른 AWS 계정에서 최소 권한을 가진 IAM 역할을 생성합니다. 중앙 집중식 계정의 Lambda IAM 역할을 신뢰할 수 있는 엔터티로 추가합니다. C. 중앙 집중식 계정에서 다른 계정의 역할을 신뢰할 수 있는 엔터티로 갖는 IAM 역할을 생성합니다. 최소한의 권한을 제공합니다. D. 다른 AWS 계정에서 중앙 집중식 계정의 역할을 맡을 수 있는 권한이 있는 IAM 역할을 생성합니다. Lambda 서비스를 신뢰할 수 있는 엔터티로 추가합니다. E. 다른 AWS 계정에서 최소 권한이 있는 IAM 역할을 생성합니다. Lambda 서비스를 신뢰할 수 있는 엔터티로 추가합니다. Answer: A,B Explanation: https://medium.com/@it.melnichenko/invoke-a-lambda-across-multiple-aws-accounts-8c094b2e70be
QUESTION NO: 27 회사에는 Application Load Balancer(ALB) 뒤에 있는 Amazon EC2 인스턴스 플릿에서 실행되는 다중 계층 웹 애플리케이션이 있습니다. 인스턴스는 Auto Scaling 그룹에 있습니다. ALB 및 Auto Scaling 그룹은 백업 AWS 리전에서 복제됩니다. Auto Scaling 그룹의 최소값과 최대값은 0으로 설정됩니다. Amazon RDS 다중 AZ DB 인스턴스는 애플리케이션의 데이터를 저장합니다. DB 인스턴스에는 백업 리전에 읽기 전용 복제본이 있습니다. 애플리케이션은 Amazon Route 53 레코드를 사용하여 최종 사용자에게 엔드포인트를 제공합니다. 회사는 애플리케이션에 백업 리전으로 자동 장애 조치할 수 있는 기능을 제공하여 RTO를 15분 미만으로 줄여야 합니다. 회사는 활성-활성 전략을 위한 충분한 예산이 없습니다. 이러한 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 권장해야 합니까? A. 두 ALB 간에 트래픽 부하를 분산하는 지연 시간 기반 라우팅 정책으로 애플리케이션의 Route 53 레코드를 재구성합니다. 백업 리전에서 AWS Lambda 함수를 생성하여 읽기 전용 복제본을 승격하고 Auto Scaling 그룹 값을 수정합니다. 기본 리전의 ALB에 대한 HTTPCode_Target_5XX_Count 지표를 기반으로 하는 Amazon CloudWatch 경보를 생성합니다. Lambda 함수를 호출하도록 CloudWatch 경보를 구성합니다. B. 백업 리전에서 AWS Lambda 함수를 생성하여 읽기 전용 복제본을 승격하고 Auto Scaling 그룹 값을 수정합니다. 웹 애플리케이션을 모니터링하고 상태 확인 상태가 비정상일 때 Lambda 함수에 Amazon Simple Notification Service(Amazon SNS) 알림을 보내는 상태 확인으로 Route 53을 구성합니다. 상태 확인 실패가 발생할 때 백업 리전의 ALB로 트래픽을 라우팅하는 장애 조치 정책으로 애플리케이션의 Route 53 레코드를 업데이트합니다. C. 기본 리전의 Auto Scaling 그룹과 동일한 값을 갖도록 백업 리전의 Auto Scaling 그룹을 구성합니다. 두 ALB 간에 트래픽 부하를 분산하는 지연 시간 기반 라우팅 정책으로 애플리케이션의 Route 53 레코드를 재구성합니다. 읽기 복제본을 제거합니다. 읽기 전용 복제본을 독립 실행형 RDS DB 인스턴스로 교체합니다. 스냅샷과 Amazon S3를 사용하여 RDS DB 인스턴스 간에 교차 리전 복제를 구성합니다. D. 두 개의 ALB를 동일한 가중치 대상으로 사용하여 AWS Global Accelerator에서 엔드포인트를 구성합니다. 백업 리전에서 AWS Lambda 함수를 생성하여 읽기 전용 복제본을 승격하고 Auto Scaling 그룹 값을 수정합니다. 기본 리전의 ALB에 대한 HTTPCode_Target_5XX_Count 지표를 기반으로 하는 Amazon CloudWatch 경보를 생성합니다. Lambda 함수를 호출하도록 CloudWatch 경보를 구성합니다. Answer: B Explanation: an AWS Lambda function in the backup region to promote the read replica and modify the Auto Scaling group values, and then configuring Route 53 with a health check that monitors the web application and sends an Amazon SNS notification to the Lambda function when the health check status is unhealthy. Finally, the application's Route 53 record should be updated with a failover policy that routes traffic to the ALB in the backup region when a health check failure occurs. This approach provides automatic failover to the backup region when a health check failure occurs, reducing the RTO to less than 15 minutes. Additionally, this approach is cost-effective as it does not require an active-active strategy.
QUESTION NO: 28 회사에서 문서 처리 워크로드를 AWS로 마이그레이션하고 있습니다. 이 회사는 기본적으로 Amazon S3 API를 사용하여 처리 서버가 초당 약 5개의 문서 속도로 생성하는 문서를 저장, 검색 및 수정하도록 많은 애플리케이션을 업데이트했습니다. 문서 처리가 완료되면 고객은 Amazon S3에서 직접 문서를 다운로드할 수 있습니다. 마이그레이션 과정에서 회사는 S3 API를 지원하기 위해 많은 문서를 생성하는 처리 서버를 즉시 업데이트할 수 없음을 발견했습니다. 서버는 Linux에서 실행되며 서버가 생성하고 수정하는 파일에 대한 빠른 로컬 액세스가 필요합니다. 서버가 처리를 마치면 파일을 30분 이내에 다운로드할 수 있도록 대중이 사용할 수 있어야 합니다. 최소한의 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 애플리케이션을 AWS Lambda 함수로 마이그레이션합니다. Java용 AWS SDK를 사용하여 회사에서 Amazon S3에 직접 저장하는 파일을 생성, 수정 및 액세스합니다. B. Amazon S3 파일 게이트웨이를 설정하고 문서 저장소에 연결된 파일 공유를 구성합니다. NFS를 사용하여 Amazon EC2 인스턴스에 파일 공유를 탑재합니다. Amazon S3에서 변경 사항이 발생하면 RefreshCache API 호출을 시작하여 S3 파일 게이트웨이를 업데이트합니다. C. 가져오기 및 내보내기 정책으로 Amazon FSx for Lustre를 구성합니다. 새 파일 시스템을 S3 버킷에 연결합니다. NFS를 사용하여 Lustre 클라이언트를 설치하고 문서 저장소를 Amazon EC2 인스턴스에 마운트합니다. D. Amazon EC2 인스턴스에 연결하도록 AWS DataSync를 구성합니다. 생성된 파일을 Amazon S3와 동기화하도록 작업을 구성합니다. Answer: C Explanation: The company should configure Amazon FSx for Lustre with an import and export policy. The company should link the new file system to an S3 bucket. The company should install the Lustre client and mount the document store to an Amazon EC2 instance by using NFS. This solution will meet the requirements with the least amount of effort because Amazon FSx for Lustre is a fully managed service that provides a high-performance file system optimized for fast processing of workloads such as machine learning, high performance computing, video processing, financial modeling, and electronic design automation1. Amazon FSx for Lustre can be linked to an S3 bucket and can import data from and export data to the bucket2. The import and export policy can be configured to automatically import new or changed objects from S3 and export new or changed files to S33. This will ensure that the files are available to the public for download within 30 minutes. Amazon FSx for Lustre supports NFS version 3.0 protocol for Linux clients. The other options are not correct because: Migrating the application to an AWS Lambda function would require a lot of effort and may not be feasible for the existing server that generates many documents. Lambda functions have limitations on execution time, memory, disk space, and network bandwidth. Setting up an Amazon S3 File Gateway would not work because S3 File Gateway does not support write-back caching, which means that files written to the file share are uploaded to S3 immediately and are not available locally until they are downloaded again. This would not provide fast local access to the files that the server generates and modifies. Configuring AWS DataSync to connect to an Amazon EC2 instance would not meet the requirement of making the files available to the public for download within 30 minutes. DataSync is a service that transfers data between on-premises storage systems and AWS storage services over the internet or AWS Direct Connect. DataSync tasks can be scheduled to run at specific times or intervals, but they are not triggered by file changes. Reference: https://aws.amazon.com/fsx/lustre/ https://docs.aws.amazon.com/fsx/latest/LustreGuide/create-fs-linked-data-repo.html https://docs.aws.amazon.com/fsx/latest/LustreGuide/import-export-data-repositories.html https://docs.aws.amazon.com/fsx/latest/LustreGuide/mounting-on-premises.html https://docs.aws.amazon.com/lambda/latest/dg/gettingstarted-limits.html https://docs.aws.amazon.com/storagegateway/latest/userguide/StorageGatewayConcepts.html https://docs.aws.amazon.com/datasync/latest/userguide/what-is-datasync.html
QUESTION NO: 29 회사에서 Amazon Connect를 사용하여 콜 센터를 구축하고 있습니다. 회사의 운영 팀은 AWS 리전 전체에서 재해 복구(DR) 전략을 정의하고 있습니다. 컨택 센터에는 수십 개의 고객 응대 흐름, 수백 명의 사용자 및 수십 개의 청구된 전화 번호가 있습니다. 가장 낮은 RTO로 DR을 제공하는 솔루션은 무엇입니까? A. Amazon Connect 인스턴스의 가용성을 확인하고 사용할 수 없는 경우 운영 팀에 알림을 보내는 AWS Lambda 함수를 생성합니다. 5분마다 Lambda 함수를 호출하는 Amazon EventBridge 규칙을 생성합니다. 알림 후 운영 팀에 AWS Management 콘솔을 사용하여 두 번째 리전에서 새 Amazon Connect 인스턴스를 프로비저닝하도록 지시합니다. AWS CloudFormation 템플릿을 사용하여 고객 응대 흐름, 사용자 및 청구된 전화번호를 배포합니다. B. 두 번째 리전의 모든 기존 사용자와 함께 새 Amazon Connect 인스턴스를 프로비저닝합니다. Amazon Connect 인스턴스의 가용성을 확인하는 AWS Lambda 함수를 생성합니다. 5분마다 Lambda 함수를 호출하는 Amazon EventBridge 규칙을 생성합니다. 문제가 발생하면 두 번째 리전에서 고객 응대 흐름과 청구 번호를 프로비저닝하는 AWS CloudFormation 템플릿을 배포하도록 Lambda 함수를 구성하십시오. C. 두 번째 리전에서 모든 기존 고객 응대 흐름 및 청구된 전화번호로 새 Amazon Connect 인스턴스를 프로비저닝합니다. Amazon Connect 인스턴스의 URL에 대한 Amazon Route 53 상태 확인을 생성합니다. 실패한 상태 확인에 대한 Amazon CloudWatch 경보를 생성합니다. 모든 사용자를 프로비저닝하는 AWS CloudFormation 템플릿을 배포하는 AWS Lambda 함수를 생성합니다. Lambda 함수를 호출하도록 경보를 구성합니다. D. 두 번째 리전의 모든 기존 사용자 및 고객 응대 흐름으로 새 Amazon Connect 인스턴스를 프로비저닝합니다. Amazon Connect 인스턴스의 URL에 대한 Amazon Route 53 상태 확인을 생성합니다. 실패한 상태 확인에 대한 Amazon CloudWatch 경보를 생성합니다. 청구된 전화번호를 프로비저닝하는 AWS CloudFormation 템플릿을 배포하는 AWS Lambda 함수를 생성합니다. Lambda 함수를 호출하도록 경보를 구성합니다. Answer: D Explanation: Option D provisions a new Amazon Connect instance with all existing users and contact flows in a second Region. It also sets up an Amazon Route 53 health check for the URL of the Amazon Connect instance, an Amazon CloudWatch alarm for failed health checks, and an AWS Lambda function to deploy an AWS CloudFormation template that provisions claimed phone numbers. This option allows for the fastest recovery time because all the necessary components are already provisioned and ready to go in the second Region. In the event of a disaster, the failed health check will trigger the AWS Lambda function to deploy the CloudFormation template to provision the claimed phone numbers, which is the only missing component.
QUESTION NO: 30 솔루션 설계자는 새로 인수한 회사의 애플리케이션 및 데이터베이스 포트폴리오를 평가해야 합니다. 솔루션 설계자는 포트폴리오를 AWS로 마이그레이션하기 위한 비즈니스 사례를 생성해야 합니다. 새로 인수한 회사는 온프레미스 데이터 센터에서 애플리케이션을 실행합니다. 데이터 센터는 잘 문서화되어 있지 않습니다. 솔루션 설계자는 존재하는 애플리케이션과 데이터베이스의 수를 즉시 확인할 수 없습니다. 애플리케이션의 트래픽은 가변적입니다. 일부 응용 프로그램은 매달 말에 실행되는 배치 프로세스입니다. 솔루션 설계자는 AWS로의 마이그레이션을 시작하기 전에 포트폴리오를 더 잘 이해해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS Server Migration Service(AWS SMS) 및 AWS Database Migration Service(AWS DMS)를 사용하여 마이그레이션을 평가합니다. AWS Service Catalog를 사용하여 애플리케이션 및 데이터베이스 종속성을 이해합니다. B. AWS 애플리케이션 마이그레이션 서비스를 사용합니다. 온프레미스 인프라에서 에이전트를 실행합니다. AWS Migration Hub를 사용하여 에이전트를 관리합니다. AWS Storage Gateway를 사용하여 로컬 스토리지 요구 사항 및 데이터베이스 종속성을 평가합니다. C. Migration Evaluator를 사용하여 서버 목록을 생성합니다. 비즈니스 사례에 대한 보고서를 작성합니다. 포트폴리오를 보려면 AWS Migration Hub를 사용하십시오. AWS Application Discovery Service를 사용하여 애플리케이션 종속성을 이해합니다. D. 대상 계정에서 AWS Control Tower를 사용하여 애플리케이션 포트폴리오를 생성합니다. AWS Server Migration Service(AWS SMS)를 사용하여 더 심층적인 보고서와 비즈니스 사례를 생성합니다. 핵심 계정 및 리소스에 랜딩 존을 사용합니다. Answer: C Explanation: The company should use Migration Evaluator to generate a list of servers and build a report for a business case. The company should use AWS Migration Hub to view the portfolio and use AWS Application Discovery Service to gain an understanding of application dependencies. This solution will meet the requirements because Migration Evaluator is a migration assessment service that helps create a data-driven business case for AWS cloud planning and migration. Migration Evaluator provides a clear baseline of what the company is running today and projects AWS costs based on measured on-premises provisioning and utilization1. Migration Evaluator can use an agentless collector to conduct broad-based discovery or securely upload exports from existing inventory tools2. Migration Evaluator integrates with AWS Migration Hub, which is a service that provides a single location to track the progress of application migrations across multiple AWS and partner solutions3. Migration Hub also supports AWS Application Discovery Service, which is a service that helps systems integrators quickly and reliably plan application migration projects by automatically identifying applications running in on-premises data centers, their associated dependencies, and their performance profile4. https://aws.amazon.com/migration-evaluator/ https://docs.aws.amazon.com/migration-evaluator/latest/userguide/what-is.html https://aws.amazon.com/migration-hub/ https://aws.amazon.com/application-discovery/ https://aws.amazon.com/server-migration-service/ https://aws.amazon.com/dms/ https://docs.aws.amazon.com/controltower/latest/userguide/what-is-control-tower.html https://aws.amazon.com/application-migration-service/ https://aws.amazon.com/storagegateway/
QUESTION NO: 31 회사가 VPC에서 웹 애플리케이션을 실행하고 있습니다. 웹 애플리케이션은 ALB(Application Load Balancer) 뒤의 Amazon EC2 인스턴스 그룹에서 실행됩니다. ALB는 AWS WAF를 사용하고 있습니다. 외부 고객은 웹 애플리케이션에 연결해야 합니다. 회사는 모든 외부 고객에게 IP 주소를 제공해야 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. ALB를 NLB(Network Load Balancer)로 교체합니다. NLB에 탄력적 IP 주소를 할당합니다. B. 탄력적 IP 주소를 할당합니다. 탄력적 IP 주소를 ALP에 할당 고객에게 탄력적 IP 주소를 제공합니다. C. AWS Global Accelerator 표준 가속기를 생성합니다. 가속기의 끝점으로 ALB를 지정합니다. 액셀러레이터의 IP 주소를 고객에게 제공합니다. D. Amazon CloudFront 배포를 구성합니다. ALB를 원점으로 설정합니다. 배포의 DNS 이름을 ping하여 배포의 퍼블릭 IP 주소를 확인합니다. 고객에게 IP 주소를 제공합니다. Answer: C Explanation: https://docs.aws.amazon.com/global-accelerator/latest/dg/about-accelerators.albaccelerator.html Option A is wrong. AWS WAF does not support associating with NLB. https://docs.aws.amazon.com/waf/latest/developerguide/waf-chapter.html Option B is wrong. An ALB does not support an Elastic IP address. https://aws.amazon.com/elasticloadbalancing/features/
QUESTION NO: 32 회사는 두 개의 개별 비즈니스 단위로 구성됩니다. 각 사업부는 AWS Organizations의 단일 조직 내에 자체 AWS 계정을 가지고 있습니다. 비즈니스 단위는 정기적으로 중요한 문서를 서로 공유합니다. 공유를 용이하게 하기 위해 회사는 각 계정에 Amazon S3 버킷을 생성하고 S3 버킷 간에 양방향 복제를 구성했습니다. S3 버킷에는 수백만 개의 객체가 있습니다. 최근 보안 감사에서 두 S3 버킷 모두 유휴 암호화가 활성화되어 있지 않은 것으로 확인되었습니다. 회사 정책에 따라 모든 문서는 저장 시 암호화된 상태로 저장되어야 합니다. 회사는 Amazon S3 관리형 암호화 키(SSE-S3)로 서버 측 암호화를 구현하려고 합니다. 이러한 요구 사항을 충족하는 운영상 가장 효율적인 솔루션은 무엇입니까? A. 두 S3 버킷에서 SSE-S3를 켭니다. S3 배치 작업을 사용하여 동일한 위치에서 객체를 복사하고 암호화합니다. B. 각 계정에서 AWS Key Management Service(AWS KMS) 키를 생성합니다. 해당 AWS 계정에서 해당 KMS 키를 사용하여 각 S3 버킷에서 AWS KMS 키(SSE-KMS)로 서버 측 암호화를 켭니다. AWS CLI에서 S3 복사 명령을 사용하여 기존 객체를 암호화합니다. C. 두 S3 버킷에서 SSE-S3를 켭니다. AWS CLI에서 S3 복사 명령을 사용하여 기존 객체를 암호화합니다. D. 각 계정에서 AWS Key Management Service(AWS KMS) 키를 생성합니다. 해당 AWS 계정에서 해당 KMS 키를 사용하여 각 S3 버킷에서 AWS KMS 키(SSE-KMS)로 서버 측 암호화를 켭니다. S3 배치 작업을 사용하여 객체를 동일한 위치에 복사합니다. Answer: A Explanation: "The S3 buckets have millions of objects" If there are million of objects then you should use Batch operations. https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3- batch-operations/
QUESTION NO: 33 회사가 AWS 클라우드에서 애플리케이션을 실행하고 있습니다. 최근 애플리케이션 메트릭은 일관성 없는 응답 시간과 오류율의 상당한 증가를 보여줍니다. 타사 서비스에 대한 호출로 인해 지연이 발생합니다. 현재 애플리케이션은 AWS Lambda 함수를 직접 호출하여 타사 서비스를 동기식으로 호출합니다. 솔루션 설계자는 타사 서비스 호출을 분리하고 모든 호출이 결국 완료되도록 해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Amazon Simple Queue Service(Amazon SQS) 대기열을 사용하여 이벤트를 저장하고 Lambda 함수를 호출합니다. B. AWS Step Functions 상태 머신을 사용하여 Lambda 함수에 이벤트를 전달합니다. C. Amazon EventBridge 규칙을 사용하여 이벤트를 Lambda 함수에 전달합니다. D. Amazon Simple Notification Service(Amazon SNS) 주제를 사용하여 이벤트를 저장하고 Lambda 함수를 호출합니다. Answer: A Explanation: Using an SQS queue to store events and invoke the Lambda function will decouple the thirdparty service calls and ensure that all the calls are eventually completed. SQS allows you to store messages in a queue and process them asynchronously, which eliminates the need for the application to wait for a response from the third-party service. The messages will be stored in the SQS queue until they are processed by the Lambda function, even if the Lambda function is currently unavailable or busy. This will ensure that all the calls are eventually completed, even if there are delays or errors. AWS Step Functions state machines can also be used to pass events to the Lambda function, but it would require additional management and configuration to set up the state machine, which would increase operational overhead. Amazon EventBridge rule can also be used to pass events to the Lambda function, but it would not provide the same level of decoupling and reliability as SQS. Using Amazon Simple Notification Service (Amazon SNS) topic to store events and Invoke the Lambda function, is similar to SQS, but SNS is a publish-subscribe messaging service and SQS is a queue service. SNS is used for sending messages to multiple recipients, SQS is used for sending messages to a single recipient, so SQS is more appropriate for this use case. Reference: AWS SQS AWS Step Functions AWS EventBridge AWS SNS
QUESTION NO: 34 한 회사에서 1,000개의 온프레미스 서버를 AWS로 마이그레이션할 계획입니다. 서버는 회사 데이터 센터의 여러 VMware 클러스터에서 실행됩니다. 마이그레이션 계획의 일부로 회사는 CPU 세부 정보, RAM 사용량, 운영 체제 정보 및 실행 중인 프로세스와 같은 서버 메트릭을 수집하려고 합니다. 그런 다음 회사는 데이터를 쿼리하고 분석하려고 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 온프레미스 호스트에 AWS Agentless Discovery Connector 가상 어플라이언스를 배포하고 구성합니다. AWS Migration Hub에서 데이터 탐색을 구성합니다. AWS Glue를 사용하여 데이터에 대해 ETL 작업을 수행합니다. Amazon S3 Select를 사용하여 데이터를 쿼리합니다. B. 온프레미스 호스트에서 VM 성능 정보만 내보냅니다. 필요한 데이터를 AWS Migration Hub로 직접 가져옵니다. Migration Hub에서 누락된 정보를 업데이트합니다. Amazon QuickSight를 사용하여 데이터를 쿼리합니다. C. 온프레미스 호스트에서 서버 정보를 자동으로 수집하는 스크립트를 만듭니다. AWS CLI를 사용하여 put-resource-attributes 명령을 실행하여 자세한 서버 데이터를 AWS Migration Hub에 저장합니다. Migration Hub 콘솔에서 직접 데이터를 쿼리합니다. D. 각 온프레미스 서버에 AWS Application Discovery Agent를 배포합니다. AWS Migration Hub에서 데이터 탐색을 구성합니다. Amazon Athena를 사용하여 Amazon S3의 데이터에 대해 사전 정의된 쿼리를 실행합니다. Answer: D Explanation: it covers all the requirements mentioned in the question, it will allow collecting the detailed metrics, including process information and it provides a way to query and analyze the data using Amazon Athena.
QUESTION NO: 35 솔루션 설계자는 새 Amazon S3 버킷에 저장될 객체에 대한 클라이언트 측 암호화 메커니즘을 구현해야 합니다. 솔루션 설계자는 이를 위해 AWS Key Management Service(AWS KMS)에 저장되는 CMK를 생성했습니다. 솔루션 설계자는 다음 IAM 정책을 생성하여 IAM 역할에 연결했습니다. 테스트 중에 Me Solutions Architect는 S3 버킷에서 기존 테스트 개체를 성공적으로 가져올 수 있었습니다. 그러나 새 개체를 업로드하려고 하면 오류 메시지가 나타납니다. 오류 메시지에는 작업이 금지되어 있다고 명시되어 있습니다. 모든 요구 사항을 충족하기 위해 솔루션 설계자가 IAM 정책에 추가해야 하는 조치는 무엇입니까? A. Kms:GenerateDataKey B. KmsGetKeyPolpcy C. kmsGetPubK키 D. kms:SKjn Answer: A Explanation: https://aws.amazon.com/premiumsupport/knowledge-center/s3-access-denied-error-kms/ "An error occurred (AccessDenied) when calling the PutObject operation: Access Denied" This error message indicates that your IAM user or role needs permission for the kms:GenerateDataKey action.
QUESTION NO: 36 회사에서 AWS Organizations를 사용하여 여러 AWS 계정을 관리하고 있습니다. 보안을 위해 회사는 모든 조직 구성원 계정에서 타사 알림 시스템과 통합할 수 있는 Amazon Simple Notification Service(Amazon SNS) 항목을 생성해야 합니다. 솔루션 설계자는 AWS CloudFormation 템플릿을 사용하여 Cloud Formation 스택의 배포를 자동화하기 위한 SNS 주제 및 스택 세트를 생성했습니다. 조직에서 신뢰할 수 있는 액세스가 활성화되었습니다. 모든 AWS 계정에 CloudFormation StackSets를 배포하려면 솔루션 설계자가 무엇을 해야 합니까? A. 조직 구성원 계정에 스택 세트를 생성합니다. 서비스 관리 권한을 사용합니다. 조직에 배포할 배포 옵션을 설정합니다. CloudFormation StackSets 드리프트 감지를 사용합니다. B. 조직 구성원 계정에 스택을 생성합니다. 셀프 서비스 권한을 사용합니다. 조직에 배포할 배포 옵션을 설정합니다. CloudFormation StackSets 자동 배포를 활성화합니다. C. 조직 마스터 계정에서 스택 세트를 생성합니다. 서비스 관리 권한을 사용합니다. 조직에 배포할 배포 옵션을 설정합니다. CloudFormation StackSets 자동 배포를 활성화합니다. D. 조직 마스터 계정에서 스택을 생성합니다. 서비스 관리 권한을 사용합니다. 조직에 배포할 배포 옵션을 설정합니다. CloudFormation StackSets 드리프트 감지를 활성화합니다. Answer: C Explanation: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/stacksets-orgsmanage-auto-deployment.html
QUESTION NO: 37 보안 엔지니어는 기존 애플리케이션이 Amazon S3의 암호화된 파일에서 Amazon RDS for MySQL 데이터베이스에 대한 자격 증명을 검색하는 것을 확인했습니다. 애플리케이션의 다음 버전에 대해 보안 엔지니어는 보안을 개선하기 위해 다음과 같은 애플리케이션 설계 변경을 구현하려고 합니다. * 데이터베이스는 안전한 AWS 관리형 서비스에 저장된 강력하고 무작위로 생성된 암호를 사용해야 합니다. * 애플리케이션 리소스는 AWS CloudFormation을 통해 배포되어야 합니다. * 애플리케이션은 90일마다 데이터베이스에 대한 자격 증명을 교체해야 합니다. 솔루션 설계자는 CloudFormation 템플릿을 생성하여 애플리케이션을 배포합니다. CloudFormation 템플릿에 지정된 리소스는 최소한의 운영 오버헤드로 보안 엔지니어의 요구 사항을 충족합니까? A. AWS Secrets Manager를 사용하여 비밀 리소스로 데이터베이스 암호를 생성합니다. 데이터베이스 암호를 교체할 AWS Lambda 함수 리소스를 생성합니다. 90일마다 데이터베이스 암호를 교체하도록 Secrets Manager RotationSchedule 리소스를 지정합니다. B. AWS Systems Manager Parameter Store를 사용하여 SecureString 파라미터 유형으로 데이터베이스 암호를 생성합니다. 데이터베이스 암호를 교체할 AWS Lambda 함수 리소스를 생성합니다. 90일마다 데이터베이스 암호를 교체하려면 Parameter Store RotationSchedule 리소스를 지정하십시오. C. AWS Secrets Manager를 사용하여 비밀 리소스로 데이터베이스 암호를 생성합니다. 데이터베이스 암호를 교체할 AWS Lambda 함수 리소스를 생성합니다. Amazon EventBridge 예약 규칙 리소스를 생성하여 90일마다 Lambda 함수 암호 교체를 트리거합니다. D. AWS Systems Manager Parameter Store를 사용하여 SecureString 파라미터 유형으로 데이터베이스 암호를 생성합니다. 90일마다 데이터베이스 암호를 자동으로 교체하도록 AWS AppSync DataSource 리소스를 지정합니다. Answer: B Explanation: https://aws.amazon.com/blogs/security/how-to-securely-provide-database-credentials-tolambda-functions-by-using-aws-secrets-manager/ https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html https://docs.aws.amazon.com/secretsmanager/latest/userguide/integrating_cloudformation.html
QUESTION NO: 38 회사에서 AWS CloudFormation을 사용하여 인프라를 배포하고 있습니다. 회사는 프로덕션 CloudFormation 스택이 삭제되면 Amazon RDS 데이터베이스 또는 Amazon EBS 볼륨에 저장된 중요한 데이터도 삭제될 수 있다고 우려하고 있습니다. 회사는 사용자가 이런 방식으로 실수로 데이터를 삭제하는 것을 어떻게 방지할 수 있습니까? A. RDS 및 EBS 리소스에 DeletionPolicy 속성을 추가하도록 CloudFormation 템플릿을 수정합니다. B. RDS 및 EBS 리소스 삭제를 허용하지 않는 스택 정책을 구성합니다. C. "awsrcloudformation: stack-name" 태그가 지정된 RDS 및 EBS 리소스 삭제를 거부하도록 1AM 정책을 수정합니다. D. AWS Config 규칙을 사용하여 RDS 및 EBS 리소스 삭제를 방지합니다. Answer: A Explanation: With the DeletionPolicy attribute you can preserve or (in some cases) backup a resource when its stack is deleted. You specify a DeletionPolicy attribute for each resource that you want to control. If a resource has no DeletionPolicy attribute, AWS CloudFormation deletes the resource by default. To keep a resource when its stack is deleted, specify Retain for that resource. You can use retain for any resource. For example, you can retain a nested stack, Amazon S3 bucket, or EC2 instance so that you can continue to use or modify those resources after you delete their stacks. https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-attributedeletionpolicy.html
QUESTION NO: 39 이미지 스토리지 서비스를 제공하는 회사는 AWS에 고객 레이싱 솔루션을 배포하려고 합니다. 수백만 명의 개인 고객이 이 솔루션을 사용할 것입니다. 이 솔루션은 대용량 이미지 파일 배치를 수신하고, 파일 크기를 조정하고, 파일을 최대 6개월 동안 Amazon S3 버킷에 저장합니다. 솔루션은 수요의 상당한 변화를 처리해야 합니다. 또한 솔루션은 엔터프라이즈 규모에서 안정적이어야 하며 장애 발생 시 처리 작업을 재실행할 수 있어야 합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까? A. AWS Step Functions를 사용하여 사용자가 이미지를 저장할 때 발생하는 S3 이벤트를 처리합니다. 이미지 크기를 조정하고 S3 버킷의 원본 파일을 대체하는 AWS Lambda 함수를 실행합니다. 저장된 모든 이미지가 6개월 후에 만료되도록 S3 수명 주기 만료 정책을 생성합니다. B. Amazon EventBridge를 사용하여 사용자가 이미지를 업로드할 때 발생하는 S3 이벤트를 처리합니다. 이미지 크기를 조정하고 S3 버킷의 원본 파일을 대체하는 AWS Lambda 함수를 실행합니다. 저장된 모든 이미지가 6개월 후에 만료되도록 S3 수명 주기 만료 정책을 생성합니다. C. 사용자가 이미지를 저장할 때 S3 이벤트 알림을 사용하여 AWS Lambda 함수를 호출합니다. Lambda 함수를 사용하여 이미지 크기를 조정하고 원본 파일을 S3 버킷에 저장합니다. 6개월 후에 저장된 모든 이미지를 S3 Standard-Infrequent Access(S3 Standard-IA)로 이동하는 S3 수명 주기 정책을 생성합니다. D. Amazon Simple Queue Service(Amazon SQS)를 사용하여 사용자가 이미지를 저장할 때 발생하는 S3 이벤트를 처리합니다. 이미지 크기를 조정하고 크기 조정된 파일을 S3 Standard-Infrequent Access(S3 Standard-IA)를 사용하는 S3 버킷에 저장하는 AWS Lambda 함수를 실행합니다. 6개월 후 저장된 모든 이미지를 S3 Glacier Deep Archive로 이동하는 S3 수명 주기 정책을 생성합니다. Answer: C Explanation: S3 Event Notifications is a feature that allows users to receive notifications when certain events happen in an S3 bucket, such as object creation or deletion1. Users can configure S3 Event Notifications to invoke an AWS Lambda function when a user stores an image in the bucket. Lambda is a serverless compute service that runs code in response to events and automatically manages the underlying compute resources2. The Lambda function can resize the image in place and store the original file in the same S3 bucket. This way, the solution can handle significant variance in demand and be reliable at enterprise scale. The solution can also rerun processing jobs in the event of failure by using the retry and dead-letter queue features of Lambda2. S3 Lifecycle is a feature that allows users to manage their objects so that they are stored cost-effectively throughout their lifecycle3. Users can create an S3 Lifecycle policy to move all stored images to S3 Standard-Infrequent Access (S3 Standard-IA) after 6 months. S3 Standard-IA is a storage class designed for data that is accessed less frequently, but requires rapid access when needed4. It offers a lower storage cost than S3 Standard, but charges a retrieval fee. Therefore, moving the images to S3 Standard-IA after 6 months can reduce the storage cost for the solution. Option A is incorrect because using AWS Step Functions to process the S3 event that occurs when a user stores an image is not necessary or cost-effective. AWS Step Functions is a service that lets users coordinate multiple AWS services into serverless workflows. However, for this use case, a single Lambda function can handle the image resizing task without needing Step Functions. Option B is incorrect because using Amazon EventBridge to process the S3 event that occurs when a user uploads an image is not necessary or cost-effective. Amazon EventBridge is a serverless event bus service that makes it easy to connect applications with data from a variety of sources. However, for this use case, S3 Event Notifications can directly invoke the Lambda function without needing EventBridge. Option D is incorrect because using Amazon Simple Queue Service (Amazon SQS) to process the S3 event that occurs when a user stores an image is not necessary or costeffective. Amazon SQS is a fully managed message queuing service that enables users to decouple and scale microservices, distributed systems, and serverless applications. However, for this use case, S3 Event Notifications can directly invoke the Lambda function without needing SQS. Moreover, storing the resized file in an S3 bucket that uses S3 Standard-IA will incur a retrieval fee every time the file is accessed, which may not be costeffective for frequently accessed files.
QUESTION NO: 40 비디오 스트리밍 회사는 최근 비디오 공유를 위한 모바일 앱을 출시했습니다. 앱은 us-east-1 리전의 Amazon S3 버킷에 다양한 파일을 업로드합니다. 파일 크기 범위는 1GB에서 10GB입니다. 호주에서 앱에 액세스하는 사용자는 오랜 시간이 걸리는 업로드를 경험했습니다. 이러한 사용자에게는 파일이 완전히 업로드되지 않는 경우가 있습니다. 솔루션 설계자는 이러한 업로드에 대한 앱의 성능을 개선해야 합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까? (2개를 선택하세요.) A. S3 버킷에서 S3 Transfer Acceleration 활성화 업로드에 Transfer Acceleration 엔드포인트를 사용하도록 앱 구성 B. 업로드를 수신하도록 각 리전에서 S3 버킷을 구성합니다. S3 교차 리전 복제를 사용하여 파일을 배포 S3 버킷에 복사합니다. C. 가장 가까운 S3 버킷 리전으로 업로드를 라우팅하도록 지연 시간 기반 라우팅으로 Amazon Route 53을 설정합니다. D. 비디오 파일을 청크로 나누도록 앱 구성 멀티파트 업로드를 사용하여 파일을 Amazon S3로 전송합니다. E. 업로드하기 전에 파일에 임의의 접두사를 추가하도록 앱을 수정합니다. Answer: A,D Explanation: https://aws.amazon.com/premiumsupport/knowledge-center/s3-upload-large-files/ Enabling S3 Transfer Acceleration on the S3 bucket and configuring the app to use the Transfer Acceleration endpoint for uploads will improve the app's performance for these uploads by leveraging Amazon CloudFront's globally distributed edge locations to accelerate the uploads. Breaking the video files into chunks and using a multipart upload to transfer files to Amazon S3 will also improve the app's performance by allowing parts of the file to be uploaded in parallel, reducing the overall upload time.
QUESTION NO: 41 한 회사에서 REST 기반 API를 통해 여러 고객에게 날씨 데이터를 제공하고 있습니다. API는 Amazon API Gateway에서 호스팅되며 각 API 작업에 대해 서로 다른 AWS Lambda 함수와 통합됩니다. 이 회사는 DNS에 Amazon Route 53을 사용하고 weather.example.com이라는 리소스 레코드를 생성했습니다. 회사는 API용 데이터를 Amazon DynamoDB 테이블에 저장합니다. 회사는 API가 다른 AWS 리전으로 장애 조치할 수 있는 기능을 제공하는 솔루션이 필요합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 새 지역에 새 Lambda 함수 세트를 배포합니다. 두 리전의 Lambda 함수를 대상으로 사용하여 엣지 최적화 API 엔드포인트를 사용하도록 API Gateway API를 업데이트합니다. DynamoDB 테이블을 전역 테이블로 변환합니다. B. 다른 리전에 새 API Gateway API 및 Lambda 함수를 배포합니다. Route 53 DNS 레코드를 다중값 응답으로 변경합니다. 답변에 두 API 게이트웨이 API를 모두 추가합니다. 대상 상태 모니터링을 활성화합니다. DynamoDB 테이블을 전역 테이블로 변환합니다. C. 다른 리전에 새 API Gateway API 및 Lambda 함수를 배포합니다. Route 53 DNS 레코드를 장애 조치 레코드로 변경합니다. 대상 상태 모니터링을 활성화합니다. DynamoDB 테이블을 전역 테이블로 변환합니다. D. 새 리전에 새 API 게이트웨이 API를 배포합니다. Lambda 함수를 전역 함수로 변경합니다. Route 53 DNS 레코드를 다중값 응답으로 변경합니다. 답변에 두 API 게이트웨이 API를 모두 추가합니다. 대상 상태 모니터링을 활성화합니다. DynamoDB 테이블을 전역 테이블로 변환합니다. Answer: C Explanation: https://docs.aws.amazon.com/apigateway/latest/developerguide/dns-failover.html
QUESTION NO: 42 회사는 AWS에 클라우드 인프라를 보유하고 있습니다. 솔루션 설계자는 인프라를 코드로 정의해야 합니다. 인프라는 현재 하나의 AWS 리전에 배포되어 있습니다. 회사의 비즈니스 확장 계획에는 여러 AWS 계정에 걸쳐 여러 리전의 배포가 포함됩니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까? A. AWS CloudFormation 템플릿 사용 IAM 정책을 추가하여 다양한 계정 제어 여러 지역에 템플릿 배포 B. AWS Organizations 사용 마스터 계정에서 AWS CloudFormation 템플릿 배포 AWS Control Tower를 사용하여 계정 간 배포 관리 C. AWS Organizations 및 AWS CloudFormation StackSets 사용 필요한 IAM 권한이 있는 계정에서 CloudFormation 템플릿 배포 D. AWS CloudFormation 템플릿과 함께 중첩 스택 사용 중첩 스택을 사용하여 리전 변경 Answer: C Explanation: https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multipleaccounts-in-an-aws-organization/ AWS Organizations allows the management of multiple AWS accounts as a single entity and AWS CloudFormation StackSets allows creating, updating, and deleting stacks across multiple accounts and regions in an organization. This solution allows creating a single CloudFormation template that can be deployed across multiple accounts and regions, and also allows for the management of access and permissions for the different accounts through the use of IAM roles and policies in the management account.
QUESTION NO: 43 대기업에서 인기 있는 웹 애플리케이션을 실행하고 있습니다. 애플리케이션은 프라이빗 서브넷의 Auto Scaling 그룹에 있는 여러 Amazon EC2 Linux 인스턴스에서 실행됩니다. Application Load Balancer는 프라이빗 서브넷의 Auto Scaling 그룹에 있는 인스턴스를 대상으로 합니다. AWS Systems Manager Session Manager가 구성되었으며 AWS Systems Manager Agent가 모든 EC2 인스턴스에서 실행 중입니다. 회사에서 최근 애플리케이션의 새 버전을 출시했습니다. 일부 EC2 인스턴스가 현재 비정상으로 표시되어 종료되고 있습니다. 그 결과 애플리케이션이 감소된 용량으로 실행되고 있습니다. 애플리케이션에서 수집되지만 로그가 결정적이지 않음 솔루션 설계자는 문제를 해결하기 위해 EC2 인스턴스에 대한 액세스 권한을 어떻게 얻어야 합니까1? A. Auto Scaling 그룹의 HealthCheck 조정 프로세스를 중지합니다. Session Manager를 사용하여 비정상으로 표시된 인스턴스에 로그인 B. EC2 인스턴스 종료 보호 활성화 Session Manager를 사용하여 비정상으로 표시된 인스턴스에 로그인합니다. C. Auto Scaling 그룹에서 종료 정책을 Oldestinstance로 설정합니다. Session Manager를 사용하여 비정상으로 표시된 인스턴스에 로그인 D. Auto Scaling 그룹의 종료 프로세스를 중지합니다. Session Manager를 사용하여 비정상으로 표시된 인스턴스에 로그인 Answer: D Explanation: https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-suspend-resume-processes.html
QUESTION NO: 44 회사에서 온프레미스 데이터 센터에서 실행되는 VMware Infrastructure에서 Amazon EC2로 애플리케이션을 마이그레이션하려고 합니다. 솔루션 설계자는 마이그레이션 중에 소프트웨어 및 구성 설정을 보존해야 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까? A. Windows File Server용 Amazon FSx에 데이터 저장소 복제를 시작하도록 AWS DataSync 에이전트를 구성합니다. SMB 공유를 사용하여 VMware 데이터 저장소를 호스팅합니다. VM Import/Export를 사용하여 VM을 Amazon EC2로 이동합니다. B. VMware vSphere 클라이언트를 사용하여 애플리케이션을 Open Virealization Format(OVF) 형식의 이미지로 내보냅니다. Amazon S3 버킷을 생성하여 대상 AWS 리전에 이미지를 저장합니다. VM 가져오기를 위한 IAM 역할 생성 및 적용 AWS CLI를 사용하여 EC2 가져오기 명령을 실행합니다. C. Common Internet File System(CIFSJ 공유)을 내보내도록 파일용 AWS Storage Gateway 서비스를 구성합니다. 공유 폴더에 대한 백업 복사본을 만듭니다. AWS Management Console에 로그인하고 백업 복사본에서 AMI를 만듭니다. EC2 인스턴스를 시작합니다. AMI를 기반으로 합니다. D. AWS Systems Manager에서 하이브리드 환경을 위한 관리형 인스턴스 활성화를 생성합니다. 온프레미스 VM에 Systems Manager Agent 다운로드 및 설치 VM을 Systems Manager에 관리형 인스턴스로 등록 AWS Backup을 사용하여 VM의 스냅샷을 생성하고 AMI를 생성합니다. AMI를 기반으로 하는 EC2 인스턴스 시작 Answer: D Explanation: https://docs.aws.amazon.com/vm-import/latest/userguide/vmimport-image-import.html - Export an OVF Template - Create / use an Amazon S3 bucket for storing the exported images. The bucket must be in the Region where you want to import your VMs. - Create an IAM role named vmimport. - You'll use AWS CLI to run the import commands. https://aws.amazon.com/premiumsupport/knowledge-center/import-instances/
QUESTION NO: 45 회사에서 대량의 보관 문서를 저장하고 회사 인트라넷을 통해 직원이 문서를 사용할 수 있도록 할 계획입니다. 직원은 VPC에 연결된 클라이언트 VPN 서비스를 통해 연결하여 시스템에 액세스합니다. 데이터는 대중이 접근할 수 없어야 합니다. 회사가 저장하고 있는 문서는 다른 곳의 물리적 매체에 보관된 데이터의 사본입니다. 요청 수가 적습니다. 가용성과 검색 속도는 회사의 관심사가 아닙니다. 최저 비용으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Amazon S3 버킷을 생성합니다. S3 One Zone-Infrequent Access(S3 One Zone-IA) 스토리지 클래스를 기본값으로 사용하도록 S3 버킷을 구성합니다. 웹 사이트 호스팅을 위해 S3 버킷을 구성합니다. S3 인터페이스 엔드포인트를 생성합니다. 해당 엔드포인트를 통해서만 액세스를 허용하도록 S3 버킷을 구성합니다. B. 웹 서버를 실행하는 Amazon EC2 인스턴스를 시작합니다. Amazon Elastic File System(Amazon EFS) 파일 시스템을 연결하여 보관된 데이터를 EFS One Zone-Infrequent Access(EFS One Zone-IA) 스토리지 클래스에 저장합니다. 프라이빗 네트워크에서만 액세스할 수 있도록 인스턴스 보안 그룹을 구성합니다. C. 웹 서버를 실행하는 Amazon EC2 인스턴스를 시작합니다. Amazon Elastic Block Store(Amazon EBS) 볼륨을 연결하여 아카이브된 데이터를 저장합니다. Cold HDD(sc1) 볼륨 유형을 사용합니다. 사설 네트워크에서만 액세스를 허용하도록 인스턴스 보안 그룹을 구성합니다. D. Amazon S3 버킷을 생성합니다. S3 Glacier Deep Archive 스토리지 클래스를 기본값으로 사용하도록 S3 버킷을 구성합니다. 웹 사이트 호스팅을 위해 S3 버킷을 구성합니다. S3 인터페이스 엔드포인트를 생성합니다. 해당 엔드포인트를 통해서만 액세스를 허용하도록 S3 버킷을 구성합니다. Answer: D Explanation: The S3 Glacier Deep Archive storage class is the lowest-cost storage class offered by Amazon S3, and it is designed for archival data that is accessed infrequently and for which retrieval time of several hours is acceptable. S3 interface endpoint for the VPC ensures that access to the bucket is only from resources within the VPC and this will meet the requirement of not being accessible to the public. And also, S3 bucket can be configured for website hosting, and this will allow employees to access the documents through the corporate intranet. Using an EC2 instance and a file system or block store would be more expensive and unnecessary because the number of requests to the data will be low and availability and speed of retrieval are not concerns. Additionally, using Amazon S3 bucket will provide durability, scalability and availability of data.
QUESTION NO: 46 회사에서 AWS로 마이그레이션하려고 합니다. 회사는 모든 계정 및 애플리케이션에 대한 액세스를 중앙에서 관리하는 다중 계정 구조를 사용하려고 합니다. 회사는 또한 개인 네트워크에서 트래픽을 유지하려고 합니다. 로그인 시 다단계 인증(MFA)이 필요하며 특정 역할이 사용자 그룹에 할당됩니다. 회사는 개발을 위해 별도의 계정을 만들어야 합니다. 스테이징, 프로덕션 및 공유 네트워크. 프로덕션 계정과 공유 네트워크 계정은 모든 계정에 연결되어 있어야 합니다. 개발 계정과 스테이징 계정은 서로에게만 액세스 권한이 있어야 합니다. 이러한 요구 사항을 충족하기 위해 솔루션 설계자가 수행해야 하는 단계 조합은 무엇입니까? (3개를 선택하세요.) A. AWS Control Tower를 사용하여 랜딩 존 환경을 배포합니다. 계정을 등록하고 기존 계정을 AWS Organizations의 결과 조직에 초대합니다. B. 모든 계정에서 AWS Security Hub를 활성화하여 교차 계정 액세스를 관리합니다. MFA 로그인을 강제하기 위해 AWS CloudTrail을 통해 조사 결과를 수집합니다. C. 각 계정에서 전송 게이트웨이 및 전송 게이트웨이 VPC 연결을 생성합니다. 적절한 라우팅 테이블을 구성합니다. D. AWS IAM Identity Center(AWS Single Sign-On)를 설정하고 활성화합니다. 기존 계정에 필요한 MFA를 사용하여 적절한 권한 집합을 만듭니다. E. 모든 Recounts에서 AWS Control Tower를 활성화하여 계정 간 라우팅을 관리합니다. MFA 로그인을 강제하기 위해 AWS CloudTrail을 통해 조사 결과를 수집합니다. F. IAM 사용자 및 그룹을 생성합니다. 모든 사용자에 대해 MFA를 구성합니다. Amazon Cognito 사용자 풀과 자격 증명 풀을 설정하여 계정에 대한 액세스와 계정 간 액세스를 관리합니다. Answer: A,C,D Explanation: The correct answer would be options A, C and D, because they address the requirements outlined in the question. A. Deploying a landing zone environment using AWS Control Tower and enrolling accounts in an organization in AWS Organizations allows for a centralized management of access to all accounts and applications. C. Creating transit gateways and transit gateway VPC attachments in each account and configuring appropriate route tables allows for private network traffic, and ensures that the production account and shared network account have connectivity to all accounts, while the development and staging accounts have access only to each other. D. Setting up and enabling AWS IAM Identity Center (AWS Single Sign-On) and creating appropriate permission sets with required MFA for existing accounts allows for multi-factor authentication at login and specific roles to be assigned to user groups.
QUESTION NO: 47 소매 회사는 AWS에서 전자 상거래 애플리케이션을 운영하고 있습니다. 애플리케이션은 ALB(Application Load Balancer) 뒤의 Amazon EC2 인스턴스에서 실행됩니다. 이 회사는 Amazon RDS DB 인스턴스를 데이터베이스 백엔드로 사용합니다. Amazon CloudFront는 ALB를 가리키는 하나의 오리진으로 구성됩니다. 정적 콘텐츠가 캐시됩니다. Amazon Route 53은 모든 퍼블릭 영역을 호스팅하는 데 사용됩니다. 애플리케이션 업데이트 후 ALB는 때때로 502 상태 코드(잘못된 게이트웨이) 오류를 반환합니다. 근본 원인은 ALB에 반환되는 잘못된 형식의 HTTP 헤더입니다. 오류가 발생한 직후 솔루션 설계자가 웹 페이지를 다시 로드하면 웹 페이지가 성공적으로 반환됩니다. 회사에서 문제를 해결하는 동안 솔루션 설계자는 방문자에게 표준 ALB 오류 페이지 대신 사용자 지정 오류 페이지를 제공해야 합니다. 최소한의 운영 오버헤드로 이 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개 선택하세요.) A. Amazon S3 버킷을 생성합니다. 정적 웹 페이지를 호스팅하도록 S3 버킷을 구성합니다. 사용자 지정 오류 페이지를 Amazon S3에 업로드합니다. B. ALB 상태 확인 응답 Target.FailedHealthChecks가 0보다 큰 경우 AWS Lambda 함수를 호출하는 Amazon CloudWatch 경보를 생성합니다. 공개적으로 액세스 가능한 웹 서버를 가리키도록 ALB에서 전달 규칙을 수정하도록 Lambda 함수를 구성합니다. C. 상태 확인을 추가하여 기존 Amazon Route 53 레코드를 수정합니다. 상태 확인이 실패할 경우 폴백 대상을 구성합니다. 공개적으로 액세스할 수 있는 웹페이지를 가리키도록 DNS 레코드를 수정합니다. D. ALB 상태 확인 응답 Elb.InternalError가 0보다 큰 경우 AWS Lambda 함수를 호출하는 Amazon CloudWatch 경보를 생성합니다. 공개적으로 액세스 가능한 웹 서버를 가리키도록 ALB에서 전달 규칙을 수정하도록 Lambda 함수를 구성합니다. E. CloudFront 사용자 지정 오류 페이지를 구성하여 사용자 지정 오류 응답을 추가합니다. 공개적으로 액세스할 수 있는 웹 페이지를 가리키도록 DNS 레코드를 수정합니다. Answer: C,E Explanation: "Save your custom error pages in a location that is accessible to CloudFront. We recommend that you store them in an Amazon S3 bucket, and that you don't store them in the same place as the rest of your website or application's content. If you store the custom error pages on the same origin as your website or application, and the origin starts to return 5xx errors, CloudFront can't get the custom error pages because the origin server is unavailable." https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/GeneratingCustom ErrorResponses.html
QUESTION NO: 48 회사는 사용자에게 맞춤형 애플리케이션에서 이미지를 업로드할 수 있는 기능을 제공합니다. 업로드 프로세스는 Amazon S3 버킷에서 이미지를 처리하고 저장하는 AWS Lambda 함수를 호출합니다. 애플리케이션은 특정 함수 버전 ARN을 사용하여 Lambda 함수를 호출합니다. Lambda 함수는 환경 변수를 사용하여 이미지 처리 매개변수를 수락합니다. 회사는 종종 최적의 이미지 처리 출력을 얻기 위해 Lambda 함수의 환경 변수를 조정합니다. 회사는 다양한 매개변수를 테스트하고 결과를 확인한 후 업데이트된 환경 변수로 새 기능 버전을 게시합니다. 또한 이 업데이트 프로세스에서는 새 기능 버전 ARN을 호출하기 위해 사용자 지정 애플리케이션을 자주 변경해야 합니다. 이러한 변경 사항은 사용자에게 방해가 됩니다. 솔루션 설계자는 이 프로세스를 단순화하여 사용자의 혼란을 최소화해야 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 게시된 Lambda 함수 버전의 환경 변수를 직접 수정합니다. SLATEST 버전을 사용하여 이미지 처리 매개변수를 테스트합니다. B. 이미지 처리 매개변수를 저장할 Amazon DynamoDB 테이블을 생성합니다. DynamoDB 테이블에서 이미지 처리 매개변수를 검색하도록 Lambda 함수를 수정합니다. C. Lambda 함수 내에서 이미지 처리 매개변수를 직접 코딩하고 환경 변수를 제거합니다. 회사에서 매개변수를 업데이트하면 새 기능 버전을 게시합니다. D. Lambda 함수 별칭을 생성합니다. 함수 별칭 ARN을 사용하도록 클라이언트 애플리케이션을 수정합니다. 회사에서 테스트를 완료하면 함수의 새 버전을 가리키도록 Lambda 별칭을 재구성합니다. Answer: D Explanation: A Lambda function alias allows you to point to a specific version of a function and also can be updated to point to a new version of the function without modifying the client application. This way, the company can test different versions of the function with different environment variables and, once the optimal parameters are found, update the alias to point to the new version, without the need to update the client application. By using this approach, the company can simplify the process of updating the environment variables, minimize disruption to users, and reduce the operational overhead. Reference: AWS Lambda documentation: https://aws.amazon.com/lambda/ AWS Lambda Aliases documentation: https://docs.aws.amazon.com/lambda/latest/dg/aliases-intro.html AWS Lambda versioning and aliases documentation: https://aws.amazon.com/blogs/compute/versioning-aliases-inaws-lambda/
QUESTION NO: 49 회사는 온프레미스 데이터 센터에서 AWS로 3계층 웹 애플리케이션을 마이그레이션할 계획입니다. 회사는 서버 측 JavaScript 라이브러리를 사용하여 Ui를 개발했습니다. 비즈니스 로직 및 API 계층은 Python 기반 웹 프레임워크를 사용합니다. 데이터 계층은 MySQL 데이터베이스 회사는 비즈니스 요구 사항을 충족하기 위해 애플리케이션을 사용자 정의했습니다. 회사는 애플리케이션을 재설계하기를 원하지 않습니다. 회사는 최소한의 개발로 애플리케이션을 AWS로 플랫폼을 변경하는 솔루션이 필요합니다. 솔루션은 가용성이 높고 운영 오버헤드를 줄여야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Amazon S3의 정적 웹사이트에 UI 배포 Amazon CloudFront를 사용하여 웹사이트 제공 Docker 이미지에 비즈니스 로직 구축 Amazon Elastic Container Registry(Amazon ECR)에 이미지 저장 Amazon Elastic Container Service(Amazon ECS) 사용 Application Load Balancer가 앞에 있는 웹사이트를 호스팅하기 위해 Fargate 시작 유형으로 Amazon Aurora MySQL DB 클러스터에 데이터 계층 배포 B. Docker 이미지에 UI 및 비즈니스 로직 구축 Amazon Elastic Container Registry(Amazon ECR)에 이미지 저장 Fargate 시작 유형과 함께 Amazon Elastic Container Service(Amazon ECS)를 사용하여 애플리케이션으로 UI 및 비즈니스 로직 애플리케이션 호스팅 로드 밸런서 앞 데이터베이스를 Amazon RDS for MySQL 다중 AZ DB 인스턴스로 마이그레이션 C. Amazon S3의 정적 웹 사이트에 UI 배포 Amazon CloudFront를 사용하여 웹 사이트 제공 비즈니스 로직을 AWS Lambda 함수로 변환 함수를 Amazon API Gateway와 통합 Amazon Aurora MySQL DB 클러스터에 데이터 계층 배포 D. Docker 이미지에 UI 및 비즈니스 로직 구축 Amazon Elastic Container Registry(Amazon ECR)에 이미지 저장 Fargate 프로필과 함께 Amazon Elastic Kubernetes Service(Amazon EKS)를 사용하여 UI 및 비즈니스 로직 호스팅 AWS Database Migration Service 사용( AWS DMS) 데이터 계층을 Amazon DynamoDB로 마이그레이션 Answer: A Explanation: This solution utilizes Amazon S3 and CloudFront to deploy the UI as a static website, which can be done with minimal development effort. The business logic and API tier can be containerized in a Docker image and stored in Amazon Elastic Container Registry (ECR) and run on Amazon Elastic Container Service (ECS) with the Fargate launch type, which allows the application to be highly available with minimal operational overhead. The data layer can be deployed on an Amazon Aurora MySQL DB cluster which is a fully managed relational database service. Amazon Aurora provides high availability and performance for the data layer without the need for managing the underlying infrastructure.
QUESTION NO: 50 회사에서 Amazon EC2 인스턴스를 사용하여 블로그 사이트를 호스팅하기 위해 웹 집합을 배포했습니다. EC2 인스턴스는 ALB(Application Load Balancer) 뒤에 있으며 Auto Scaling 그룹에 구성됩니다. 웹 애플리케이션은 모든 블로그 콘텐츠를 Amazon EFS 볼륨에 저장합니다. 이 회사는 최근 자신의 게시물에 동영상을 추가하는 'Moggers' 기능을 추가하여 하루 중 피크 시간에 이전 사용자 트래픽의 10배를 끌어들였습니다. 사용자가 사이트에 접속하거나 비디오를 시청하려고 시도하는 동안 버퍼링 및 시간 초과 문제를 보고합니다. 사용자의 문제를 해결하는 가장 비용 효율적이고 확장 가능한 배포는 무엇입니까? A. 최대 I/O를 활성화하도록 Amazon EFS를 재구성합니다. B. 인스턴스 스토어 볼륨을 스토리지로 사용하도록 Nog 사이트를 업데이트합니다. 시작할 때 사이트 콘텐츠를 볼륨에 복사하고 Amazon S3 종료 시에 복사합니다. C. Amazon CloudFront 배포를 구성합니다. 배포를 S3 버킷으로 지정하고 비디오를 EFS에서 Amazon S3로 마이그레이션합니다. D. 모든 사이트 콘텐츠에 대해 Amazon CloudFront 배포를 설정하고 배포가 ALB를 가리키도록 합니다. Answer: C Explanation: https://aws.amazon.com/premiumsupport/knowledge-center/cloudfront-https-connection-fails/ Using an Amazon S3 bucket Using a MediaStore container or a MediaPackage channel Using an Application Load Balancer Using a Lambda function URL Using Amazon EC2 (or another custom origin) Using CloudFront origin groups https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/restrict-access-toload-balancer.html
QUESTION NO: 51 회사는 Amazon EC2 Linux 인스턴스에서 독점 상태 비저장 ETL 애플리케이션을 실행합니다. 응용 프로그램은 Linux 바이너리이며 소스 코드를 수정할 수 없습니다. 응용 프로그램은 단일 스레드이며 2GB의 RAM을 사용합니다. CPU 집약적입니다. 애플리케이션이 4시간마다 실행되도록 예약되어 최대 20분 동안 실행됩니다. 솔루션 설계자가 솔루션의 아키텍처를 수정하려고 합니다. 솔루션 설계자는 어떤 전략을 사용해야 합니까? A. AWS Lambda를 사용하여 애플리케이션을 실행합니다. Amazon CloudWatch Logs를 사용하여 4시간마다 Lambda 함수를 호출합니다. B. AWS Batch를 사용하여 애플리케이션을 실행합니다. AWS Step Functions 상태 머신을 사용하여 4시간마다 AWS Batch 작업을 호출합니다. C. AWS Fargate를 사용하여 애플리케이션을 실행합니다. Amazon EventBridge(Amazon CloudWatch Events)를 사용하여 4시간마다 Fargate 작업을 호출합니다. D. Amazon EC2 스팟 인스턴스를 사용하여 애플리케이션을 실행합니다. AWS CodeDeploy를 사용하여 4시간마다 애플리케이션을 배포하고 실행합니다. Answer: C Explanation: step function could run a scheduled task when triggered by eventbrige, but why would you add that layer of complexity just to run aws batch when you could directly invoke it through eventbridge. The link provided - https://aws.amazon.com/pt/blogs/compute/orchestrating-high -performance-computing-with-aws-step-functions-and-aws-batch/ makes sense only for HPC, this is a single instance that needs to be run
QUESTION NO: 52 회사는 AWS 클라우드에 애플리케이션을 가지고 있습니다. 이 애플리케이션은 20개의 Amazon EC2 인스턴스 플릿에서 실행됩니다. EC2 인스턴스는 영구적이며 연결된 여러 Amazon Elastic Block Store(Amazon EBS) 볼륨에 데이터를 저장합니다. 회사는 별도의 AWS 리전에서 백업을 유지 관리해야 합니다. 회사는 1일 분량의 데이터 손실 없이 영업일 기준 1일 이내에 EC2 인스턴스와 해당 구성을 복구할 수 있어야 합니다. 이 회사는 직원이 제한되어 있으며 운영 효율성과 비용을 최적화하는 백업 솔루션이 필요합니다. 이 회사는 보조 리전에 필요한 네트워크 구성을 배포할 수 있는 AWS CloudFormation 템플릿을 이미 생성했습니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 보조 지역에서 EC2 인스턴스를 다시 생성할 수 있는 두 번째 CloudFormation 템플릿을 생성합니다. AWS Systems Manager Automation Runbook을 사용하여 매일 다중 볼륨 스냅샷을 실행합니다. 스냅샷을 보조 리전에 복사합니다. 장애가 발생한 경우 CloudFormation 템플릿을 시작하고 스냅샷에서 EBS 볼륨을 복원하고 사용량을 보조 리전으로 전송합니다. B. Amazon Data Lifecycle Manager(Amazon DLM)를 사용하여 EBS 볼륨의 일일 다중 볼륨 스냅샷을 생성합니다. 장애가 발생한 경우 CloudFormation 템플릿을 시작하고 Amazon DLM을 사용하여 EBS 볼륨을 복원하고 사용량을 보조 리전으로 전송합니다. C. AWS Backup을 사용하여 EC2 인스턴스에 대한 예약된 일일 백업 계획을 생성합니다. 보조 리전의 볼트에 백업을 복사하도록 백업 작업을 구성합니다. 장애가 발생한 경우 CloudFormation 템플릿을 시작하고 백업 볼트에서 인스턴스 볼륨 및 구성을 복원하고 사용량을 보조 리전으로 전송합니다. D. 동일한 크기 및 구성의 EC2 인스턴스를 보조 지역에 배포합니다. 기본 리전에서 보조 리전으로 데이터를 복사하도록 매일 AWS DataSync를 구성합니다. 장애가 발생한 경우 CloudFormation 템플릿을 시작하고 사용량을 보조 리전으로 전송합니다. Answer: C Explanation: Using AWS Backup to create a scheduled daily backup plan for the EC2 instances will enable taking snapshots of the EC2 instances and their attached EBS volumes1. Configuring the backup task to copy the backups to a vault in the secondary Region will enable maintaining backups in a separate Region1. In the event of a failure, launching the CloudFormation template will enable deploying the network configuration in the secondary Region2. Restoring the instance volumes and configurations from the backup vault will enable recovering the EC2 instances and their data1. Transferring usage to the secondary Region will enable resuming operations2.
QUESTION NO: 53 신생 회사는 최신 Amazon Linux 2 AMI를 사용하여 프라이빗 서브넷에서 Amazon EC2 인스턴스 플릿을 호스팅합니다. 회사의 엔지니어는 문제 해결을 위해 인스턴스에 대한 SSH 액세스에 크게 의존합니다. 회사의 기존 아키텍처에는 다음이 포함됩니다. * 프라이빗 및 퍼블릭 서브넷이 있는 VPC와 NAT 게이트웨이 * 온프레미스 환경과의 연결을 위한 사이트 간 VPN * 온프레미스 환경에서 직접 SSH에 액세스하는 EC2 보안 그룹 회사는 SSH 액세스에 대한 보안 제어를 강화하고 엔지니어가 실행하는 명령에 대한 감사를 제공해야 합니다. 솔루션 설계자는 어떤 전략을 사용해야 합니까? A. EC2 인스턴스 플릿에 EC2 Instance Connect를 설치하고 구성합니다. 포트 22에서 인바운드 TCP를 허용하는 EC2 인스턴스에 연결된 모든 보안 그룹 규칙을 제거합니다. 엔지니어에게 EC2 Instance Connect CLI를 사용하여 인스턴스에 원격으로 액세스하도록 조언합니다. B. 포트 22에서 엔지니어 장치의 IP 주소에 대한 인바운드 TCP만 허용하도록 EC2 보안 그룹을 업데이트합니다. 모든 EC2 인스턴스에 Amazon CloudWatch 에이전트를 설치하고 운영 체제 감사 로그를 CloudWatch Logs로 보냅니다. C. EC2 보안 그룹을 업데이트하여 엔지니어 장치의 IP 주소에 대한 포트 22의 인바운드 TCP만 허용합니다. EC2 보안 그룹 리소스 변경에 대해 AWS Config를 활성화합니다. AWS Firewall Manager를 활성화하고 규칙 변경 사항을 자동으로 수정하는 보안 그룹 정책을 적용합니다. D. AmazonSSMManagedInstanceCore 관리형 정책이 연결된 IAM 역할을 생성합니다. 모든 EC2 인스턴스에 IAM 역할을 연결합니다. 포트 22에서 인바운드 TCP를 허용하는 EC2 인스턴스에 연결된 모든 보안 그룹 규칙을 제거합니다. 엔지니어가 디바이스에 AWS Systems Manager Session Manager 플러그인을 설치하고 Systems Manager에서 시작 세션 API 호출을 사용하여 인스턴스에 원격으로 액세스하도록 합니다. Answer: D Explanation: Allows client machines to be able to connect to Session Manager using the AWS CLI instead of going through the AWS EC2 or AWS Server Manager console. https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager-workingwith-install-plugin.html https://docs.aws.amazon.com/systemsmanager/ latest/userguide/session-manager-working-with-installplugin.html#:~:text=aws%20ssm%20start%2Dsession%20%2D%2Dtarget%20instance%2Di d
QUESTION NO: 54 회사에서 새로운 웹 기반 애플리케이션을 배포하고 있으며 Linux 애플리케이션 서버용 스토리지 솔루션이 필요합니다. 회사는 모든 인스턴스에 대한 애플리케이션 데이터 업데이트를 위한 단일 위치를 생성하려고 합니다. 활성 데이터 세트의 크기는 최대 100GB입니다. 솔루션 설계자는 매일 3시간 동안 최대 작업이 발생하고 총 225MiBps의 읽기 처리량이 필요하다고 결정했습니다. 솔루션 설계자는 재해 복구(DR)를 위해 다른 AWS 리전에서 사용할 수 있는 데이터 사본을 만드는 다중 AZ 솔루션을 설계해야 합니다. DR 사본의 RPO는 1시간 미만입니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 새로운 Amazon Elastic File System(Amazon EFS) 다중 AZ 파일 시스템을 배포합니다. 프로비저닝된 처리량의 75MiBps에 대해 파일 시스템을 구성합니다. DR 지역의 파일 시스템에 대한 복제를 구현합니다. B. 새로운 Amazon FSx for Lustre 파일 시스템을 배포합니다. 파일 시스템에 대한 버스팅 처리량 모드를 구성합니다. AWS Backup을 사용하여 파일 시스템을 DR 리전에 백업합니다. C. 처리량이 225MiBps인 범용 SSD(gp3) Amazon Elastic Block Store(Amazon EBS) 볼륨을 배포합니다. EBS 볼륨에 대해 다중 연결을 활성화합니다. AWS Elastic Disaster Recovery를 사용하여 EBS 볼륨을 DR 리전에 복제합니다. D. 프로덕션 리전과 DR 리전 모두에 Amazon FSx for OpenZFS 파일 시스템을 배포합니다. 10분마다 프로덕션 파일 시스템에서 DR 파일 시스템으로 데이터를 복제하는 AWS DataSync 예약 작업을 생성합니다. Answer: A Explanation: The company should deploy a new Amazon Elastic File System (Amazon EFS) Multi-AZ file system. The company should configure the file system for 75 MiBps of provisioned throughput. The company should implement replication to a file system in the DR Region. This solution will meet the requirements because Amazon EFS is a serverless, fully elastic file storage service that lets you share file data without provisioning or managing storage capacity and performance. Amazon EFS is built to scale on demand to petabytes without disrupting applications, growing and shrinking automatically as you add and remove files1. By deploying a new Amazon EFS Multi-AZ file system, the company can create a single location for updates to application data for all instances. A Multi-AZ file system replicates data across multiple Availability Zones (AZs) within a Region, providing high availability and durability2. By configuring the file system for 75 MiBps of provisioned throughput, the company can ensure that it meets the peak operations requirement of 225 MiBps of read throughput. Provisioned throughput is a feature that enables you to specify a level of throughput that the file system can drive independent of the file system's size or burst credit balance3. By implementing replication to a file system in the DR Region, the company can make a copy of the data available in another AWS Region for disaster recovery. Replication is a feature that enables you to replicate data from one EFS file system to another EFS file system across AWS Regions. The replication process has an RPO of less than 1 hour. The other options are not correct because: Deploying a new Amazon FSx for Lustre file system would not provide a single location for updates to application data for all instances. Amazon FSx for Lustre is a fully managed service that provides cost-effective, high-performance storage for compute workloads. However, it does not support concurrent write access from multiple instances. Using AWS Backup to back up the file system to the DR Region would not provide real-time replication of dat a. AWS Backup is a service that enables you to centralize and automate data protection across AWS services. However, it does not support continuous data replication or cross-Region disaster recovery. Deploying a General Purpose SSD (gp3) Amazon Elastic Block Store (Amazon EBS) volume with 225 MiBps of throughput would not provide a single location for updates to application data for all instances. Amazon EBS is a service that provides persistent block storage volumes for use with Amazon EC2 instances. However, it does not support concurrent access from multiple instances, unless Multi-Attach is enabled. Enabling Multi-Attach for the EBS volume would not provide Multi-AZ resilience or cross-Region replication. Multi-Attach is a feature that enables you to attach an EBS volume to multiple EC2 instances within the same Availability Zone. Using AWS Elastic Disaster Recovery to replicate the EBS volume to the DR Region would not provide real-time replication of data. AWS Elastic Disaster Recovery (AWS DRS) is a service that enables you to orchestrate and automate disaster recovery workflows across AWS Regions. However, it does not support continuous data replication or sub-hour RPOs. Deploying an Amazon FSx for OpenZFS file system in both the production Region and the DR Region would not be as simple or cost-effective as using Amazon EFS. Amazon FSx for OpenZFS is a fully managed service that provides high-performance storage with strong data consistency and advanced data management features for Linux workloads. However, it requires more configuration and management than Amazon EFS, which is serverless and fully elastic. Creating an AWS DataSync scheduled task to replicate the data from the production file system to the DR file system every 10 minutes would not provide real-time replication of data. AWS DataSync is a service that enables you to transfer data between onpremises storage and AWS services, or between AWS services. However, it does not support continuous data replication or sub-minute RPOs. Reference: https://aws.amazon.com/efs/ https://docs.aws.amazon.com/efs/latest/ug/how-it-works.html#how-it-works-azs https://docs.aws.amazon.com/efs/latest/ug/performance.html#provisioned-throughput https://docs.aws.amazon.com/efs/latest/ug/replication.html https://aws.amazon.com/fsx/lustre/ https://aws.amazon.com/backup/ https://aws.amazon.com/ebs/ https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html
QUESTION NO: 55 솔루션 설계자가 회사의 AWS Lambda 함수 보안 설정을 감사하고 있습니다. Lambda 함수는 Amazon Aurora 데이터베이스에서 최신 변경 사항을 검색합니다. Lambda 함수와 데이터베이스는 동일한 VPC에서 실행됩니다. Lambda 환경 변수는 Lambda 함수에 데이터베이스 자격 증명을 제공합니다. Lambda 함수는 데이터를 집계하고 AWS KMS 관리형 암호화 키(SSE-KMS)를 사용한 서버 측 암호화용으로 구성된 Amazon S3 버킷에서 데이터를 사용할 수 있도록 합니다. 데이터는 인터넷을 통해 이동하면 안 됩니다. 데이터베이스 자격 증명이 손상되면 회사는 손상의 영향을 최소화하는 솔루션이 필요합니다. 이러한 요구 사항을 충족하기 위해 솔루션 설계자는 무엇을 권장해야 합니까? A. Aurora DB 클러스터에서 IAM 데이터베이스 인증을 활성화합니다. 함수가 IAM 데이터베이스 인증을 사용하여 데이터베이스에 액세스할 수 있도록 Lambda 함수의 IAM 역할을 변경합니다. VPC에서 Amazon S3용 게이트웨이 VPC 엔드포인트를 배포합니다. B. Aurora DB 클러스터에서 IAM 데이터베이스 인증을 활성화합니다. 함수가 IAM 데이터베이스 인증을 사용하여 데이터베이스에 액세스할 수 있도록 Lambda 함수의 IAM 역할을 변경합니다. 데이터 전송 중 Amazon S3 연결에 HTTPS를 적용합니다. C. AWS Systems Manager Parameter Store에 데이터베이스 자격 증명을 저장합니다. Parameter Store의 자격 증명에 대한 암호 교체를 설정합니다. 함수가 Parameter Store에 액세스할 수 있도록 Lambda 함수의 IAM 역할을 변경합니다. Parameter Store에서 자격 증명을 검색하도록 Lambda 함수를 수정합니다. VPC에서 Amazon S3용 게이트웨이 VPC 엔드포인트를 배포합니다. D. AWS Secrets Manager에 데이터베이스 자격 증명을 저장합니다. Secrets Manager에서 자격 증명에 대한 암호 교체를 설정합니다. 함수가 Secrets Manager에 액세스할 수 있도록 Lambda 함수의 IAM 역할을 변경합니다. Om Secrets Manager 자격 증명을 검색하도록 Lambda 함수를 수정합니다. 데이터 전송 중 Amazon S3 연결에 HTTPS를 적용합니다. Answer: A Explanation: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/UsingWithRDS.IAMDBA uth.html
QUESTION NO: 56 애플리케이션이 us-east-1 리전에서 Amazon RDS for MySQL 다중 AZ DB 인스턴스를 사용하고 있습니다. 장애 조치 테스트 후 애플리케이션에서 데이터베이스에 대한 연결이 끊어져 연결을 다시 설정할 수 없습니다. 애플리케이션을 다시 시작한 후 애플리케이션이 연결을 다시 설정했습니다. 솔루션 설계자는 응용 프로그램이 다시 시작할 필요 없이 데이터베이스에 대한 연결을 다시 설정할 수 있도록 솔루션을 구현해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Amazon Aurora MySQL Serverless v1 DB 인스턴스를 생성합니다. RDS DB 인스턴스를 Aurora Serverless v1 DB 인스턴스로 마이그레이션합니다. Aurora 리더 엔드포인트를 가리키도록 애플리케이션의 연결 설정을 업데이트합니다. B. RDS 프록시를 생성합니다. 기존 RDS 엔드포인트를 대상으로 구성합니다. RDS 프록시 엔드포인트를 가리키도록 애플리케이션의 연결 설정을 업데이트합니다. C. 2노드 Amazon Aurora MySQL DB 클러스터를 생성합니다. RDS DB 인스턴스를 Aurora DB 클러스터로 마이그레이션합니다. RDS 프록시를 생성합니다. 기존 RDS 엔드포인트를 대상으로 구성합니다. RDS 프록시 엔드포인트를 가리키도록 애플리케이션의 연결 설정을 업데이트합니다. D. Amazon S3 버킷을 생성합니다. AWS Database Migration Service(AWS DMS)를 사용하여 데이터베이스를 Amazon S3로 내보냅니다. S3 버킷을 데이터 저장소로 사용하도록 Amazon Athena를 구성합니다. 애플리케이션용 최신 ODBC(Open Database Connectivity) 드라이버를 설치합니다. Athena 엔드포인트를 가리키도록 애플리케이션의 연결 설정 업데이트 Answer: B Explanation: Amazon RDS Proxy is a fully managed database proxy service for Amazon Relational Database Service (RDS) that makes applications more scalable, resilient, and secure. It allows applications to pool and share connections to an RDS database, which can help reduce database connection overhead, improve scalability, and provide automatic failover and high availability.
QUESTION NO: 57 회사는 AWS에 클라우드 인프라를 보유하고 있습니다. 솔루션 설계자는 인프라를 코드로 정의해야 합니다. 인프라는 현재 하나의 AWS 리전에 배포되어 있습니다. 회사의 비즈니스 확장 계획에는 여러 AWS 계정에 걸쳐 여러 리전의 배포가 포함됩니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까? A. AWS CloudFormation 템플릿 사용 IAM 정책을 추가하여 다양한 계정 제어 여러 지역에 템플릿 배포 B. AWS Organizations 사용 마스터 계정에서 AWS CloudFormation 템플릿 배포 AWS Control Tower를 사용하여 계정 간 배포 관리 C. AWS Organizations 및 AWS CloudFormation StackSets 사용 필요한 IAM 권한이 있는 계정에서 CloudFormation 템플릿 배포 D. AWS CloudFormation 템플릿과 함께 중첩 스택 사용 중첩 스택을 사용하여 리전 변경 Answer: C Explanation: https://aws.amazon.com/blogs/aws/new-use-aws-cloudformation-stacksets-for-multipleaccounts-in-an-aws-organization/ AWS Organizations allows the management of multiple AWS accounts as a single entity and AWS CloudFormation StackSets allows creating, updating, and deleting stacks across multiple accounts and regions in an organization. This solution allows creating a single CloudFormation template that can be deployed across multiple accounts and regions, and also allows for the management of access and permissions for the different accounts through the use of IAM roles and policies in the management account.
QUESTION NO: 58 글로벌 미디어 회사는 애플리케이션의 다중 지역 배포를 계획하고 있습니다. Amazon DynamoDB 전역 테이블은 사용자가 집중된 두 대륙에서 일관된 사용자 경험을 유지하기 위해 배포를 지원합니다. 각 배포에는 퍼블릭 Application Load Balancer(ALB)가 있습니다. 퍼블릭 DNS는 회사에서 내부적으로 관리합니다. 회사는 apex 도메인을 통해 애플리케이션을 사용할 수 있게 하려고 합니다. 최소한의 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 퍼블릭 DNS를 Amazon Route 53으로 마이그레이션합니다. apex 도메인이 ALB를 가리키도록 CNAME 레코드를 생성합니다. 지리적 위치 라우팅 정책을 사용하여 사용자 위치를 기반으로 트래픽을 라우팅합니다. B. ALB 앞에 NLB(Network Load Balancer)를 배치합니다. 퍼블릭 DNS를 Amazon Route 53으로 마이그레이션합니다. NLB의 정적 IP 주소를 가리키도록 apex 도메인에 대한 CNAME 레코드를 생성합니다. 지리적 위치 라우팅 정책을 사용하여 사용자 위치를 기반으로 트래픽을 라우팅합니다. C. 적절한 AWS 리전에서 엔드포인트를 대상으로 하는 여러 엔드포인트 그룹이 있는 AWS Global Accelerator 액셀러레이터를 생성합니다. 액셀러레이터의 정적 IP 주소를 사용하여 apex 도메인에 대한 퍼블릭 DNS의 레코드를 생성합니다. D. AWS 리전 중 하나에서 AWS Lambda가 지원하는 Amazon API Gateway API를 생성합니다. 라운드 로빈 방법을 사용하여 트래픽을 애플리케이션 배포로 라우팅하도록 Lambda 함수를 구성합니다. API의 URL을 가리키도록 apex 도메인에 대한 CNAME 레코드를 생성합니다. Answer: C Explanation: AWS Global Accelerator is a service that directs traffic to optimal endpoints (in this case, the Application Load Balancer) based on the health of the endpoints and network routing. It allows you to create an accelerator that directs traffic to multiple endpoint groups, one for each Region where the application is deployed. The accelerator uses the AWS global network to optimize the traffic routing to the healthy endpoint. By using Global Accelerator, the company can use a single static IP address for the apex domain, and traffic will be directed to the optimal endpoint based on the user's location, without the need for additional load balancers or routing policies. Reference: AWS Global Accelerator documentation: https://aws.amazon.com/global-accelerator/ Routing User Traffic to the Optimal AWS Region using Global Accelerator documentation: https://aws.amazon.com/blogs/networking-and-content-delivery/routing-user-traffic-to-theoptimal-aws-region-using-global-accelerator/
QUESTION NO: 59 회사는 스트리밍 시장 데이터를 수집하고 처리합니다. 데이터 속도는 일정합니다. 집계 통계를 계산하는 야간 프로세스가 실행되며 각 실행을 완료하는 데 약 4시간이 걸립니다. 통계 분석은 비즈니스에 미션 크리티컬하지 않으며 특정 실행이 실패하면 다음 실행에서 이전 데이터 포인트가 선택됩니다. 현재 아키텍처는 연결된 Amazon EBS 볼륨에 스트리밍 데이터를 수집하고 저장하기 위해 상시 실행되는 1년 예약이 포함된 Amazon EC2 예약 인스턴스 풀을 사용합니다. 온디맨드 EC2 인스턴스는 매일 밤 시작되어 야간 처리를 수행하고, 수집 서버의 NFS 공유에서 저장된 데이터에 액세스하고, 완료되면 야간 처리 서버를 종료합니다. 예약 인스턴스 예약이 만료되며 회사는 새 예약을 구매할지 아니면 새로운 디자인을 구현할지 결정해야 합니다. 가장 비용 효율적인 디자인은 무엇입니까? A. Amazon Kinesis Data Firehose를 사용하여 Amazon S3에 데이터를 저장하도록 수집 프로세스를 업데이트합니다. 예약된 스크립트를 사용하여 매일 밤 EC2 온디맨드 인스턴스 플릿을 시작하여 S3 데이터의 일괄 처리를 수행합니다. 처리가 완료되면 인스턴스를 종료하도록 스크립트를 구성하십시오. B. Amazon Kinesis Data Firehose를 사용하여 데이터를 Amazon S3에 저장하도록 수집 프로세스를 업데이트합니다. 스팟 인스턴스와 함께 AWS Batch를 사용하여 온디맨드 가격의 50%인 최대 스팟 가격으로 야간 처리를 수행합니다. C. 수집 프로세스를 업데이트하여 Network Load Balancer 뒤에 3년 예약된 EC2 예약 인스턴스 플릿을 사용합니다. 스팟 인스턴스와 함께 AWS Batch를 사용하여 온디맨드 가격의 50%인 최대 스팟 가격으로 야간 처리를 수행합니다. D. Amazon Kinesis Data Firehose를 사용하여 데이터를 Amazon Redshift에 저장하도록 수집 프로세스를 업데이트합니다. Amazon EventBridge를 사용하여 AWS Lambda 함수가 매일 밤 실행되도록 예약하고 Amazon Redshift를 쿼리하여 일일 통계를 생성합니다. Answer: B Explanation: Updating the ingestion process to use Amazon Kinesis Data Firehose to save data to Amazon S3 will reduce the need for EC2 instances and EBS volumes for data storage1. Using AWS Batch with Spot Instances to perform nightly processing will leverage the cost savings of Spot Instances, which are up to 90% cheaper than On-Demand Instances2. AWS Batch will also handle the scheduling and scaling of the processing jobs. Setting the maximum Spot price to 50% of the On-Demand price will reduce the chances of interruption and ensure that the processing is cost-effective.
QUESTION NO: 60 솔루션 설계자는 Amazon API Gateway 지역 엔드포인트와 AWS Lambda 함수를 사용하는 웹 애플리케이션을 개발했습니다. 웹 애플리케이션의 소비자는 모두 애플리케이션이 배포될 AWS 리전에 가깝습니다. Lambda 함수는 Amazon Aurora MySQL 데이터베이스만 쿼리합니다. 솔루션 설계자는 3개의 읽기 전용 복제본을 갖도록 데이터베이스를 구성했습니다. 테스트 중에 응용 프로그램이 성능 요구 사항을 충족하지 않습니다. 부하가 높을 때 응용 프로그램은 많은 수의 데이터베이스 연결을 엽니다. 솔루션 설계자는 애플리케이션의 성능을 개선해야 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 조치를 취해야 합니까? (2개 선택하세요.) A. Aurora 데이터베이스의 클러스터 엔드포인트를 사용합니다. B. RDS Proxy를 사용하여 Aurora 데이터베이스의 리더 엔드포인트에 대한 연결 풀을 설정합니다. C. Lambda 프로비저닝된 동시성 기능을 사용합니다. D. 이벤트 핸들러 외부의 Lambda 함수에서 데이터베이스 연결을 열기 위한 코드를 이동합니다. E. API 게이트웨이 엔드포인트를 엣지 최적화 엔드포인트로 변경합니다. Answer: B,D Explanation: Connect to RDS outside of Lambda handler method to improve performance https://awstut.com/en/2022/04/30/connect-to-rds-outside-of-lambda-handler-method-toimprove-performance-en/ Using RDS Proxy, you can handle unpredictable surges in database traffic. Otherwise, these surges might cause issues due to oversubscribing connections or creating new connections at a fast rate. RDS Proxy establishes a database connection pool and reuses connections in this pool. This approach avoids the memory and CPU overhead of opening a new database connection each time. To protect the database against oversubscription, you can control the number of database connections that are created. https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html
QUESTION NO: 61 회사에서 일부 애플리케이션을 AWS로 마이그레이션하고 있습니다. 이 회사는 네트워킹 및 보안 전략을 확정한 후 신속하게 애플리케이션을 마이그레이션하고 현대화하려고 합니다. 회사는 중앙 네트워크 계정에서 AWS Direct Connection 연결을 설정했습니다. 이 회사는 가까운 장래에 수백 개의 AWS 계정과 VPC를 갖게 될 것으로 예상합니다. 회사 네트워크는 AWS의 리소스에 원활하게 액세스할 수 있어야 하며 모든 VPC와 통신할 수 있어야 합니다. 회사는 또한 온프레미스 데이터 센터를 통해 클라우드 리소스를 인터넷으로 라우팅하려고 합니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (3개를 선택하세요.) A. 중앙 계정에 Direct Connect 게이트웨이를 생성합니다. 각 계정에서 Direct Connect 게이트웨이와 모든 가상 프라이빗 게이트웨이의 계정 ID를 사용하여 연결 제안을 생성합니다. B. 중앙 네트워크 계정에서 Direct Connect 게이트웨이 및 전송 게이트웨이를 생성합니다. 전송 VIF를 사용하여 전송 게이트웨이를 Direct Connect 게이트웨이에 연결합니다. C. 인터넷 게이트웨이를 프로비저닝합니다. 인터넷 게이트웨이를 서브넷에 연결합니다. 게이트웨이를 통한 인터넷 트래픽을 허용합니다. D. 전송 게이트웨이를 다른 계정과 공유합니다. Transit Gateway에 VPC를 연결합니다. E. 필요에 따라 VPC 피어링을 프로비저닝합니다. F. 프라이빗 서브넷만 프로비저닝합니다. 전송 게이트웨이 및 고객 게이트웨이에서 필요한 경로를 열어 AWS의 아웃바운드 인터넷 트래픽이 데이터 센터에서 실행되는 NAT 서비스를 통해 흐를 수 있도록 합니다. Answer: B,D,F Explanation: Option A is incorrect because creating a Direct Connect gateway in the central account and creating an association proposal by using the Direct Connect gateway and the account ID for every virtual private gateway does not enable active-passive failover between the regions. A Direct Connect gateway is a globally available resource that enables you to connect your AWS Direct Connect connection over a private virtual interface (VIF) to one or more VPCs in any AWS Region. A virtual private gateway is the VPN concentrator on the Amazon side of a VPN connection. You can associate a Direct Connect gateway with either a transit gateway or a virtual private gateway. However, a Direct Connect gateway does not provide any load balancing or failover capabilities by itself1 Option B is correct because creating a Direct Connect gateway and a transit gateway in the central network account and attaching the transit gateway to the Direct Connect gateway by using a transit VIF meets the requirement of enabling the corporate network to access the resources on AWS seamlessly and also to communicate with all the VPCs. A transit VIF is a type of private VIF that you can use to connect your AWS Direct Connect connection to a transit gateway or a Direct Connect gateway. A transit gateway is a network transit hub that you can use to interconnect your VPCs and on-premises networks. By using a transit VIF, you can route traffic between your on-premises network and multiple VPCs across different AWS accounts and Regions through a single connection23 Option C is incorrect because provisioning an internet gateway, attaching the internet gateway to subnets, and allowing internet traffic through the gateway does not meet the requirement of routing cloud resources to the internet through its onpremises data center. An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between your VPC and the internet. An internet gateway serves two purposes: to provide a target in your VPC route tables for internet-routable traffic, and to perform network address translation (NAT) for instances that have been assigned public IPv4 addresses. By using an internet gateway, you are routing cloud resources directly to the internet, not through your on-premises data center. Option D is correct because sharing the transit gateway with other accounts and attaching VPCs to the transit gateway meets the requirement of enabling the corporate network to access the resources on AWS seamlessly and also to communicate with all the VPCs. You can share your transit gateway with other AWS accounts within the same organization by using AWS Resource Access Manager (AWS RAM). This allows you to centrally manage connectivity from multiple accounts without having to create individual peering connections between VPCs or duplicate network appliances in each account. You can attach VPCs from different accounts and Regions to your shared transit gateway and enable routing between them. Option E is incorrect because provisioning VPC peering as necessary does not meet the requirement of enabling the corporate network to access the resources on AWS seamlessly and also to communicate with all the VPCs. VPC peering is a networking connection between two VPCs that enables you to route traffic between them using private IPv4 addresses or IPv6 addresses. You can create a VPC peering connection between your own VPCs, or with a VPC in another AWS account within a single Region. However, VPC peering does not allow you to route traffic from your on-premises network to your VPCs or between multiple Regions. You would need to create multiple VPN connections or Direct Connect connections for each VPC peering connection, which increases operational complexity and costs. Option F is correct because provisioning only private subnets, opening the necessary route on the transit gateway and customer gateway to allow outbound internet traffic from AWS to flow through NAT services that run in the data center meets the requirement of routing cloud resources to the internet through its on-premises data center. A private subnet is a subnet that's associated with a route table that has no route to an internet gateway. Instances in a private subnet can communicate with other instances in the same VPC but cannot access resources on the internet directly. To enable outbound internet access from instances in private subnets, you can use NAT devices such as NAT gateways or NAT instances that are deployed in public subnets. A public subnet is a subnet that's associated with a route table that has a route to an internet gateway. Alternatively, you can use your on-premises data center as a NAT device by configuring routes on your transit gateway and customer gateway that direct outbound internet traffic from your private subnets through your VPN connection or Direct Connect connection. This way, you can route cloud resources to the internet through your on-premises data center instead of using an internet gateway.
QUESTION NO: 62 한 회사가 단일 AWS 계정에서 여러 워크로드를 실행하고 있습니다. 새로운 회사 정책에 따르면 엔지니어는 승인된 리소스만 프로비저닝할 수 있으며 엔지니어는 AWS CloudFormation을 사용하여 이러한 리소스를 프로비저닝해야 합니다. 솔루션 설계자는 엔지니어가 액세스에 사용하는 IAM 역할에 새로운 제한을 적용하는 솔루션을 만들어야 합니다. 솔루션 설계자는 솔루션을 생성하기 위해 무엇을 해야 합니까? A. 승인된 리소스가 포함된 AWS CloudFormation 템플릿을 Amazon S3 버킷에 업로드합니다. Amazon S3 및 AWS CloudFormation에 대한 액세스만 허용하도록 엔지니어의 IAM 역할에 대한 IAM 정책을 업데이트합니다. AWS CloudFormation 템플릿을 사용하여 리소스를 프로비저닝합니다. B. 승인된 리소스 및 AWS CloudFormation의 프로비저닝만 허용하는 권한으로 엔지니어의 IAM 역할에 대한 IAM 정책을 업데이트합니다. AWS CloudFormation 템플릿을 사용하여 승인된 리소스로 스택을 생성합니다. C. AWS CloudFormation 작업만 허용하는 권한으로 엔지니어의 IAM 역할에 대한 IAM 정책을 업데이트합니다. 승인된 리소스를 프로비저닝할 수 있는 권한이 있는 새 IAM 정책을 생성하고 정책을 새 IAM 서비스 역할에 할당합니다. 스택 생성 중에 AWS CloudFormation에 IAM 서비스 역할을 할당합니다. D. AWS CloudFormation 스택에서 리소스를 프로비저닝합니다. 자체 AWS CloudFormation 스택에 대한 액세스만 허용하도록 엔지니어의 IAM 역할에 대한 IAM 정책을 업데이트합니다. Answer: B Explanation: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/security-bestpractices.html#use-iam-to-control-access https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-iamservicerole.html
QUESTION NO: 63 회사는 다양한 위치에 있는 AWS loT 센서에서 loT 플랫폼을 실행합니다. Application Load Balancer 뒤에서 실행되는 Amazon EC2 인스턴스에서 회사의 Node js API 서버로 데이터를 보냅니다. 데이터는 4TB 범용 스토리지를 사용하는 Amazon RDS MySQL DB 인스턴스에 저장됩니다. SSD 볼륨 회사가 현장에 배치한 센서의 수는 시간이 지남에 따라 증가했으며 크게 증가할 것으로 예상됩니다. API 서버는 지속적으로 과부하 상태이며 RDS 메트릭은 높은 쓰기 대기 시간을 나타냅니다. 다음 단계 중 문제를 영구적으로 해결하고 성장을 가능하게 하는 단계는 무엇입니까? 이 플랫폼을 비용 효율적으로 유지하면서 새로운 센서가 프로비저닝됨에 따라? {2개를 선택합니다.) A. 볼륨의 IOPS를 개선하기 위해 MySQL 범용 SSD 스토리지의 크기를 6TB로 조정 B. RDS MySQL DB 인스턴스 대신 Amazon Aurora를 사용하도록 데이터베이스 계층을 재설계하고 읽기 전용 복제본을 추가합니다. C. Am azon Kinesis Data Streams 및 AWS Lambda를 활용하여 원시 데이터 수집 및 처리 D. AWS X-Ray를 사용하여 애플리케이션 문제를 분석 및 디버깅하고 부하에 맞게 API 서버를 더 추가합니다. E. RDS MySQL DB 인스턴스 대신 Amazon DynamoDB를 사용하도록 데이터베이스 계층 재설계 Answer: C,E Explanation: Option C is correct because leveraging Amazon Kinesis Data Streams and AWS Lambda to ingest and process the raw data resolves the issues permanently and enable growth as new sensors are provisioned. Amazon Kinesis Data Streams is a serverless streaming data service that simplifies the capture, processing, and storage of data streams at any scale. Kinesis Data Streams can handle any amount of streaming data and process data from hundreds of thousands of sources with very low latency. AWS Lambda is a serverless compute service that lets you run code without provisioning or managing servers. Lambda can be triggered by Kinesis Data Streams events and process the data records in real time. Lambda can also scale automatically based on the incoming data volume. By using Kinesis Data Streams and Lambda, the company can reduce the load on the API servers and improve the performance and scalability of the data ingestion and processing layer3 Option E is correct because re-architecting the database tier to use Amazon DynamoDB instead of an RDS MySQL DB instance resolves the issues permanently and enable growth as new sensors are provisioned. Amazon DynamoDB is a fully managed key-value and document database that delivers single-digit millisecond performance at any scale. DynamoDB supports auto scaling, which automatically adjusts read and write capacity based on actual traffic patterns. DynamoDB also supports on-demand capacity mode, which instantly accommodates up to double the previous peak traffic on a table. By using DynamoDB instead of RDS MySQL DB instance, the company can eliminate high write latency and improve scalability and performance of the database tier.
QUESTION NO: 64 회사에는 각각 AWS에 별도의 계정이 있는 여러 비즈니스 단위가 있습니다. 각 사업부는 CIDR 범위가 겹치는 여러 VPC로 자체 네트워크를 관리합니다. 회사의 마케팅 팀은 새로운 내부 응용 프로그램을 만들었고 다른 모든 비즈니스 단위에서 응용 프로그램에 액세스할 수 있도록 하려고 합니다. 솔루션은 사설 IP 주소만 사용해야 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 각 사업부에 지시하여 사업부의 VPC에 고유한 보조 CIDR 범위를 추가합니다. VPC를 피어링하고 보조 범위의 프라이빗 NAT 게이트웨이를 사용하여 트래픽을 마케팅 팀으로 라우팅합니다. B. 마케팅 계정의 VPC에서 가상 어플라이언스 역할을 할 Amazon EC2 인스턴스를 생성합니다. 마케팅 팀과 각 사업부의 VPC 간에 AWS Site-to-Site VPN 연결을 생성합니다. 필요한 경우 NAT를 수행합니다. C. 마케팅 애플리케이션을 공유할 AWS PrivateLink 엔드포인트 서비스를 생성합니다. 서비스에 연결할 수 있도록 특정 AWS 계정에 권한을 부여합니다. 프라이빗 IP 주소를 사용하여 애플리케이션에 액세스하려면 다른 계정에 인터페이스 VPC 엔드포인트를 생성합니다. D. 프라이빗 서브넷의 마케팅 애플리케이션 앞에 NLB(Network Load Balancer)를 생성합니다. API 게이트웨이 API를 생성합니다. Amazon API Gateway 프라이빗 통합을 사용하여 API를 NLB에 연결합니다. API에 대한 IAM 승인을 활성화합니다. 다른 사업부의 계정에 대한 액세스 권한을 부여합니다. Answer: C Explanation: With AWS PrivateLink, the marketing team can create an endpoint service to share their internal application with other accounts securely using private IP addresses. They can grant permission to specific AWS accounts to connect to the service and create interface VPC endpoints in the other accounts to access the application by using private IP addresses. This option does not require any changes to the network of the other business units, and it does not require peering or NATing. This solution is both scalable and secure. https://aws.amazon.com/blogs/networking-and-content-delivery/connecting-networks-withoverlapping-ip-ranges/
QUESTION NO: 65 한 회사가 Application Load Balancer 뒤에 있는 Auto Scaling 그룹의 여러 Amazon EC2 인스턴스에서 애플리케이션을 실행하고 있습니다. 애플리케이션의 부하는 하루 종일 달라지며 EC2 인스턴스는 정기적으로 확장 및 축소됩니다. EC2 인스턴스의 로그 파일은 15분마다 중앙 Amazon S3 버킷에 복사됩니다. 보안 팀은 종료된 일부 EC2 인스턴스에서 로그 파일이 누락되었음을 발견했습니다. 로그 파일이 종료된 EC2 인스턴스에서 중앙 S3 버킷으로 복사되도록 하는 일련의 작업은 무엇입니까? A. Amazon S3에 로그 파일을 복사하는 스크립트를 생성하고 스크립트를 EC2 인스턴스의 파일에 저장합니다. Auto Scaling 그룹에서 수명 주기 이벤트를 감지하는 Auto Scaling 수명 주기 후크 및 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. autoscaling:EC2_INSTANCE_TERMINATING 전환 시 AWS Lambda 함수를 호출하여 Auto Scaling 그룹에 ABANDON을 전송하여 종료를 방지하고 스크립트를 실행하여 로그 파일을 복사하고 AWS SDK를 사용하여 인스턴스를 종료합니다. B. 로그 파일을 Amazon S3에 복사하는 스크립트로 AWS Systems Manager 문서를 생성합니다. Auto Scaling 그룹에서 수명 주기 이벤트를 감지하는 Auto Scaling 수명 주기 후크 및 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. autoscaling:EC2_INSTANCE_TERMINATING 전환에서 AWS Lambda 함수를 호출하여 AWS Systems Manager API SendCommand 작업을 호출하여 문서를 실행하여 로그 파일을 복사하고 CONTINUE를 Auto Scaling 그룹에 전송하여 인스턴스를 종료합니다. C. 로그 전송 속도를 5분 간격으로 변경합니다. Amazon S3에 로그 파일을 복사하는 스크립트를 생성하고 EC2 인스턴스 사용자 데이터에 스크립트를 추가합니다. Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성하여 EC2 인스턴스 종료를 감지합니다. AWS CLI를 사용하여 사용자 데이터 스크립트를 실행하여 로그 파일을 복사하고 인스턴스를 종료하는 EventBridge(CloudWatch 이벤트) 규칙에서 AWS Lambda 함수를 호출합니다. D. Amazon S3에 로그 파일을 복사하는 스크립트로 AWS Systems Manager 문서를 생성합니다. Amazon Simple Notification Service(Amazon SNS) 주제에 메시지를 게시하는 Auto Scaling 수명 주기 후크를 생성합니다. SNS 알림에서 AWS Systems Manager API SendCommand 작업을 호출하여 문서를 실행하여 로그 파일을 복사하고 ABANDON을 Auto Scaling 그룹에 전송하여 인스턴스를 종료합니다. Answer: B Explanation: https://docs.aws.amazon.com/autoscaling/ec2/userguide/adding-lifecycle-hooks.html - Refer to Default Result section - If the instance is terminating, both abandon and continu e allow the instance to terminate. However, abandon stops any remaining actions, such as other lifecycle hooks, and continue allows any other lifecycle hooks to complete. https://aws.amazon.com/blogs/infrastructure-and-automation/run-code-before-terminating-anec2- auto-scaling-instance/ https://github.com/aws-samples/aws-lambda-lifecycle-hooks-function https://github.com/aws-samples/aws-lambda-lifecycle-hooksfunction/ blob/master/cloudformation/template.yaml
QUESTION NO: 66 회사에서 사용자 인증을 위해 온프레미스 Active Directory 서비스를 사용하고 있습니다. 회사는 동일한 인증 서비스를 사용하여 AWS Organizations를 사용하는 회사의 AWS 계정에 로그인하려고 합니다. 온프레미스 환경과 회사의 모든 AWS 계정 간에 AWS Site-to-Site VPN 연결이 이미 존재합니다. 회사의 보안 정책에 따라 사용자 그룹 및 역할에 따라 계정에 대한 조건부 액세스가 필요합니다. 사용자 ID는 단일 위치에서 관리되어야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. SAML 2.0을 사용하여 Active Directory에 연결하도록 AWS Single Sign-On(AWS SSO)을 구성합니다. SCIM(System for Cross-domain Identity Management) v2.0 프로토콜을 사용하여 자동 프로비저닝을 활성화합니다. ABAC(속성 기반 액세스 제어)를 사용하여 AWS 계정에 대한 액세스 권한을 부여합니다. B. AWS SSO를 자격 증명 소스로 사용하여 AWS Single Sign-On(AWS SSO)을 구성합니다. SCIM(System for Cross-domain Identity Management) v2.0 프로토콜을 사용하여 자동 프로비저닝을 활성화합니다. AWS SSO 권한 집합을 사용하여 AWS 계정에 대한 액세스 권한을 부여합니다. C. 회사의 AWS 계정 중 하나에서 SAML 2.0 자격 증명 공급자를 사용하도록 AWS Identity and Access Management(IAM)를 구성합니다. 연동 사용자에 매핑된 IAM 사용자를 프로비저닝합니다. Active Directory의 적절한 그룹에 해당하는 액세스 권한을 부여합니다. 교차 계정 IAM 사용자를 사용하여 필요한 AWS 계정에 대한 액세스 권한을 부여합니다. D. 회사의 AWS 계정 중 하나에서 OIDC(OpenID Connect) 자격 증명 공급자를 사용하도록 AWS Identity and Access Management(IAM)를 구성합니다. Active Directory의 적절한 그룹에 해당하는 연동 사용자의 AWS 계정에 대한 액세스 권한을 부여하는 IAM 역할을 프로비저닝합니다. 교차 계정 IAM 역할을 사용하여 필요한 AWS 계정에 대한 액세스 권한을 부여합니다. Answer: D Explanation: https://aws.amazon.com/blogs/aws/new-attributes-based-access-control-with-aws-singlesign-on/
QUESTION NO: 67 한 회사에서 AWS CloudFormation 스택에 배포된 AWS Lambda를 기반으로 애플리케이션을 구축했습니다. 웹 애플리케이션의 마지막 프로덕션 릴리스에서 몇 분 동안 중단되는 문제가 발생했습니다. 솔루션 설계자는 카나리아 릴리스를 지원하도록 배포 프로세스를 조정해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 새로 배포된 Lambda 함수 버전마다 별칭을 만듭니다. 라우팅 구성 파라미터와 함께 AWS CLI update-alias 명령을 사용하여 로드를 분산합니다. B. 애플리케이션을 새 CloudFormation 스택에 배포합니다. Amazon Route 53 가중 라우팅 정책을 사용하여 로드를 분산합니다. C. 새로 배포된 모든 Lambda 함수에 대한 버전을 생성합니다. 라우팅 구성 파라미터와 함께 AWS CLI update-function-contiguration 명령을 사용하여 로드를 분산합니다. D. AWS CodeDeploy를 구성하고 배포 구성에서 CodeDeployDefault.OneAtATime을 사용하여 로드를 분산합니다. Answer: A Explanation: https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambdafunctions-with-alias-traffic-shifting/
QUESTION NO: 68 한 회사에서 AWS Organizations의 조직을 사용하여 수백 개의 AWS 계정을 관리하고 있습니다. 솔루션 설계자는 OWASP(Open Web Application Security Project) 상위 10개 웹 애플리케이션 취약성에 대한 기본 보호를 제공하는 솔루션을 개발하고 있습니다. 솔루션 설계자는 조직 내에 배포된 모든 기존 및 신규 Amazon CloudFront 배포에 AWS WAF를 사용하고 있습니다. 솔루션 설계자가 기본 보호를 제공하기 위해 수행해야 하는 단계의 조합은 무엇입니까? (3개를 선택하세요.) A. 모든 계정에서 AWS Config를 활성화합니다. B. 모든 계정에서 Amazon GuardDuty를 활성화합니다. C. 조직의 모든 기능을 활성화합니다. D. AWS Firewall Manager를 사용하여 모든 CloudFront 배포의 모든 계정에 AWS WAF 규칙을 배포합니다. E. AWS Shield Advanced를 사용하여 모든 CloudFront 배포의 모든 계정에 AWS WAF 규칙을 배포합니다. F. AWS Security Hub를 사용하여 모든 CloudFront 배포의 모든 계정에 AWS WAF 규칙을 배포합니다. Answer: C,D,E Explanation: Enabling all features for the organization will enable using AWS Firewall Manager to centrally configure and manage firewall rules across multiple AWS accounts1. Using AWS Firewall Manager to deploy AWS WAF rules in all accounts for all CloudFront distributions will enable providing baseline protection for the OWASP top 10 web application vulnerabilities2. AWS Firewall Manager supports AWS WAF rules that can help protect against common web exploits such as SQL injection and cross-site scripting3. Configuring redirection of HTTP requests to HTTPS requests in CloudFront will enable encrypting the data in transit using SSL/TLS.
QUESTION NO: 69 솔루션 설계자는 AWS 클라우드에서 호스팅되는 애플리케이션을 개선해야 합니다. 애플리케이션은 과부하 연결이 발생하는 Amazon Aurora MySQL DB 인스턴스를 사용합니다. 대부분의 애플리케이션 작업은 레코드를 데이터베이스에 삽입합니다. 애플리케이션은 현재 텍스트 기반 구성 파일에 자격 증명을 저장합니다. 솔루션 설계자는 애플리케이션이 현재 연결 로드를 처리할 수 있도록 솔루션을 구현해야 합니다. 솔루션은 자격 증명을 안전하게 유지하고 정기적으로 자격 증명을 자동으로 교체하는 기능을 제공해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. DB 인스턴스 앞에 Amazon RDS 프록시 계층을 배포합니다. 연결 자격 증명을 AWS Secrets Manager에 비밀로 저장합니다. B. DB 인스턴스 앞에 Amazon RDS 프록시 계층을 배포합니다. AWS Systems Manager Parameter Store에 연결 자격 증명을 저장합니다. C. Aurora 복제본을 생성합니다. 연결 자격 증명을 AWS Secrets Manager에 비밀로 저장합니다. D. Aurora 복제본을 생성합니다. AWS Systems Manager Parameter Store에 연결 자격 증명을 저장합니다. Answer: A Explanation: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/rds-proxy.html
QUESTION NO: 70 회사는 Amazon RDS에서 Microsoft SQL Server DB 인스턴스에 액세스해야 하는 AWS Lambda 함수를 사용하여 서버리스 아키텍처를 구현하고 있습니다. 이 회사는 데이터베이스 시스템의 복제본을 포함하여 개발 및 생산을 위한 별도의 환경을 가지고 있습니다. 회사의 개발자는 개발 데이터베이스의 자격 증명에 액세스할 수 있습니다. 그러나 프로덕션 데이터베이스의 자격 증명은 IT 보안 팀의 IAM 사용자 그룹 구성원만 액세스할 수 있는 키로 암호화되어야 합니다. 이 키는 정기적으로 순환되어야 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 프로덕션 환경에서 무엇을 해야 합니까? A. AWS Key Management Service(AWS KMS) 고객 관리형 키로 암호화된 SecureString 파라미터를 사용하여 AWS Systems Manager Parameter Store에 데이터베이스 자격 증명을 저장합니다. SecureString 파라미터에 대한 액세스를 제공하기 위해 각 Lambda 함수에 역할을 연결합니다. IT 보안 팀만 매개 변수와 키에 액세스할 수 있도록 Securestring 매개 변수 및 고객 관리 키에 대한 액세스를 제한합니다. B. AWS Key Management Service(AWS KMS) 기본 Lambda 키를 사용하여 데이터베이스 자격 증명을 암호화합니다. 각 Lambda 함수의 환경 변수에 자격 증명을 저장합니다. Lambda 코드의 환경 변수에서 자격 증명을 로드합니다. IT 보안 팀만 키에 액세스할 수 있도록 KMS 키에 대한 액세스를 제한합니다. C. 각 Lambda 함수의 환경 변수에 데이터베이스 자격 증명을 저장합니다. AWS Key Management Service(AWS KMS) 고객 관리형 키를 사용하여 환경 변수를 암호화합니다. IT 보안 팀만 키에 액세스할 수 있도록 고객 관리형 키에 대한 액세스를 제한합니다. D. 데이터베이스 자격 증명을 AWS Secrets Manager에 AWS Key Management Service(AWS KMS) 고객과 연결된 암호로 저장합니다. Answer: D managed key. Attach a role to each Lambda function to provide access to the secret. Restrict access to the secret and the customer managed key so that only the IT security team can access the secret and the key. Explanation: Storing the database credentials in AWS Secrets Manager as a secret that is associated with an AWS Key Management Service (AWS KMS) customer managed key will enable encrypting and managing the credentials securely1. AWS Secrets Manager helps you to securely encrypt, store, and retrieve credentials for your databases and other services2. Attaching a role to each Lambda function to provide access to the secret will enable retrieving the credentials programmatically1. Restricting access to the secret and the customer managed key so that only members of the IT security team's IAM user group can access them will enable meeting the security requirements1.
QUESTION NO: 71 한 회사에 데이터 계층이 단일 AWS 리전에 배포된 중요한 애플리케이션이 있습니다. 데이터 계층은 Amazon DynamoDB 테이블과 Amazon Aurora MySQL DB 클러스터를 사용합니다. 현재 Aurora MySQL 엔진 버전은 글로벌 데이터베이스를 지원합니다. 애플리케이션 계층은 이미 두 지역에 배포되었습니다. 회사 정책에 따르면 중요한 애플리케이션에는 애플리케이션 계층 구성 요소와 데이터 계층 구성 요소가 두 지역에 걸쳐 배포되어야 합니다. RTO 및 RPO는 각각 몇 분을 넘지 않아야 합니다. 솔루션 설계자는 데이터 계층이 회사 정책을 준수하도록 하는 솔루션을 권장해야 합니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개 선택하세요.) A. Aurora MySQL DB 클러스터에 다른 리전 추가 B. Aurora MySQL DB 클러스터의 각 테이블에 다른 리전 추가 C. DynamoDB 테이블 및 Aurora MySQL DB 클러스터에 대한 예약된 교차 리전 백업 설정 D. 구성에 다른 리전을 추가하여 기존 DynamoDB 테이블을 전역 테이블로 변환 E. Amazon Route 53 애플리케이션 복구 컨트롤러를 사용하여 보조 리전으로 데이터베이스 백업 및 복구 자동화 Answer: A,D Explanation: The company should use Amazon Aurora global database and Amazon DynamoDB global table to deploy the data tier components across two Regions. Amazon Aurora global database is a feature that allows a single Aurora database to span multiple AWS Regions, enabling low-latency global reads and fast recovery from Region-wide outages1. Amazon DynamoDB global table is a feature that allows a single DynamoDB table to span multiple AWS Regions, enabling low-latency global reads and writes and fast recovery from Regionwide outages2. Reference: https://aws.amazon.com/rds/aurora/global-database/ https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/globaltables_HowItW orks.html https://aws.amazon.com/route53/application-recovery-controller/
QUESTION NO: 72 회사는 사용자가 문서를 업로드하는 전자 문서 관리 시스템을 구축하고 있습니다. 애플리케이션 스택은 완전히 서버리스이며 eu-central-1 리전의 AWS에서 실행됩니다. 이 시스템에는 Amazon S3를 원본으로 사용하여 Amazon CloudFront 배포를 사용하는 웹 애플리케이션이 포함되어 있습니다. 웹 애플리케이션은 Amazon API Gateway 지역 엔드포인트와 통신합니다. API Gateway API는 Amazon Aurora Serverless 데이터베이스에 메타데이터를 저장하고 문서를 S3 버킷에 저장하는 AWS Lambda 함수를 호출합니다. 이 회사는 꾸준히 성장하고 있으며 가장 큰 고객과 개념 증명을 완료했습니다. 회사는 유럽 밖에서 대기 시간을 개선해야 합니다. 이러한 요구 사항을 충족하는 작업 조합은 무엇입니까? (2개를 선택하세요.) A. S3 버킷에서 S3 Transfer Acceleration을 활성화합니다. 웹 애플리케이션이 Transfer Acceleration 서명 URL을 사용하는지 확인하십시오. B. AWS Global Accelerator에서 액셀러레이터를 생성합니다. 가속기를 CloudFront 배포에 연결합니다. C. API Gateway 지역 엔드포인트를 엣지 최적화 엔드포인트로 변경합니다. D. 전 세계에 흩어져 있는 다른 두 위치에 전체 스택을 프로비저닝합니다. Aurora 서버리스 클러스터에서 글로벌 데이터베이스를 사용하십시오. E. Lambda 함수와 Aurora Serverless 데이터베이스 사이에 Amazon RDS 프록시를 추가합니다. Answer: A,C Explanation: https://aws.amazon.com/global-accelerator/faqs/
QUESTION NO: 73 회사는 온프레미스에서 호스팅하는 애플리케이션에서 메타데이터를 수집하기 위해 서비스를 사용합니다. TV 및 인터넷 라디오와 같은 소비자 장치는 애플리케이션에 액세스합니다. 많은 구형 장치는 특정 HTTP 헤더를 지원하지 않으며 이러한 헤더가 응답에 있을 때 오류를 표시합니다. 회사는 User-Agent 헤더로 회사에서 식별한 이전 장치로 전송된 응답에서 지원되지 않는 헤더를 제거하도록 온프레미스 로드 밸런서를 구성했습니다. 회사는 서비스를 AWS로 마이그레이션하고 서버리스 기술을 채택하고 이전 장치를 지원할 수 있는 기능을 유지하려고 합니다. 회사는 이미 애플리케이션을 AWS Lambda 함수 세트로 마이그레이션했습니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 메타데이터 서비스를 위한 Amazon CloudFront 배포를 생성합니다. Application Load Balancer(ALB)를 생성합니다. 요청을 ALB로 전달하도록 CloudFront 배포를 구성합니다. 각 요청 유형에 대해 올바른 Lambda 함수를 호출하도록 ALB를 구성합니다. User-Agent 헤더의 값을 기반으로 문제가 있는 헤더를 제거하는 CloudFront 함수를 생성합니다. B. 메타데이터 서비스에 대한 Amazon API Gateway REST API를 생성합니다. 각 요청 유형에 대해 올바른 Lambda 함수를 호출하도록 API Gateway를 구성합니다. User-Agent 헤더의 값을 기반으로 문제가 있는 헤더를 제거하도록 기본 게이트웨이 응답을 수정합니다. C. 메타데이터 서비스에 대한 Amazon API Gateway HTTP API를 생성합니다. 각 요청 유형에 대해 올바른 Lambda 함수를 호출하도록 API Gateway를 구성합니다. User-Agent의 값을 기반으로 문제가 있는 헤더를 제거하는 응답 매핑 템플릿을 만듭니다. 응답 데이터 매핑을 HTTP API와 연결합니다. D. 메타데이터 서비스에 대한 Amazon CloudFront 배포를 생성합니다. Application Load Balancer(ALB)를 생성합니다. 요청을 ALB로 전달하도록 CloudFront 배포를 구성합니다. 각 요청 유형에 대해 올바른 Lambda 함수를 호출하도록 ALB를 구성합니다. User-Agent 헤더 값을 기반으로 최종 사용자 요청에 대한 응답으로 문제가 있는 헤더를 제거하는 Lambda@Edge 함수를 생성합니다. Answer: D Explanation: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambdaexamples.html
QUESTION NO: 74 비디오 처리 회사는 회사의 온프레미스 네트워크 연결 스토리지 시스템에 수천 개의 파일로 저장된 600TB의 압축 데이터를 사용하여 기계 학습(ML) 모델을 구축하려고 합니다. 이 회사는 ML 실험에 필요한 컴퓨팅 리소스가 온프레미스에 없으며 AWS를 사용하려고 합니다. 회사는 3주 이내에 AWS로 데이터 전송을 완료해야 합니다. 데이터 전송은 일회성 전송입니다. 데이터는 전송 중에 암호화되어야 합니다. 회사 인터넷 연결의 측정된 업로드 속도는 100Mbps이며 여러 부서에서 연결을 공유합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까? A. AWS Management Console을 사용하여 여러 AWS Snowball Edge Storage Optimized 장치를 주문하십시오. 대상 S3 버킷으로 디바이스를 구성합니다. 데이터를 장치에 복사합니다. 기기를 AWS로 반송합니다. B. 회사 위치와 가장 가까운 AWS 리전 간에 10Gbps AWS Direct Connect 연결을 설정합니다. VPN 연결을 통해 데이터를 리전으로 전송하여 Amazon S3에 데이터를 저장합니다. C. 온프레미스 네트워크 스토리지와 가장 가까운 AWS 리전 간에 VPN 연결을 생성합니다. VPN 연결을 통해 데이터를 전송합니다. D. 온프레미스에 AWS Storage Gateway 파일 게이트웨이를 배포합니다. 대상 S3 버킷으로 파일 게이트웨이를 구성합니다. 파일 게이트웨이에 데이터를 복사합니다. Answer: A Explanation: This solution will meet the requirements of the company as it provides a secure, costeffective and fast way of transferring large data sets from on-premises to AWS. Snowball Edge devices encrypt the data during transfer, and the devices are shipped back to AWS for import into S3. This option is more cost effective than using Direct Connect or VPN connections as it does not require the company to pay for long-term dedicated connections.
QUESTION NO: 75 솔루션 아키텍트는 비용을 최적화하고 단일 AWS 계정에서 Amazon EC2 인스턴스의 크기를 적절하게 조정하려고 합니다. 솔루션 아키텍트는 인스턴스가 CPU, 메모리 및 네트워크 메트릭을 기반으로 최적화되었는지 확인하려고 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까? (2개 선택하세요.) A. 해당 계정에 대해 AWS Business Support 또는 AWS Enterprise Support를 구매합니다. B. AWS Trusted Advisor를 켜고 "사용률이 낮은 Amazon EC2 인스턴스" 권장 사항을 검토하십시오. C. Amazon CloudWatch 에이전트를 설치하고 EC2 인스턴스에서 메모리 지표 수집을 구성합니다. D. 결과 및 최적화 권장 사항을 수신하도록 AWS 계정에서 AWS Compute Optimizer를 구성합니다. E. AWS 리전, 인스턴스 제품군 및 관심 있는 운영 체제에 대한 EC2 인스턴스 절감 계획을 생성합니다. Answer: B,D Explanation: AWS Trusted Advisor is a service that provides real-time guidance to help users provision their resources following AWS best practices1. One of the Trusted Advisor checks is "Low Utilization Amazon EC2 Instances", which identifies EC2 instances that appear to be underutilized based on CPU, network I/O, and disk I/O metrics1. This check can help users optimize the cost and size of their EC2 instances by recommending smaller or more appropriate instance types. AWS Compute Optimizer is a service that analyzes the configuration and utilization metrics of AWS resources and generates optimization recommendations to reduce the cost and improve the performance of workloads2. Compute Optimizer supports four types of AWS resources: EC2 instances, EBS volumes, ECS services on AWS Fargate, and Lambda functions2. For EC2 instances, Compute Optimizer evaluates the vCPUs, memory, storage, and other specifications, as well as the CPU utilization, network in and out, disk read and write, and other utilization metrics of currently running instances3. It then recommends optimal instance types based on price-performance trade-offs. Option A is incorrect because purchasing AWS Business Support or AWS Enterprise Support for the account will not directly help with cost-optimization and sizing of EC2 instances. However, these support plans do provide access to more Trusted Advisor checks than the basic support plan1. Option C is incorrect because installing the Amazon CloudWatch agent and configuring memory metric collection on the EC2 instances will not provide any optimization recommendations by itself. However, memory metrics can be used by Compute Optimizer to enhance its recommendations if enabled3. Option E is incorrect because creating an EC2 Instance Savings Plan for the AWS Regions, instance families, and operating systems of interest will not help with cost-optimization and sizing of EC2 instances. Savings Plans are a flexible pricing model that offer lower prices on Amazon EC2 usage in exchange for a commitment to a consistent amount of usage for a 1- or 3-year term4. Savings Plans do not affect the configuration or utilization of EC2 instances.
QUESTION NO: 76 회사는 Amazon S3에서 정적 웹 사이트로 새 애플리케이션을 실행합니다. 이 회사는 프로덕션 AWS 계정에 애플리케이션을 배포했으며 Amazon CloudFront를 사용하여 웹 사이트를 제공합니다. 웹사이트는 Amazon API Gateway REST API를 호출합니다. AWS Lambda 함수는 각 API 메서드를 지원합니다. 회사는 2주마다 CSV 보고서를 생성하여 각 API Lambda 함수의 권장 구성 메모리, 권장 비용, 현재 구성과 권장 사항 간의 가격 차이를 표시하려고 합니다. 회사는 보고서를 S3 버킷에 저장합니다. 최소 개발 시간으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 2주 동안 Amazon CloudWatch Logs에서 각 API Lambda 함수에 대한 메트릭 데이터를 추출하는 Lambda 함수를 생성합니다. 데이터를 표 형식으로 수집합니다. 데이터를 S3 버킷에 _csvfile로 저장합니다. Amazon Eventaridge 규칙을 생성하여 Lambda 함수가 2주마다 실행되도록 예약합니다. B. AWS Compute Optimizer에 옵트인합니다. ExportLambdaFunctionRecommendatlons 작업을 호출하는 Lambda 함수를 생성합니다. _csv 파일을 S3 버킷으로 내보냅니다. Amazon Eventaridge 규칙을 생성하여 Lambda 함수가 2주마다 실행되도록 예약합니다. C. AWS Compute Optimizer에 옵트인합니다. 향상된 인프라 메트릭을 설정합니다. Compute Optimizer 콘솔 내에서 Lambda 권장 사항을 _csvfile_로 내보내는 작업을 예약합니다. 파일을 2주마다 S3 버킷에 저장합니다. D. 프로덕션 계정에 대한 AWS Business Support 플랜을 구매합니다. AWS Trusted Advisor 검사를 위해 AWS Compute Optimizer에 옵트인합니다. Trusted Advisor 콘솔에서 비용 최적화 검사를 _csvfile_로 내보내는 작업을 예약합니다. 파일을 2주마다 S3 버킷에 저장합니다. Answer: B Explanation: https://docs.aws.amazon.com/computeoptimizer/ latest/APIReference/API_ExportLambdaFunctionRecommendations.html
QUESTION NO: 77 회사는 Amazon EC2 인스턴스에서 기존 웹 애플리케이션을 실행하고 있습니다. 회사는 애플리케이션을 컨테이너에서 실행되는 마이크로서비스로 리팩터링해야 합니다. 애플리케이션의 별도 버전은 생산 및 테스트라는 두 가지 환경에 존재합니다. 애플리케이션에 대한 부하는 가변적이지만 최소 부하와 최대 부하가 알려져 있습니다. 솔루션 설계자는 운영 복잡성을 최소화하는 서버리스 아키텍처로 업데이트된 애플리케이션을 설계해야 합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까? A. 컨테이너 이미지를 함수로 AWS Lambda에 업로드합니다. 예상되는 최대 로드를 처리하도록 연결된 Lambda 함수에 대한 동시성 제한을 구성합니다. Amazon API Gateway 내에서 두 개의 별도 Lambda 통합을 구성합니다. 하나는 프로덕션용이고 다른 하나는 테스트용입니다. B. 컨테이너 이미지를 Amazon Elastic Container Registry(Amazon ECR)에 업로드합니다. 예상 로드를 처리하기 위해 Fargate 시작 유형을 사용하여 2개의 자동 확장 Amazon Elastic Container Service(Amazon ECS) 클러스터를 구성합니다. ECR 이미지에서 작업을 배포합니다. 두 개의 개별 Application Load Balancer를 구성하여 트래픽을 ECS 클러스터로 보냅니다. C. 컨테이너 이미지를 Amazon Elastic Container Registry(Amazon ECR)에 업로드합니다. 예상 로드를 처리하기 위해 Fargate 시작 유형을 사용하여 2개의 자동 확장 Amazon Elastic Kubernetes Service(Amazon EKS) 클러스터를 구성합니다. ECR 이미지에서 작업을 배포합니다. 두 개의 개별 Application Load Balancer를 구성하여 트래픽을 EKS 클러스터로 보냅니다. D. 컨테이너 이미지를 AWS Elastic Beanstalk에 업로드합니다. Elastic Beanstalk에서 프로덕션 및 테스트를 위한 별도의 환경과 배포를 생성합니다. 트래픽을 Elastic Beanstalk 배포로 보내도록 두 개의 개별 Application Load Balancer를 구성합니다. Answer: D Explanation: minimizes operational + microservices that run on containers = AWS Elastic Beanstalk
QUESTION NO: 78 회사는 Amazon API Gateway, Amazon DynamoDB 및 AWS Lambda를 사용하여 AWS에서 블로그 게시물 애플리케이션을 호스팅합니다. 애플리케이션은 현재 API 키를 사용하여 요청을 승인하지 않습니다. API 모델은 다음과 같습니다. GET/posts/[postid] 게시물 세부 정보를 가져옵니다. GET/users[userid] 사용자 세부 정보 가져오기 GET/comments/[commentid] 댓글 세부정보 가져오기 회사는 사용자가 댓글 섹션에서 주제에 대해 적극적으로 논의하고 있음을 확인했으며 댓글을 실시간으로 표시하여 사용자 참여를 늘리고자 합니다. 댓글 대기 시간을 줄이고 사용자 경험을 개선하려면 어떤 디자인을 사용해야 합니까? A. Amazon CloudFront와 함께 엣지 최적화 API를 사용하여 API 응답을 캐시합니다. B. 10초마다 GET 댓글[주석]을 요청하도록 블로그 애플리케이션 코드를 수정합니다. C. AWS AppSync를 사용하고 WebSocket을 활용하여 의견을 전달하십시오. D. Lambda 함수의 동시성 제한을 변경하여 API 응답 시간을 낮춥니다. Answer: C Explanation: https://docs.aws.amazon.com/appsync/latest/devguide/graphql-overview.html AWS AppSync is a fully managed GraphQL service that allows applications to securely access, manipulate, and receive data as well as real-time updates from multiple data sources1. AWS AppSync supports GraphQL subscriptions to perform real-time operations and can push data to clients that choose to listen to specific events from the backend1. AWS AppSync uses WebSockets to establish and maintain a secure connection between the clients and the API endpoint2. Therefore, using AWS AppSync and leveraging WebSockets is a suitable design to reduce comment latency and improve user experience.
QUESTION NO: 79 여러 AWS 계정이 있는 회사에서 AWS Organizations 및 서비스 제어 정책(SCP)을 사용하고 있습니다. 관리자가 다음 SCP를 생성하여 AWS 계정 1111-1111-1111이 포함된 조직 단위(OU)에 연결했습니다. 계정 1111-1111-1111에서 작업하는 개발자는 Amazon S3 버킷을 생성할 수 없다고 불평합니다. 관리자는 이 문제를 어떻게 해결해야 합니까? A. SCP에 "허용" 효과가 있는 s3:CreateBucket을 추가합니다. B. OU에서 계정을 제거하고 SCP를 계정 1111-1111-1111에 직접 연결합니다. C. 개발자에게 IAM 엔터티에 Amazon S3 권한을 추가하도록 지시합니다. D. 계정 1111-1111-1111에서 SCP를 제거합니다. Answer: C Explanation: However A's explanation is incorrect - https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html "SCPs are similar to AWS Identity and Access Management (IAM) permission policies and use almost the same syntax. However, an SCP never grants permissions." SCPs alone are not sufficient to granting permissions to the accounts in your organization. No permissions are granted by an SCP. An SCP defines a guardrail, or sets limits, on the actions that the account's administrator can delegate to the IAM users and roles in the affected accounts. The administrator must still attach identity-based or resource-based policies to IAM users or roles, or to the resources in your accounts to actually grant permissions. The effective permissions are the logical intersection between what is allowed by the SCP and what is allowed by the IAM and resource-based policies.
QUESTION NO: 80 ALB(Application Load Balancer) 뒤의 Amazon EC2 인스턴스에서 실행되는 웹 사이트가 있는 회사가 있습니다. 인스턴스는 Auto Scaling 그룹에 있습니다. ALB는 AWS WAF 웹 ACL과 연결됩니다. 웹 사이트는 종종 응용 프로그램 계층에서 공격에 직면합니다. 공격으로 인해 응용 프로그램 서버의 트래픽이 갑자기 크게 증가합니다. 액세스 로그는 각 공격이 서로 다른 IP 주소에서 시작되었음을 보여줍니다. 솔루션 설계자는 이러한 공격을 완화하기 위한 솔루션을 구현해야 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 서버 액세스를 모니터링하는 Amazon CloudWatch 경보를 생성합니다. IP 주소별 액세스를 기반으로 임계값을 설정합니다. 웹 ACL의 거부 목록에 IP 주소를 추가하는 경보 동작을 구성합니다. B. AWS WAF 외에 AWS Shield Advanced를 배포합니다. ALB를 보호 리소스로 추가합니다. C. 사용자 IP 주소를 모니터링하는 Amazon CloudWatch 경보를 생성합니다. IP 주소별 액세스를 기반으로 임계값을 설정합니다. 경보를 활성화하는 모든 IP 주소에 대해 애플리케이션 서버의 서브넷 라우팅 테이블에 거부 규칙을 추가하도록 AWS Lambda 함수를 호출하도록 경보를 구성합니다. D. 액세스 로그를 검사하여 공격을 시작한 IP 주소의 패턴을 찾습니다. Amazon Route 53 지리적 위치 라우팅 정책을 사용하여 해당 IP 주소를 호스팅하는 국가에서 오는 트래픽을 거부합니다. Answer: C Explanation: "The AWS WAF API supports security automation such as blacklisting IP addresses that exceed request limits, which can be useful for mitigating HTTP flood attacks." > https://aws.amazon.com/blogs/security/how-to-protect-dynamic-web-applications-againstddos-attacks-by-using-amazon-cloudfront-and-amazon-route-53/
QUESTION NO: 81 회사에서 정적 콘텐츠를 호스팅하는 새 웹 사이트를 디자인하고 있습니다. 이 웹사이트는 사용자에게 대용량 파일을 업로드하고 다운로드할 수 있는 기능을 제공합니다. 회사 요구 사항에 따라 모든 데이터는 전송 중 및 유휴 상태에서 암호화되어야 합니다. 솔루션 설계자는 Amazon S3 및 Amazon CloudFront를 사용하여 솔루션을 구축하고 있습니다. 어떤 단계 조합이 암호화 요구 사항을 충족합니까? (3개를 선택하세요.) A. 웹 애플리케이션이 사용하는 S3 버킷에 대해 S3 서버 측 암호화를 켭니다. B. S3 ACL의 읽기 및 쓰기 작업에 대해 "aws:SecureTransport": "true" 정책 속성을 추가합니다. C. 웹 애플리케이션이 사용하는 S3 버킷에서 암호화되지 않은 작업을 거부하는 버킷 정책을 생성합니다. D. AWS KMS 키(SSE-KMS)로 서버 측 암호화를 사용하여 CloudFront에서 유휴 암호화를 구성합니다. E. CloudFront에서 HTTP 요청의 HTTPS 요청으로의 리디렉션을 구성합니다. F. 웹 애플리케이션이 사용하는 S3 버킷에 대해 미리 서명된 URL을 생성할 때 RequireSSL 옵션을 사용하십시오. Answer: A,C,E Explanation: Turning on S3 server-side encryption for the S3 bucket that the web application uses will enable encrypting the data at rest using Amazon S3 managed keys (SSE-S3)1. Creating a bucket policy that denies any unencrypted operations in the S3 bucket that the web application uses will enable enforcing encryption for all requests to the bucket2. Configuring redirection of HTTP requests to HTTPS requests in CloudFront will enable encrypting the data in transit using SSL/TLS3.
QUESTION NO: 82 회사의 서버리스 애플리케이션에 대한 외부 감사에서 너무 많은 권한을 부여하는 IAM 정책이 드러났습니다. 이러한 정책은 회사의 AWS Lambda 실행 역할에 연결됩니다. 수백 개의 회사 Lambda 함수에는 Amazon S3 버킷 및 Amazon DynamoDB 테이블에 대한 전체 액세스와 같은 광범위한 액세스 권한이 있습니다. 회사는 각 기능이 작업을 완료하는 데 필요한 최소한의 권한만 갖기를 원합니다. 솔루션 설계자는 각 Lambda 함수에 필요한 권한을 결정해야 합니다. 솔루션 설계자는 최소한의 노력으로 이 요구 사항을 충족하기 위해 무엇을 해야 합니까? A. Lambda 함수를 프로파일링하고 AWS API 호출을 검색하도록 Amazon CodeGuru를 설정합니다. 각 Lambda 함수에 필요한 API 호출 및 리소스의 인벤토리를 생성합니다. 각 Lambda 함수에 대한 새 IAM 액세스 정책을 생성합니다. 새 정책을 검토하여 회사의 비즈니스 요구 사항을 충족하는지 확인하십시오. B. AWS 계정에 대한 AWS CloudTrail 로깅을 켭니다. AWS Identity and Access Management Access Analyzer를 사용하여 CloudTrail 로그에 기록된 활동을 기반으로 IAM 액세스 정책을 생성합니다. 생성된 정책을 검토하여 회사의 비즈니스 요구 사항을 충족하는지 확인하십시오. C. AWS 계정에 대한 AWS CloudTrail 로깅을 켭니다. CloudTrail 로그를 구문 분석하고 Lambda 실행 역할별로 AWS API 호출을 검색하고 요약 보고서를 생성하는 스크립트를 생성합니다. 보고서를 검토합니다. 각 Lambda 함수에 대해 더 제한적인 권한을 제공하는 IAM 액세스 정책을 만듭니다. D. AWS 계정에 대한 AWS CloudTrail 로깅을 켭니다. CloudTrail 로그를 Amazon S3로 내보냅니다. Amazon EMR을 사용하여 Amazon S3에서 CloudTrail 로그를 처리하고 각 실행 역할에서 사용하는 API 호출 및 리소스에 대한 보고서를 생성합니다. 각 역할에 대한 새 IAM 액세스 정책을 생성합니다. 생성된 역할을 S3 버킷으로 내보냅니다. 생성된 정책을 검토하여 회사의 비즈니스 요구 사항을 충족하는지 확인하십시오. Answer: B Explanation: IAM Access Analyzer helps you identify the resources in your organization and accounts, such as Amazon S3 buckets or IAM roles, shared with an external entity. This lets you identify unintended access to your resources and data, which is a security risk. IAM Access Analyzer identifies resources shared with external principals by using logic-based reasoning to analyze the resource-based policies in your AWS environment. https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html
QUESTION NO: 83 한 회사가 단일 Amazon EC2 인스턴스에서 중요한 애플리케이션을 호스팅하고 있습니다. 이 애플리케이션은 인 메모리 데이터 스토어를 위해 Redis 단일 노드 클러스터용 Amazon ElastiCache를 사용합니다. 이 애플리케이션은 관계형 데이터베이스에 Amazon RDS for MariaDB DB 인스턴스를 사용합니다. 애플리케이션이 작동하려면 인프라의 각 부분이 정상이어야 하고 활성 상태여야 합니다. 솔루션 설계자는 애플리케이션의 아키텍처를 개선하여 인프라가 가동 중지 시간을 최소화하면서 장애로부터 자동으로 복구할 수 있도록 해야 합니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (3개를 선택하세요.) A. Elastic Load Balancer를 사용하여 여러 EC2 인스턴스에 트래픽을 분산합니다. EC2 인스턴스가 최소 용량이 인스턴스 2개인 Auto Scaling 그룹의 일부인지 확인합니다. B. Elastic Load Balancer를 사용하여 여러 EC2 인스턴스에 트래픽 분산 EC2 인스턴스가 무제한 모드로 구성되었는지 확인합니다. C. 동일한 가용 영역에 읽기 전용 복제본을 생성하도록 DB 인스턴스를 수정합니다. 장애 시나리오에서 읽기 전용 복제본을 기본 DB 인스턴스로 승격합니다. D. 두 가용 영역에 걸쳐 확장되는 다중 AZ 배포를 생성하도록 DB 인스턴스를 수정합니다. E. Redis 클러스터용 ElastiCache에 대한 복제 그룹을 생성합니다. 최소 용량이 인스턴스 2개인 Auto Scaling 그룹을 사용하도록 클러스터를 구성합니다. F. Redis 클러스터용 ElastiCache에 대한 복제 그룹을 생성합니다. 클러스터에서 다중 AZ를 활성화합니다. Answer: A,D,F Explanation: Option A is correct because using an Elastic Load Balancer and an Auto Scaling group with a minimum capacity of two instances can improve the availability and scalability of the EC2 instances that host the application. The load balancer can distribute traffic across multiple instances and the Auto Scaling group can replace any unhealthy instances automatically1 Option D is correct because modifying the DB instance to create a Multi-AZ deployment that extends across two Availability Zones can improve the availability and durability of the RDS for MariaDB database. Multi-AZ deployments provide enhanced data protection and minimize downtime by automatically failing over to a standby replica in another Availability Zone in case of a planned or unplanned outage4 Option F is correct because creating a replication group for the ElastiCache for Redis cluster and enabling Multi-AZ on the cluster can improve the availability and fault tolerance of the in-memory data store. A replication group consists of a primary node and up to five read-only replica nodes that are synchronized with the primary node using asynchronous replication. Multi-AZ allows automatic failover to one of the replicas if the primary node fails or becomes unreachable6
QUESTION NO: 84 회사는 AWS Organizations의 회사 조직 내 개발자 계정 전체에서 AWS 데이터 전송 비용 및 컴퓨팅 비용을 최적화하려고 합니다. 개발자는 단일 AWS 리전에서 VPC를 구성하고 Amazon EC2 인스턴스를 시작할 수 있습니다. EC2 인스턴스는 Amazon에서 매일 약 1TB의 데이터를 검색합니다. S3 개발자 활동으로 인해 높은 컴퓨팅 비용과 함께 EC2 인스턴스와 S3 버킷 간의 과도한 월 데이터 전송 요금 및 NAT 게이트웨이 처리 요금이 발생합니다. AWS 계정 회사는 이러한 시행이 개발자가 작업을 수행할 수 있는 속도에 부정적인 영향을 미치기를 원하지 않습니다. 어떤 솔루션이 이러한 요구 사항을 가장 비용 효율적으로 충족할까요? A. 개발자가 승인되지 않은 EC2 인스턴스 유형을 시작하지 못하도록 SCP를 생성합니다. 개발자에게 AWS CloudFormation 템플릿을 제공하여 승인된 VPC 구성을 S3 인터페이스 엔드포인트로 배포합니다. 개발자* IAM 권한 범위를 지정하여 개발자가 CloudFormation에서만 VPC 리소스를 시작할 수 있도록 합니다. B. 개발자 계정 전체에서 EC2 컴퓨팅 비용 및 S3 데이터 전송 비용을 모니터링하기 위해 AWS 예산으로 일일 예측 예산을 생성합니다. 예측 비용이 실제 예산 비용의 75%이면 개발자 팀에 알림을 보냅니다. 예산 비용은 100%입니다. 개발자의 EC2 인스턴스 및 VPC 인프라를 종료하기 위한 예산 조치 생성 C. 사용자가 S3 게이트웨이 엔드포인트 및 승인된 EC2 인스턴스로 승인된 VPC 구성을 생성하는 데 사용할 수 있는 AWS Service Catalog 포트폴리오 생성 개발자 계정과 포트폴리오 공유 승인된 IAM 역할을 사용하도록 AWS Service Catalog 시작 제약 조건 구성 범위 AWS Service Catalog에 대한 액세스만 허용하는 개발자의 IAM 권한 D. AWS Config 규칙을 생성 및 배포하여 개발자 AWS 계정에서 EC2 및 VPC 리소스의 규정 준수를 모니터링합니다. 개발자가 승인되지 않은 EC2 인스턴스를 시작하거나 개발자가 S3 게이트웨이 엔드포인트 없이 VPC를 생성하는 경우 수정 작업을 수행하여 승인되지 않은 리소스를 종료합니다. Answer: C Explanation: This solution allows developers to quickly launch resources using pre-approved configurations and instance types, while also ensuring that the resources launched comply with the company's architectural patterns. This can help reduce data transfer and compute costs associated with the resources. Using AWS Service Catalog also allows the company to control access to the approved configurations and resources through the use of IAM roles, while also allowing developers to quickly provision resources without negatively affecting their ability to perform their tasks. Reference: AWS Service Catalog: https://aws.amazon.com/service-catalog/ AWS Service Catalog Constraints: https://docs.aws.amazon.com/servicecatalog/latest/adminguide/constraints.html AWS Service Catalog Launch Constraints: https://docs.aws.amazon.com/servicecatalog/latest/adminguide/launch-constraints.html
QUESTION NO: 85 솔루션 설계자는 AWS 계정의 Amazon S3 버킷에서 새 AWS 계정의 새 S3 버킷으로 데이터를 복사해야 합니다. 솔루션 설계자는 AWS CLI를 사용하는 솔루션을 구현해야 합니다. 어떤 단계 조합이 데이터를 성공적으로 복사합니까? (3개를 선택하세요.) A. 원본 버킷이 콘텐츠를 나열하고 대상 버킷에 개체를 넣고 개체 ACL을 설정하도록 허용하는 버킷 정책을 생성합니다. 대상 버킷에 버킷 정책을 연결합니다. B. 대상 계정의 사용자가 원본 버킷의 콘텐츠를 나열하고 원본 버킷의 개체를 읽을 수 있도록 허용하는 버킷 정책을 만듭니다. 버킷 정책을 원본 버킷에 연결합니다. C. 소스 계정에서 IAM 정책을 생성합니다. 원본 계정의 사용자가 원본 버킷의 콘텐츠를 나열하고 객체를 가져오고 콘텐츠를 나열하고 객체를 넣고 대상 버킷에서 객체 ACL을 설정할 수 있도록 정책을 구성합니다. 정책을 사용자 _에 연결 D. 대상 계정에 IAM 정책을 생성합니다. 대상 계정의 사용자가 원본 버킷의 콘텐츠를 나열하고 객체를 가져오고 대상 버킷의 콘텐츠를 나열하고 객체를 넣고 objectACL을 설정할 수 있도록 정책을 구성합니다. 정책을 사용자에게 연결합니다. E. 소스 계정의 사용자로 aws s3 sync 명령을 실행합니다. 데이터를 복사할 원본 및 대상 버킷을 지정합니다. F. 대상 계정의 사용자로 aws s3 sync 명령을 실행합니다. 데이터를 복사할 원본 및 대상 버킷을 지정합니다. Answer: B,D,F Explanation: Step B is necessary so that the user in the destination account has the necessary permissions to access the source bucket and list its contents, read its objects. Step D is needed so that the user in the destination account has the necessary permissions to access the destination bucket and list contents, put objects, and set object ACLs Step F is necessary because the aws s3 sync command needs to be run using the IAM user credentials from the destination account, so that the objects will have the appropriate permissions for the user in the destination account once they are copied.
QUESTION NO: 86 소매 회사는 유럽에 온프레미스 데이터 센터가 있습니다. 이 회사는 또한 eu-west-1 및 useast-1 리전을 포함하는 다중 리전 AWS를 보유하고 있습니다. 회사는 온프레미스 인프라에서 해당 리전 중 하나의 VPC로 네트워크 트래픽을 라우팅할 수 있기를 원합니다. 또한 회사는 해당 리전의 VPC 간에 직접 라우팅되는 트래픽을 지원해야 합니다. 네트워크에는 단일 장애 지점이 존재할 수 없습니다. 이 회사는 이미 온프레미스 데이터 센터에서 2개의 1Gbps AWS Direct Connect 연결을 생성했습니다. 각 연결은 고가용성을 위해 유럽의 별도 Direct Connect 위치로 이동합니다. 이 두 위치는 각각 DX-A 및 DX-B로 명명됩니다. 각 리전에는 해당 리전 내의 모든 VPC 간 트래픽을 라우팅하도록 구성된 단일 AWS Transit Gateway가 있습니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. DX-A 연결에서 Direct Connect 게이트웨이로 개인 VIF를 만듭니다. 고가용성을 위해 DX-B 연결에서 동일한 Direct Connect 게이트웨이로 프라이빗 VIF를 생성합니다. eu-west-1 및 useast-1 전송 게이트웨이를 모두 Direct Connect 게이트웨이와 연결합니다. 교차 리전 라우팅을 지원하기 위해 전송 게이트웨이를 서로 피어링합니다. B. DX-A 연결에서 Direct Connect 게이트웨이로 전송 VIF를 생성합니다. eu-west-1 전송 게이트웨이를 이 Direct Connect 게이트웨이와 연결합니다. DX-B 연결에서 별도의 Direct Connect 게이트웨이로 전송 VIF를 생성합니다. us-east-1 전송 게이트웨이를 이 별도의 Direct Connect 게이트웨이와 연결합니다. Direct Connect 게이트웨이를 서로 피어링하여 고가용성 및 리전 간 라우팅을 지원합니다. C. DX-A 연결에서 Direct Connect 게이트웨이로 전송 VIF를 생성합니다. 고가용성을 위해 DXB 연결에서 동일한 Direct Connect 게이트웨이로 전송 VIF를 생성합니다. eu-west-1 및 useast-1 전송 게이트웨이를 모두 이 Direct Connect 게이트웨이와 연결합니다. 전송 게이트웨이 간에 트래픽을 라우팅하도록 Direct Connect 게이트웨이를 구성합니다. D. DX-A 연결에서 Direct Connect 게이트웨이로 전송 VIF를 생성합니다. 고가용성을 위해 DX-B 연결에서 동일한 Direct Connect 게이트웨이로 전송 VIF를 생성합니다. eu-west-1 및 us-east-1 전송 게이트웨이를 모두 이 Direct Connect 게이트웨이와 연결합니다. 교차 리전 라우팅을 지원하기 위해 전송 게이트웨이를 서로 피어링합니다. Answer: D Explanation: in this solution, two transit VIFs are created - one from the DX-A connection and one from the DX-B connection - into the same Direct Connect gateway for high availability. Both the eu -west-1 and us-east-1 transit gateways are then associated with this Direct Connect gateway. The transit gateways are then peered with each other to support cross-Region routing. This solution meets the requirements of the company by creating a highly available connection between the on-premises data center and the VPCs in both the eu-west-1 and us-east-1 regions, and by enabling direct traffic routing between VPCs in those regions.
QUESTION NO: 87 한 회사가 AWS에서 데이터 집약적인 애플리케이션을 실행하고 있습니다. 이 애플리케이션은 수백 개의 Amazon EC2 인스턴스로 구성된 클러스터에서 실행됩니다. 공유 파일 시스템은 200TB의 데이터를 저장하는 여러 EC2 인스턴스에서도 실행됩니다. 애플리케이션은 공유 파일 시스템의 데이터를 읽고 수정하며 보고서를 생성합니다. 작업은 한 달에 한 번 실행되며 공유 파일 시스템에서 파일의 하위 집합을 읽고 완료하는 데 약 72시간이 걸립니다. 컴퓨트 인스턴스는 Auto Scaling 그룹에서 확장되지만 공유 파일 시스템을 호스팅하는 인스턴스는 계속 실행됩니다. 컴퓨팅 및 스토리지 인스턴스는 모두 동일한 AWS 리전에 있습니다. 솔루션 설계자는 공유 파일 시스템 인스턴스를 교체하여 비용을 줄여야 합니다. 파일 시스템은 72시간 실행 기간 동안 필요한 데이터에 대한 고성능 액세스를 제공해야 합니다. 이러한 요구 사항을 충족하면서 가장 큰 전체 비용 절감 효과를 제공하는 솔루션은 무엇입니까? A. 기존 공유 파일 시스템의 데이터를 S3 Intelligent-Tiering 스토리지 클래스를 사용하는 Amazon S3 버킷으로 마이그레이션합니다. 매월 작업이 실행되기 전에 Lustre용 Amazon FSx를 사용하여 지연 로딩을 사용하여 Amazon S3의 데이터로 새 파일 시스템을 생성합니다. 작업 기간 동안 새 파일 시스템을 공유 스토리지로 사용합니다. 작업이 완료되면 파일 시스템을 삭제합니다. B. 기존 공유 파일 시스템에서 다중 연결이 활성화된 대용량 Amazon Elastic Block Store(Amazon EBS) 볼륨으로 데이터를 마이그레이션합니다. Auto Scaling 그룹 시작 템플릿의 사용자 데이터 스크립트를 사용하여 각 인스턴스에 EBS 볼륨을 연결합니다. 작업 기간 동안 EBS 볼륨을 공유 스토리지로 사용합니다. 작업이 완료되면 EBS 볼륨을 분리합니다. C. 기존 공유 파일 시스템의 데이터를 S3 Standard 스토리지 클래스를 사용하는 Amazon S3 버킷으로 마이그레이션합니다. 매월 작업이 실행되기 전에 Lustre용 Amazon FSx를 사용하여 일괄 로드를 사용하여 Amazon S3의 데이터로 새 파일 시스템을 생성합니다. 작업 기간 동안 새 파일 시스템을 공유 스토리지로 사용합니다. 작업이 완료되면 파일 시스템을 삭제합니다. D. 기존 공유 파일 시스템에서 Amazon S3 버킷으로 데이터를 마이그레이션합니다. 매월 작업이 실행되기 전에 AWS Storage Gateway를 사용하여 Amazon S3의 데이터로 파일 게이트웨이를 생성합니다. 파일 게이트웨이를 작업의 공유 스토리지로 사용합니다. 작업이 완료되면 파일 게이트웨이를 삭제합니다. Answer: A Explanation: https://aws.amazon.com/blogs/storage/new-enhancements-for-moving-data-betweenamazon-fsx-for-lustre-and-amazon-s3/
QUESTION NO: 88 한 회사가 인기 있는 온라인 게임의 속편을 만들고 있습니다. 전 세계의 많은 사용자가 출시 후 첫 주 이내에 게임을 플레이할 것입니다. 현재 이 게임은 단일 AWS 리전에 배포된 다음 구성 요소로 구성됩니다. * 게임 자산을 저장하는 Amazon S3 버킷 * 플레이어 점수를 저장하는 Amazon DynamoDB 테이블 솔루션 설계자는 대기 시간을 줄이고 안정성을 개선하고 구현하는 데 최소한의 노력이 필요한 다중 리전 솔루션을 설계해야 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까? A. Amazon CloudFront 배포를 생성하여 S3 버킷의 자산 제공 S3 교차 리전 복제 구성 새 리전에서 사용할 수 있는 새 DynamoDB 생성 새 테이블을 DynamoDB 전역 테이블의 복제 대상으로 사용합니다. B. Amazon CloudFront 배포를 생성하여 S3 버킷의 자산을 제공합니다. S3 동일 지역 복제를 구성합니다. 새 리전에서 사용할 수 있는 새 DynamoDB를 생성합니다. 변경 데이터 캡처(CDC)와 함께 AWS DMS(AWS Database Migration Service)를 사용하여 DynamoDB 테이블 간의 비동기식 복제 구성 C. 새 리전에 또 다른 S3 버킷을 생성하고 버킷 간에 S3 교차 리전 복제를 구성합니다. Amazon CloudFront 배포를 생성하고 각 리전의 S3 버킷에 액세스하는 두 개의 오리진으로 오리진 장애 조치를 구성합니다. Amazon DynamoDB Streams를 활성화하여 DynamoDB 글로벌 테이블을 구성하고 새 리전에 복제본 테이블을 추가합니다. D. 동일한 리전에 다른 S3 버킷을 생성하고 버킷 간에 S3 동일 리전 복제를 구성합니다. Amazon CloudFront 배포를 생성하고 S3 버킷에 액세스하는 두 개의 오리진으로 오리진 장애 조치를 구성합니다. 새 리전에서 새 DynamoDB 테이블을 생성합니다. DynamoDB 전역 테이블의 복제본 대상으로 새 테이블. Answer: C Explanation: https://aws.amazon.com/premiumsupport/knowledge-center/dynamodb-global-table-streamlambda/? nc1=h_ls
QUESTION NO: 89 회사는 Auto Scaling 그룹의 Amazon EC2 인스턴스를 사용하여 AWS 클라우드에서 비디오를 처리하고 있습니다. 비디오를 처리하는 데 30분이 걸립니다. 여러 EC2 인스턴스는 Amazon Simple Queue Service(Amazon SQS) 대기열의 비디오 수에 따라 확장 및 축소됩니다. 회사는 대상 배달 못한 편지 대기열과 maxReceiveCount를 1로 지정하는 리드라이브 정책으로 SQS 대기열을 구성했습니다. 회사는 SQS 대기열에 대한 가시성 제한 시간을 1시간으로 설정했습니다. 이 회사는 배달 못한 편지 대기열에 메시지가 있을 때 개발 팀에 알리도록 Amazon CloudWatch 경보를 설정했습니다. 개발팀은 메시지가 데드 레터 대기열에 있고 비디오가 제대로 처리되지 않았다는 알림을 하루 중 여러 번 받습니다. 조사 결과 애플리케이션 로그에서 오류가 발견되지 않았습니다. 회사는 이 문제를 어떻게 해결할 수 있습니까? A. EC2 인스턴스에 대한 종료 방지 기능을 켭니다. B. SOS 대기열의 가시성 제한 시간을 3시간으로 업데이트합니다. C. 처리 중에 인스턴스에 대한 축소 보호를 구성합니다. D. 리드라이브 정책을 업데이트하고 maxReceiveCount를 0으로 설정합니다. Answer: B Explanation: The best solution for this problem is to update the visibility timeout for the SQS queue to 3 hours. This is because when the visibility timeout is set to 1 hour, it means that if the EC2 instance doesn't process the message within an hour, it will be moved to the dead-letter queue. By increasing the visibility timeout to 3 hours, this should give the EC2 instance enough time to process the message before it gets moved to the dead-letter queue. Additionally, configuring scale-in protection for the EC2 instances during processing will help to ensure that the instances are not terminated while the messages are being processed.
QUESTION NO: 90 한 회사가 AWS 클라우드에서 여러 프로젝트를 개발하고 호스팅하고 있습니다. 프로젝트는 AWS Organizations의 동일한 조직에 있는 여러 AWS 계정에서 개발됩니다. 회사는 비용 또는 클라우드 인프라를 소유 프로젝트에 할당해야 합니다. 모든 AWS 계정을 담당하는 팀은 여러 Amazon EC2 인스턴스에 비용 할당에 사용되는 프로젝트 태그가 없음을 발견했습니다. 솔루션 설계자는 문제를 해결하고 향후 문제가 발생하지 않도록 하기 위해 어떤 조치를 취해야 합니까? (3개를 선택하세요.) A. 태그가 누락된 리소스를 찾기 위해 각 계정에서 AWS Config 규칙을 생성합니다. B. 프로젝트 태그가 누락된 경우 조직에서 ec2:Runlnstances에 대한 거부 작업으로 SCP를 생성합니다. C. 조직에서 Amazon Inspector를 사용하여 태그가 누락된 리소스를 찾습니다. D. 프로젝트 태그가 누락된 경우 ec2:RunInstances에 대한 거부 작업으로 각 계정에 IAM 정책을 생성합니다. E. 조직에서 프로젝트 태그가 누락된 EC2 인스턴스 목록을 수집할 AWS Config 집계자를 생성합니다. F. AWS Security Hub를 사용하여 프로젝트 태그가 누락된 EC2 인스턴스 목록을 집계합니다. Answer: A,B,E Explanation: https://docs.aws.amazon.com/config/latest/developerguide/config-rule-multi-accountdeployment.html https://docs.aws.amazon.com/config/latest/developerguide/aggregate-data.html https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_ex amples_tagging.html
QUESTION NO: 91 회사는 회사의 기본 온프레미스 애플리케이션이 실패하는 경우 AWS를 사용하여 비즈니스 연속성 솔루션을 생성하려고 합니다. 애플리케이션은 다른 애플리케이션도 실행하는 물리적 서버에서 실행됩니다. 회사에서 마이그레이션할 계획인 온프레미스 애플리케이션은 MySQL 데이터베이스를 데이터 저장소로 사용합니다. 회사의 모든 온프레미스 애플리케이션은 Amazon EC2와 호환되는 운영 체제를 사용합니다. 최소한의 운영 오버헤드로 회사의 목표를 달성할 수 있는 솔루션은 무엇입니까? A. MySQL 서버를 포함하여 소스 서버에 AWS Replication Agent를 설치합니다. 모든 서버에 대한 복제를 설정합니다. 일반 훈련을 위한 테스트 인스턴스를 시작합니다. 실패 이벤트의 경우 워크로드를 장애 조치하기 위해 테스트 인스턴스로 컷오버합니다. B. MySQL 서버를 포함하여 소스 서버에 AWS Replication Agent를 설치합니다. 대상 AWS 리전에서 AWS Elastic Disaster Recovery를 초기화합니다. 시작 설정을 정의합니다. 가장 최근 시점부터 장애 조치(failover) 및 폴백(fallback)을 자주 수행합니다. C. 데이터베이스를 호스팅할 AWS DMS(AWS Database Migration Service) 복제 서버 및 대상 Amazon Aurora MySQL DB 클러스터를 생성합니다. DMS 복제 작업을 생성하여 기존 데이터를 대상 DB 클러스터에 복사합니다. 데이터 동기화를 유지하기 위해 로컬 AWS Schema Conversion Tool(AWS SCT) 변경 데이터 캡처(CDC) 작업을 생성합니다. 호환되는 기본 AMI로 시작하여 나머지 소프트웨어를 EC2 인스턴스에 설치합니다. D. 온프레미스에 AWS Storage Gateway Volume Gateway를 배포합니다. 모든 온프레미스 서버에 볼륨을 탑재합니다. 애플리케이션과 MySQL 데이터베이스를 새 볼륨에 설치합니다. 정기적으로 스냅샷을 찍습니다. 호환되는 기본 AMI로 시작하여 EC2 인스턴스에 모든 소프트웨어를 설치합니다. EC2 인스턴스에서 볼륨 게이트웨이를 시작합니다. 최신 스냅샷에서 볼륨을 복원합니다. 실패 이벤트의 경우 EC2 인스턴스에 새 볼륨을 탑재합니다. Answer: B Explanation: https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html https://docs.aws.amazon.com/drs/latest/userguide/recovery-workflow-gs.html
QUESTION NO: 92 회사가 AWS 클라우드에서 애플리케이션을 실행하고 있습니다. 이 애플리케이션은 Amazon S3 버킷에 대량의 비정형 데이터를 수집하고 저장합니다. S3 버킷에는 수 테라바이트의 데이터가 포함되어 있으며 S3 Standard 스토리지 클래스를 사용합니다. 데이터의 크기는 매일 몇 기가바이트씩 증가합니다. 회사는 데이터를 조회하고 분석해야 합니다. ㅏ. 회사는 1년이 지난 데이터에 접근하지 않습니다. 그러나 회사는 규정 준수를 위해 모든 데이터를 무기한 보관해야 합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까? A. S3 Select를 사용하여 데이터를 쿼리합니다. S3 수명 주기 정책을 생성하여 1년 이상 된 데이터를 S3 Glacier Deep Archive로 전환합니다. B. Amazon Redshift Spectrum을 사용하여 데이터를 쿼리합니다. S3 수명 주기 정책을 생성하여 1년 이상 된 데이터를 S3 Glacier Deep Archive로 전환합니다. C. AWS Glue 데이터 카탈로그 및 Amazon Athena를 사용하여 데이터를 쿼리합니다. S3 수명 주기 정책을 생성하여 1년 이상 된 데이터를 S3 Glacier Deep Archive로 전환합니다. D. Amazon Redshift Spectrum을 사용하여 데이터를 쿼리합니다. S3 수명 주기 정책을 생성하여 1년 이상 된 데이터를 S3 Intelligent-Tiering으로 전환합니다. Answer: B Explanation: Generally, unstructured data should be converted structured data before querying them. AWS Glue can do that. https://docs.aws.amazon.com/glue/latest/dg/schemarelationalize.html https://docs.aws.amazon.com/athena/latest/ug/glue-athena.html
QUESTION NO: 93 회사는 온프레미스 데이터 센터와 AWS 클라우드의 서버를 포함하는 하이브리드 환경을 구축하고 있습니다. 이 회사는 3개의 VPC에 Amazon EC2 인스턴스를 배포했습니다. 각 VPC는 서로 다른 AWS 리전에 있습니다. 회사는 데이터 센터에 가장 가까운 리전에서 데이터 센터로의 AWS Direct Connect 연결을 설정했습니다. 회사는 3개의 VPC 모두에서 EC2 인스턴스에 액세스할 수 있는 온프레미스 데이터 센터의 서버가 필요합니다. 온프레미스 데이터 센터의 서버도 AWS 퍼블릭 서비스에 액세스할 수 있어야 합니다. 최소 비용으로 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개를 선택하세요.) A. 데이터 센터와 가장 가까운 지역에 Direct Connect 게이트웨이를 생성합니다. Direct Connect 연결을 Direct Connect 게이트웨이에 연결합니다. 사용 B. 다른 두 리전의 VPC를 연결하기 위한 Direct Connect 게이트웨이. C. 온프레미스 데이터 센터에서 다른 두 리전으로 추가 Direct Connect 연결을 설정합니다. D. 비공개 VIE를 생성합니다. 프라이빗 VIF를 통해 다른 두 리전의 VPC에 대한 AWS Site-to-Site VPN 연결을 설정합니다. E. 공개 VIF를 생성합니다. 퍼블릭 VIF를 통해 다른 두 리전의 VPC에 대한 AWS Site-to-Site VPN 연결을 설정합니다. F. VPC 피어링을 사용하여 여러 지역의 VPC 간에 연결을 설정합니다. 기존 Direct Connect 연결로 프라이빗 VIF를 생성하여 피어링된 VPC에 연결합니다. Answer: A,E Explanation: A Direct Connect gateway allows you to connect multiple VPCs across different Regions to a Direct Connect connection1. A public VIF allows you to access AWS public services such as EC21. A Site-to-Site VPN connection over the public VIF provides encryption and redundancy for the traffic between the on-premises data center and the VPCs2. This solution is cheaper than setting up additional Direct Connect connections or using a private VIF with VPC peering.
QUESTION NO: 94 회사는 개발자가 Amazon EC2만 사용하도록 제한하기 위해 AWS Organizations를 구현하는 과정에 있습니다. Amazon S3 및 Amazon DynamoDB. 개발자 계정은 전용 조직 단위(OU)에 상주합니다. 솔루션 설계자는 개발자 계정에 다음 SCP를 구현했습니다. 이 정책이 배포되면 개발자 계정의 IAM 사용자는 정책에 나열되지 않은 AWS 서비스를 계속 사용할 수 있습니다. 이 정책의 범위를 벗어나는 서비스를 사용하는 개발자의 능력을 제거하기 위해 솔루션 설계자는 무엇을 해야 합니까? A. 제한해야 하는 각 AWS 서비스에 대해 명시적인 거부 문을 만듭니다. B. 개발자 계정의 OU에서 전체 AWS 액세스 SCP를 제거합니다. C. 모든 서비스를 명시적으로 거부하도록 전체 AWS 액세스 SCP를 수정합니다. D. SCP 끝에 와일드카드를 사용하여 명시적인 거부 문을 추가합니다. Answer: B Explanation: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_inheritan ce_auth.html
QUESTION NO: 95 회사에서 각 사업부에 대한 내부 클라우드 청구 전략을 변경하려고 합니다. 현재 클라우드 거버넌스팀은 전체 클라우드 지출에 대한 보고서를 각 사업부장과 공유하고 있다. 회사는 각 사업부에 대해 별도의 AWS 계정을 관리하기 위해 AWS Organizations를 사용합니다. 조직의 기존 태깅 표준에는 애플리케이션, 환경 및 소유자가 포함됩니다. 클라우드 거버넌스 팀은 각 사업부가 클라우드 지출에 대한 월별 보고서를 받을 수 있도록 중앙 집중식 솔루션을 원합니다. 또한 솔루션은 설정된 임계값을 초과하는 모든 클라우드 지출에 대한 알림을 보내야 합니다. 이러한 요구 사항을 충족하는 가장 비용 효율적인 방법은 무엇입니까? A. 각 계정에서 AWS 예산을 구성하고 애플리케이션, 환경 및 소유자별로 그룹화된 예산 알림을 구성합니다. 각 알림에 대한 Amazon SNS 주제에 각 사업부를 추가합니다. 각 계정에서 비용 탐색기를 사용하여 각 비즈니스 단위에 대한 월별 보고서를 생성합니다. B. 조직의 마스터 계정에서 AWS 예산을 구성하고 애플리케이션, 환경 및 소유자별로 그룹화된 예산 알림을 구성합니다. 각 알림에 대한 Amazon SNS 주제에 각 사업부를 추가합니다. 조직의 마스터 계정에서 비용 탐색기를 사용하여 각 사업부에 대한 월별 보고서를 생성합니다. C. 각 계정에서 AWS 예산을 구성하고 애플리케이션, 환경 및 소유자별로 그룹화된 예산 알림을 구성합니다. 각 알림에 대한 Amazon SNS 주제에 각 사업부를 추가합니다. 각 계정에서 AWS Billing and Cost Management 대시보드를 사용하여 각 비즈니스 단위에 대한 월별 보고서를 생성합니다. D. 조직의 마스터 계정에서 AWS 비용 및 사용 보고서를 활성화하고 애플리케이션, 환경 및 소유자별로 그룹화된 보고서를 구성합니다. AWS 비용 및 사용 보고서를 처리하고, 예산 알림을 보내고, 각 사업부의 이메일 목록에 월별 보고서를 보내는 AWS Lambda 함수를 생성합니다. Answer: B Explanation: Configure AWS Budgets in the organization's master account and configure budget alerts that are grouped by application, environment, and owner. Add each business unit to an Amazon SNS topic for each alert. Use Cost Explorer in the organization's master account to create monthly reports for each business unit. https://aws.amazon.com/about-aws/whats-new/2019/07/introducing-aws-budgetsreports/#:~: text=AWS%20Budgets%20gives%20you%20the,below%20the%20threshold%20 you%20define
QUESTION NO: 96 회사는 직원이 VPN을 사용하여 연결하는 경우 집에서 원격으로 작업할 수 있도록 허용하는 새로운 정책을 도입했습니다. 회사는 여러 AWS 계정에서 VPC로 내부 애플리케이션을 호스팅하고 있습니다. 현재 애플리케이션은 다음을 통해 회사의 온프레미스 사무실 네트워크에서 액세스할 수 있습니다. AWS Site-to-Site VPN 연결 회사의 기본 AWS 계정에 있는 VPC에는 다른 AWS 계정에 있는 VPC와 피어링 연결이 설정되어 있습니다. 솔루션 설계자는 직원이 재택 근무 중에 사용할 수 있도록 확장 가능한 AWS 클라이언트 VPN 솔루션을 설계해야 합니다. 이러한 요구 사항을 충족하는 가장 비용 효율적인 솔루션은 무엇입니까? A. 각 AWS 계정에 클라이언트 VPN 엔드포인트 생성 내부 애플리케이션에 대한 액세스를 허용하는 필수 라우팅 구성 B. mam AWS 계정에 클라이언트 VPN 엔드포인트 생성 내부 애플리케이션에 대한 액세스를 허용하는 필수 라우팅 구성 C. 기본 AWS 계정에 클라이언트 VPN 엔드포인트 생성 각 AWS 계정에 연결된 전송 게이트웨이 프로비저닝 내부 애플리케이션에 대한 액세스를 허용하는 필수 라우팅 구성 D. mam AWS 계정에서 클라이언트 VPN 엔드포인트 생성 클라이언트 VPN 엔드포인트와 AWS Site-to-Site VPN 간의 연결 설정 Answer: C Explanation: https://docs.aws.amazon.com/vpn/latest/clientvpn-admin/scenario-peered.html
QUESTION NO: 97 회사는 전화를 받고 자동으로 모든 고객에게 문자 메시지를 통해 관리되는 양방향 양방향 경험 설문 조사를 보내는 고객 서비스 센터를 운영합니다. 고객 서비스 센터를 지원하는 애플리케이션은 회사가 온프레미스 데이터 센터에서 호스팅하는 시스템에서 실행됩니다. 회사에서 사용하는 하드웨어가 노후되어 시스템 가동 중지 시간이 발생하고 있습니다. 회사는 안정성을 향상시키기 위해 시스템을 AWS로 마이그레이션하려고 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Amazon Connect를 사용하여 기존 콜 센터 하드웨어를 교체하십시오. Amazon Pinpoint를 사용하여 고객에게 문자 메시지 설문 조사를 보냅니다. B. Amazon Connect를 사용하여 기존 콜 센터 하드웨어를 교체하십시오. Amazon Simple Notification Service(Amazon SNS)를 사용하여 고객에게 문자 메시지 설문 조사를 보냅니다. C. 콜 센터 소프트웨어를 Auto Scaling 그룹에 있는 Amazon EC2 인스턴스로 마이그레이션합니다. EC2 인스턴스를 사용하여 고객에게 문자 메시지 설문 조사를 보냅니다. D. Amazon Pinpoint를 사용하여 기존 콜 센터 하드웨어를 교체하고 고객에게 문자 메시지 설문 조사를 보냅니다. Answer: A Explanation: Amazon Connect is a cloud-based contact center service that allows you to set up a virtual call center for your business. It provides an easy-to-use interface for managing customer interactions through voice and chat. Amazon Connect integrates with other AWS services, such as Amazon S3 and Amazon Kinesis, to help you collect, store, and analyze customer data for insights into customer behavior and trends. On the other hand, Amazon Pinpoint is a marketing automation and analytics service that allows you to engage with your customers across different channels, such as email, SMS, push notifications, and voice. It helps you create personalized campaigns based on user behavior and enables you to track user engagement and retention. While both services allow you to communicate with your customers, they serve different purposes. Amazon Connect is focused on customer support and service, while Amazon Pinpoint is focused on marketing and engagement.
QUESTION NO: 98 회사는 AWS에서 이벤트 티켓팅 플랫폼을 실행 중이며 플랫폼의 비용 효율성을 최적화하려고 합니다. 이 플랫폼은 Amazon EC2와 함께 Amazon Elastic Kubernetes Service(Amazon EKS)에 배포되며 Amazon RDS for MySQL DB 인스턴스의 지원을 받습니다. 이 회사는 AWS Fargate를 사용하여 Amazon EKS에서 실행할 새로운 애플리케이션 기능을 개발하고 있습니다. 플랫폼은 드물게 최고 수요를 경험합니다. 수요 급증은 이벤트 날짜에 따라 다릅니다. 플랫폼에 가장 비용 효율적인 설정을 제공하는 솔루션은 무엇입니까? A. EKS 클러스터가 기준 로드에서 사용하는 EC2 인스턴스에 대한 표준 예약 인스턴스를 구입하십시오. 피크를 처리하기 위해 스팟 인스턴스로 클러스터를 확장합니다. 해당 연도의 예상 최대 로드를 충족하려면 데이터베이스에 대한 1년 전체 선결제 예약 인스턴스를 구입하십시오. B. EKS 클러스터의 예상되는 중간 부하에 대한 Compute Savings Plan을 구입하십시오. 피크 이벤트 날짜를 기준으로 온디맨드 용량 예약으로 클러스터를 확장합니다. 예측된 기본 로드를 충족하기 위해 데이터베이스에 대한 1년 선결제 없음 예약 인스턴스를 구매합니다. 사용량이 많을 때 일시적으로 데이터베이스 읽기 전용 복제본을 확장합니다. C. EKS 클러스터의 예상 기본 로드에 대한 EC2 Instance Savings Plans를 구매합니다. 피크를 처리하기 위해 스팟 인스턴스로 클러스터를 확장합니다. 예측된 기본 부하를 충족하기 위해 데이터베이스에 대한 1년 전체 선결제 예약 인스턴스를 구입합니다. 사용량이 많을 때 일시적으로 DB 인스턴스를 수동으로 확장합니다. D. EKS 클러스터의 예측된 기본 로드에 대한 Compute Savings Plan을 구매합니다. 피크를 처리하기 위해 스팟 인스턴스로 클러스터를 확장합니다. 예측된 기본 부하를 충족하기 위해 데이터베이스에 대한 1년 전체 선결제 예약 인스턴스를 구입합니다. 사용량이 많을 때 일시적으로 DB 인스턴스를 수동으로 확장합니다. Answer: B Explanation: They all mention using spot instances and EKS based on EC2. A spot instance is not appropriate for a production server and the company is developing new application designed for AWS Fargate, which means we must plan the future cost improvement including AWS Fargate. https://aws.amazon.com/savingsplans/compute-pricing/
QUESTION NO: 99 회사에서 다른 공급업체로부터 가전제품을 구입했습니다. 어플라이언스에는 모두 loT 센서가 있습니다. 센서는 정보를 JSON으로 구문 분석하는 레거시 응용 프로그램에 공급업체의 독점 형식으로 상태 정보를 보냅니다. 구문 분석은 간단하지만 각 공급업체마다 고유한 형식이 있습니다. 애플리케이션은 매일 한 번 모든 JSON 레코드를 구문 분석하고 분석을 위해 레코드를 관계형 데이터베이스에 저장합니다. 회사는 더 빠르게 제공하고 비용을 최적화할 수 있는 새로운 데이터 분석 솔루션을 설계해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. loT 센서를 AWS loT Core에 연결합니다. AWS Lambda 함수를 호출하여 정보를 구문 분석하고 .csv 파일을 Amazon S3에 저장하는 규칙을 설정합니다. AWS Glue를 사용하여 파일을 분류합니다. 분석을 위해 Amazon Athena 및 Amazon OuickSight를 사용합니다. B. 애플리케이션 서버를 AWS Fargate로 마이그레이션하면 loT 센서에서 정보를 수신하고 정보를 관계형 형식으로 구문 분석합니다. 분석을 위해 구문 분석된 정보를 Amazon Redshift에 저장합니다. C. SFTP 서버용 AWS 전송을 생성합니다. SFTP를 통해 정보를 .csv 파일로 서버에 전송하도록 loT 센서 코드를 업데이트합니다. AWS Glue를 사용하여 파일을 분류합니다. Amazon Athena를 사용하여 분석하십시오. D. AWS Snowball Edge를 사용하여 loT 센서에서 직접 데이터를 수집하여 로컬 분석을 수행합니다. 정기적으로 데이터를 Amazon Redshift로 수집하여 글로벌 분석을 수행합니다. Answer: A Explanation: Connect the IoT sensors to AWS IoT Core. Set a rule to invoke an AWS Lambda function to parse the information and save a .csv file to Amazon S3. Use AWS Glue to catalog the files. Use Amazon Athena and Amazon QuickSight for analysis. This solution meets the requirement of faster analysis and cost optimization by using AWS IoT Core to collect data from the IoT sensors in real-time and then using AWS Glue and Amazon Athena for efficient data analysis. This solution involves connecting the loT sensors to the AWS loT Core, setting a rule to invoke an AWS Lambda function to parse the information, and saving a .csv file to Amazon S3. AWS Glue can be used to catalog the files and Amazon Athena and Amazon QuickSight can be used for analysis. This solution will enable faster and more cost-effective data analysis. This solution is in line with the official Amazon Textbook and Resources for the AWS Certified Solutions Architect - Professional certification. In particular, the book states that: "AWS IoT Core can be used to ingest and process the data, AWS Lambda can be used to process and transform the data, and Amazon S3 can be used to store the data. AWS Glue can be used to catalog and access the data, Amazon Athena can be used to query the data, and Amazon QuickSight can be used to visualize the data." (Source: https://d1.awsstatic.com/training-and-certification/docs-sapro/ AWS_Certified_Solutions_Architect_Professional_Exam_Guide_EN_v1.5.pdf)
QUESTION NO: 100 한 회사가 최근 AWS에 애플리케이션을 배포했습니다. 애플리케이션은 Amazon DynamoDB를 사용합니다. 회사는 애플리케이션 부하를 측정하고 DynamoDB 테이블의 RCU 및 WCU를 예상 최대 부하와 일치하도록 구성했습니다. 최대 부하는 4시간 동안 일주일에 한 번 발생하며 평균 부하의 두 배입니다. 애플리케이션 로드는 나머지 주의 평균 로드에 가깝습니다. 액세스 패턴에는 테이블 읽기보다 테이블에 대한 쓰기가 더 많이 포함됩니다. 솔루션 설계자는 테이블 비용을 최소화하는 솔루션을 구현해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS Application Auto Scaling을 사용하여 피크 기간 동안 용량을 늘립니다. 예약된 RCU 및 WCU를 구매하여 평균 부하에 맞춥니다. B. 테이블에 대한 온디맨드 용량 모드를 구성합니다. C. 테이블 앞에서 DynamoDB Accelerator(DAX)를 구성합니다. 프로비저닝된 읽기 용량을 테이블의 새로운 최대 로드와 일치하도록 줄입니다. D. 테이블 앞에 DynamoDB Accelerator(DAX)를 구성합니다. 테이블에 대한 온디맨드 용량 모드를 구성합니다. Answer: D Explanation: This solution meets the requirements by using Application Auto Scaling to automatically increase capacity during the peak period, which will handle the double the average load. And by purchasing reserved RCUs and WCUs to match the average load, it will minimize the cost of the table for the rest of the week when the load is close to the average.
QUESTION NO: 101 회사는 전자 상거래 웹 사이트를 위한 DR(재해 복구) 솔루션을 구축해야 합니다. 웹 애플리케이션은 t3.Iarge Amazon EC2 인스턴스 플릿에서 호스팅되며 Amazon RDS for MySQL DB 인스턴스를 사용합니다. EC2 인스턴스는 여러 가용 영역에 걸쳐 확장되는 Auto Scaling 그룹에 있습니다. 재해 발생 시 웹 애플리케이션은 RPO가 30초이고 R TO가 10분인 보조 환경으로 장애 조치해야 합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까? A. 코드형 인프라(IaC)를 사용하여 DR 지역에서 새 인프라를 프로비저닝합니다. DB 인스턴스에 대한 리전 간 읽기 전용 복제본을 생성합니다. AWS Backup에서 백업 계획을 설정하여 EC2 인스턴스 및 DB 인스턴스에 대한 교차 리전 백업을 생성합니다. EC2 인스턴스와 DB 인스턴스를 30초마다 DR 지역에 백업하는 cron 표현식을 생성합니다. 최신 EC2 백업에서 EC2 인스턴스를 복구합니다. Amazon Route 53 지리적 위치 라우팅 정책을 사용하여 재해 발생 시 자동으로 DR 리전으로 장애 조치합니다. B. 코드형 인프라(laC)를 사용하여 DR 지역에서 새 인프라를 프로비저닝합니다. DB 인스턴스에 대한 리전 간 읽기 전용 복제본을 생성합니다. EC2 인스턴스를 DR 지역에 지속적으로 복제하도록 AWS Elastic Disaster Recovery를 설정합니다. DR 지역에서 최소 용량으로 EC2 인스턴스 실행 Amazon Route 53 장애 조치 라우팅 정책을 사용하여 재해 발생 시 자동으로 DR 지역으로 장애 조치합니다. Auto Scaling 그룹의 원하는 용량을 늘립니다. C. AWS Backup에서 백업 계획을 설정하여 EC2 인스턴스 및 DB 인스턴스에 대한 교차 리전 백업을 생성합니다. EC2 인스턴스와 DB 인스턴스를 30초마다 DR 지역에 백업하는 cron 표현식을 생성합니다. 코드형 인프라(IaC)를 사용하여 DR 리전에서 새 인프라를 프로비저닝합니다. 새 인스턴스에서 백업된 데이터를 수동으로 복원합니다. 재해 발생 시 Amazon Route 53 단순 라우팅 정책을 사용하여 DR 리전으로 자동 장애 조치합니다. D. 코드형 인프라(IaC)를 사용하여 DR 지역에서 새 인프라를 프로비저닝합니다. Amazon Aurora 글로벌 데이터베이스를 생성합니다. EC2 인스턴스를 DR 지역에 지속적으로 복제하도록 AWS Elastic Disaster Recovery를 설정합니다. DR 지역에서 전체 용량으로 EC2 인스턴스의 Auto Scaling 그룹을 실행합니다. Amazon Route 53 장애 조치 라우팅 정책을 사용하여 재해 발생 시 자동으로 DR 리전으로 장애 조치합니다. Answer: B Explanation: The company should use infrastructure as code (IaC) to provision the new infrastructure in the DR Region. The company should create a cross-Region read replica for the DB instance. The company should set up AWS Elastic Disaster Recovery to continuously replicate the EC2 instances to the DR Region. The company should run the EC2 instances at the minimum capacity in the DR Region. The company should use an Amazon Route 53 failover routing policy to automatically fail over to the DR Region in the event of a disaster. The company should increase the desired capacity of the Auto Scaling group. This solution will meet the requirements most cost-effectively because AWS Elastic Disaster Recovery (AWS DRS) is a service that minimizes downtime and data loss with fast, reliable recovery of onpremises and cloud-based applications using affordable storage, minimal compute, and point-in-time recovery. AWS DRS enables RPOs of seconds and RTOs of minutes1. AWS DRS continuously replicates data from the source servers to a staging area subnet in the DR Region, where it uses low-cost storage and minimal compute resources to maintain ongoing replication. In the event of a disaster, AWS DRS automatically converts the servers to boot and run natively on AWS and launches recovery instances on AWS within minutes2. By using AWS DRS, the company can save costs by removing idle recovery site resources and paying for the full disaster recovery site only when needed. By creating a cross-Region read replica for the DB instance, the company can have a standby copy of its primary database in a different AWS Region3. By using infrastructure as code (IaC), the company can provision the new infrastructure in the DR Region in an automated and consistent way4. By using an Amazon Route 53 failover routing policy, the company can route traffic to a resource that is healthy or to another resource when the first resource becomes unavailable. The other options are not correct because: Using AWS Backup to create cross-Region backups for the EC2 instances and the DB instance would not meet the RPO and RTO requirements. AWS Backup is a service that enables you to centralize and automate data protection across AWS services. You can use AWS Backup to back up your application data across AWS services in your account and across accounts. However, AWS Backup does not provide continuous replication or fast recovery; it creates backups at scheduled intervals and requires manual restoration. Creating backups every 30 seconds would also incur high costs and network bandwidth. Creating an Amazon API Gateway Data API service integration with Amazon Redshift would not help with disaster recovery. The Data API is a feature that enables you to query your Amazon Redshift cluster using HTTP requests, without needing a persistent connection or a SQL client. It is useful for building applications that interact with Amazon Redshift, but not for replicating or recovering data. Creating an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift cluster would not help with disaster recovery. AWS Data Exchange is a service that makes it easy for AWS customers to exchange data in the cloud. You can use AWS Data Exchange to subscribe to a diverse selection of third-party data products or offer your own data products to other AWS customers. A datashare is a feature that enables you to share live and secure access to your Amazon Redshift data across your accounts or with third parties without copying or moving the underlying dat a. It is useful for sharing query results and views with other users, but not for replicating or recovering data. Reference: https://aws.amazon.com/disaster-recovery/ https://docs.aws.amazon.com/drs/latest/userguide/what-is-drs.html https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/USER_ReadRepl.html#USER_ ReadRepl.XRgn https://aws.amazon.com/cloudformation/ https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/dns-failover.html https://aws.amazon.com/backup/ https://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html https://aws.amazon.com/data-exchange/ https://docs.aws.amazon.com/redshift/latest/dg/datashare-overview.html
QUESTION NO: 102 회사는 온프레미스 데이터 센터에서 2계층 웹 기반 애플리케이션을 실행하고 있습니다. 애플리케이션 계층은 상태 저장 애플리케이션을 실행하는 단일 서버로 구성됩니다. 애플리케이션은 별도의 서버에서 실행되는 PostgreSQL 데이터베이스에 연결됩니다. 애플리케이션의 사용자 기반이 크게 성장할 것으로 예상되므로 회사는 애플리케이션과 데이터베이스를 AWS로 마이그레이션하고 있습니다. 이 솔루션은 Amazon Aurora PostgreSQL, Amazon EC2 Auto Scaling 및 Elastic Load Balancing을 사용합니다. 애플리케이션 및 데이터베이스 계층을 확장할 수 있는 일관된 사용자 경험을 제공하는 솔루션은 무엇입니까? A. Aurora 복제본에 대해 Aurora Auto Scaling을 활성화합니다. 최소 미결 요청 라우팅 알고리즘 및 고정 세션이 활성화된 Network Load Balancer를 사용하십시오. B. Aurora 작성자에 대해 Aurora Auto Scaling을 활성화합니다. 라운드 로빈 라우팅 알고리즘 및 고정 세션이 활성화된 Application Load Balancer를 사용합니다. C. Aurora 복제본에 대해 Aurora Auto Scaling을 활성화합니다. 라운드 로빈 라우팅 및 고정 세션이 활성화된 Application Load Balancer를 사용합니다. D. Aurora 작성자에 대해 Aurora Scaling을 활성화합니다. 최소 미결 요청 라우팅 알고리즘 및 고정 세션이 활성화된 Network Load Balancer를 사용하십시오. Answer: C Explanation: Aurora Auto Scaling enables your Aurora DB cluster to handle sudden increases in connectivity or workload. When the connectivity or workload decreases, Aurora Auto Scaling removes unnecessary Aurora Replicas so that you don't pay for unused provisioned DB instances
QUESTION NO: 103 회사는 AWS 클라우드에서 웹 애플리케이션을 실행하고 있습니다. 애플리케이션은 Amazon EC2 인스턴스 세트에서 생성되는 동적 콘텐츠로 구성됩니다. EC2 인스턴스는 Application Load Balancer(ALB)의 대상 그룹으로 구성된 Auto Scaling 그룹에서 실행됩니다. 이 회사는 Amazon CloudFront 배포를 사용하여 애플리케이션을 전 세계에 배포하고 있습니다. CloudFront 배포는 ALB를 오리진으로 사용합니다. 이 회사는 DNS에 Amazon Route 53을 사용하고 CloudFront 배포를 위해 www.example.com의 A 레코드를 생성했습니다. 솔루션 설계자는 가용성이 높고 내결함성이 있도록 애플리케이션을 구성해야 합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까? A. 다른 AWS 리전에서 전체 보조 애플리케이션 배포를 프로비저닝합니다. 장애 조치 레코드가 되도록 Route 53 A 레코드를 업데이트합니다. 두 CloudFront 배포를 모두 값으로 추가합니다. Route 53 상태 확인을 생성합니다. B. 다른 AWS 리전에서 ALB, Auto Scaling 그룹 및 EC2 인스턴스를 프로비저닝합니다. CloudFront 배포를 업데이트하고 새 ALB에 대한 두 번째 오리진을 생성합니다. 두 오리진에 대한 오리진 그룹을 생성합니다. 하나의 오리진을 기본으로, 하나의 오리진을 보조로 구성하십시오. C. 다른 AWS 리전에서 Auto Scaling 그룹 및 EC2 인스턴스를 프로비저닝합니다. ALB에서 새 Auto Scaling 그룹에 대한 두 번째 대상을 생성합니다. ALB에서 장애 조치 라우팅 알고리즘을 설정합니다. D. 다른 AWS 리전에서 전체 보조 애플리케이션 배포를 프로비저닝합니다. 두 번째 CloudFront 배포를 생성하고 새 애플리케이션 설정을 오리진으로 추가합니다. AWS Global Accelerator 액셀러레이터를 생성합니다. 두 CloudFront 배포를 엔드포인트로 추가합니다. Answer: B Explanation: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/DownloadDistS3An dCustomOrigins.html https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_ori gin_failover.html You can set up CloudFront with origin failover for scenarios that require high availability. To get started, you create an origin group with two origins: a primary and a secondary. If the primary origin is unavailable, or returns specific HTTP response status codes that indicate a failure, CloudFront automatically switches to the secondary origin.
QUESTION NO: 104 소매 회사는 여러 AWS 리전에 걸쳐 AWS에서 전자 상거래 웹 사이트를 호스팅하고 있습니다. 회사는 온라인 구매를 위해 웹 사이트가 항상 작동하기를 원합니다. 웹 사이트는 MySQL DB 인스턴스용 Amazon RDS에 데이터를 저장합니다. 데이터베이스에 가장 높은 가용성을 제공하는 솔루션은 무엇입니까? A. Amazon RDS에서 자동 백업을 구성합니다. 중단 시 자동 백업을 독립 실행형 DB 인스턴스로 승격합니다. 데이터베이스 트래픽을 승격된 DB 인스턴스로 보냅니다. 승격된 DB 인스턴스를 원본으로 하는 대체 읽기 전용 복제본을 생성합니다. B. Amazon RDS에서 전역 테이블 및 읽기 전용 복제본을 구성합니다. 리전 간 범위를 활성화합니다. 중단된 경우 AWS Lambda를 사용하여 한 리전에서 다른 리전으로 읽기 전용 복제본을 복사하십시오. C. Amazon RDS에서 전역 테이블 및 자동 백업을 구성합니다. 중단된 경우 AWS Lambda를 사용하여 한 리전에서 다른 리전으로 읽기 전용 복제본을 복사하십시오. D. Amazon RDS에서 읽기 전용 복제본을 구성합니다. 중단의 경우 리전 간 및 읽기 전용 복제본을 독립 실행형 DB 인스턴스로 승격합니다. 데이터베이스 트래픽을 승격된 DB 인스턴스로 보냅니다. 승격된 DB 인스턴스를 원본으로 하는 대체 읽기 전용 복제본을 생성합니다. Answer: D Explanation: This solution will provide the highest availability for the database, as the read replicas will allow the database to be available in multiple Regions, thus reducing the chances of disruption. Additionally, the promotion of the cross-Region read replica to become a standalone DB instance will ensure that the database is still available even if one of the Regions experiences disruptions.
QUESTION NO: 105 소프트웨어 회사는 여러 AWS 계정 및 리전의 리소스를 사용하여 AWS에서 애플리케이션을 호스팅합니다. 애플리케이션은 IPv4 CIDR 블록이 10.10.0.0/16인 us-east-1 리전에 위치한 애플리케이션 VPC의 Amazon EC2 인스턴스 그룹에서 실행됩니다. 다른 AWS 계정에서 공유 서비스 VPC는 IPv4 CIDR 블록이 10.10.10.0/24인 us-east-2 리전에 있습니다. 클라우드 엔지니어가 AWS CloudFormation을 사용하여 애플리케이션 VPC를 공유 서비스 VPC와 피어링하려고 시도하면 피어링 실패를 나타내는 오류 메시지가 표시됩니다. 어떤 요인이 이 오류를 일으킬 수 있습니까? (2개 선택하세요.) A. 두 VPC의 IPv4 CIDR 범위가 겹칩니다. B. VPC가 동일한 지역에 있지 않습니다. C. 하나 또는 두 계정 모두 인터넷 게이트웨이에 액세스할 수 없습니다. D. VPC 중 하나가 AWS Resource Access Manager를 통해 공유되지 않았습니다. E. 피어 수락자 계정의 IAM 역할에 올바른 권한이 없습니다. Answer: A,E Explanation: https://aws.amazon.com/about-aws/whats-new/2017/11/announcing-support-for-inter-regionvpc-peering/
QUESTION NO: 106 국제 배송 회사는 AWS에서 배송 관리 시스템을 호스팅합니다. 드라이버는 시스템을 사용하여 배송 확인을 업로드합니다. 확인에는 수령인의 서명 또는 수령인과 함께 패키지 사진이 포함됩니다. 운전자의 핸드헬드 장치는 FTP를 통해 서명과 사진을 단일 Amazon EC2 인스턴스에 업로드합니다. 각 핸드헬드 장치는 로그인한 사용자를 기반으로 디렉토리에 파일을 저장하며 파일 이름은 배송 번호와 일치합니다. 그런 다음 EC2 인스턴스는 배달 정보를 가져오기 위해 중앙 데이터베이스를 쿼리한 후 파일에 메타데이터를 추가합니다. 그런 다음 파일은 보관을 위해 Amazon S3에 배치됩니다. 회사가 확장됨에 따라 운전자는 시스템이 연결을 거부하고 있다고 보고합니다. 연결 끊김 및 메모리 문제로 인해 FTP 서버에 문제가 있습니다. 이러한 문제에 대응하여 시스템 엔지니어는 30분마다 EC2 인스턴스를 재부팅하도록 cron 작업을 예약합니다. 청구 팀은 파일이 아카이브에 항상 있는 것은 아니며 중앙 시스템이 항상 업데이트되는 것은 아니라고 보고합니다. 솔루션 설계자는 아카이브가 항상 파일을 수신하고 시스템이 항상 업데이트되도록 확장성을 최대화하는 솔루션을 설계해야 합니다. 핸드헬드 장치는 수정할 수 없으므로 회사에서 새 애플리케이션을 배포할 수 없습니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 기존 EC2 인스턴스의 AMI를 생성합니다. Application Load Balancer 뒤에 EC2 인스턴스의 Auto Scaling 그룹을 생성합니다. 최소 3개의 인스턴스를 포함하도록 Auto Scaling 그룹을 구성합니다. B. AWS Transfer Family를 사용하여 파일을 Amazon Elastic File System(Amazon EFS)에 배치하는 FTP 서버를 생성합니다. EFS 볼륨을 기존 EC2 인스턴스에 마운트합니다. EC2 인스턴스가 파일 처리를 위한 새 경로를 가리킵니다. C. AWS Transfer Family를 사용하여 파일을 Amazon S3에 배치하는 FTP 서버를 생성합니다. Amazon Simple Notification Service(Amazon SNS)를 통해 S3 이벤트 알림을 사용하여 AWS Lambda 함수를 호출합니다. 메타데이터를 추가하고 전달 시스템을 업데이트하도록 Lambda 함수를 구성합니다. D. 파일을 Amazon S3에 직접 배치하도록 핸드헬드 장치를 업데이트합니다. Amazon Simple Queue Service(Amazon SQS)를 통해 S3 이벤트 알림을 사용하여 AWS Lambda 함수를 호출합니다. 메타데이터를 추가하고 전달 시스템을 업데이트하도록 Lambda 함수를 구성합니다. Answer: C Explanation: Using AWS Transfer Family to create an FTP server that places the files in Amazon S3 and using S3 event notifications through Amazon Simple Notification Service (Amazon SNS) to invoke an AWS Lambda function will ensure that the archive always receives the files and that the central system is always updated. This solution maximizes scalability and eliminates the need for manual intervention, such as rebooting the EC2 instance.
QUESTION NO: 107 회사는 AWS Organizations에서 조직의 일부인 10개의 계정을 가지고 있습니다. AWS Config는 각 계정에 구성되어 있습니다. 모든 계정은 Prod OU 또는 NonProd OU에 속합니다. 회사는 각 AWS 계정에 Amazon EventBridge 규칙을 설정하여 Amazon에 알립니다. 소스가 0.0.0.0/0인 Amazon EC2 보안 그룹 인바운드 규칙이 생성될 때 Simple Notification Service(Amazon SNS) 주제 회사의 보안 팀이 SNS 주제를 구독함 NonProd OU의 모든 계정에 대해 보안 팀이 제거해야 함 0.0.0.0/0을 소스로 포함하는 보안 그룹 인바운드 규칙을 생성하는 기능 어떤 솔루션이 최소한의 운영 오버헤드로 이 요구 사항을 충족합니까? A. AWS Lambda 함수를 호출하여 보안 그룹 인바운드 규칙을 제거하고 SNS 주제에 게시하도록 EventBridge 규칙을 수정합니다. 업데이트된 규칙을 NonProd OU에 배포합니다. B. NonProd OU에 vpc-sg-open-only-to-authorized-ports AWS Config 관리 규칙 추가 C. aws Sourcelp 조건 키의 값이 0.0.0.0/0이 아닌 경우 ec2 AulhonzeSecurityGrouplngress 작업을 허용하도록 SCP를 구성합니다 . NonProd OU에 S CP를 적용합니다. D. aws Sourcelp 조건 키의 값이 0.0.0.0/0일 때 ec2 AuthorizeSecurityGrouplngress 작업을 거부하도록 SCP를 구성합니다. SCP를 NonProd OU에 적용합니다. Answer: D Explanation: This solution will meet the requirement with the least operational overhead because it directly denies the creation of the security group inbound rule with 0.0.0.0/0 as the source, which is the exact requirement. Additionally, it does not require any additional steps or resources such as invoking a Lambda function or adding a Config rule. An SCP (Service Control Policy) is a policy that you can use to set fine-grained permissions for your AWS accounts within your organization. You can use SCPs to set permissions for the root user of an account and to delegate permissions to IAM users and roles in the accounts. You can use SCPs to set permissions that allow or deny access to specific services, actions, and resources. To implement this solution, you would need to create an SCP that denies the ec2:AuthorizeSecurityGroupIngress action when the value of the aws:SourceIp condition key is 0.0.0.0/0. This SCP would then be applied to the NonProd OU. This would ensure that any security group inbound rule that includes 0.0.0.0/0 as the source will be denied, thus meeting the requirement. Reference: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scp.html https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_condition-keys.html
QUESTION NO: 108 한 회사가 다중 계정 환경에서 AWS에서 애플리케이션을 실행하고 있습니다. 회사의 영업팀과 마케팅팀은 AWS Organizations에서 별도의 AWS 계정을 사용합니다. 영업팀은 Amazon S3 버킷에 페타바이트 규모의 데이터를 저장합니다. 마케팅 팀은 데이터 시각화를 위해 Amazon QuickSight를 사용합니다. 마케팅 팀은 상태 팀이 S3 버킷에 저장하는 데이터에 액세스해야 합니다. 회사는 AWS Key Management Service(AWS KMS) 키로 S3 버킷을 암호화했습니다. 마케팅 팀은 마케팅 AWS 계정에서 QuickSight 액세스를 제공하기 위해 QuickSight용 IAM 서비스 역할을 이미 생성했습니다. 회사는 AWS 계정 전체에서 S3 버킷의 데이터에 대한 보안 액세스를 제공할 솔루션이 필요합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 마케팅 계정에 새 S3 버킷을 생성합니다. 판매 계정에서 S3 복제 규칙을 생성하여 객체를 마케팅 계정의 새 S3 버킷에 복사합니다. 새 S3 버킷에 대한 액세스 권한을 부여하려면 마케팅 계정에서 QuickSight 권한을 업데이트하십시오. B. S3 버킷에 대한 액세스 권한을 마케팅 계정에 부여하는 SCP를 생성합니다. AWS Resource Access Manager(AWS RAM)를 사용하여 sates 계정의 KMS 키를 마케팅 계정과 공유합니다. 마케팅 계정에서 QuickSight 권한을 업데이트하여 S3 버킷에 대한 액세스 권한을 부여합니다. C. 마케팅 계정에서 S3 버킷 정책을 업데이트하여 QuickSight 역할에 대한 액세스 권한을 부여합니다. S3 버킷에서 사용되는 암호화 키에 대한 KMS 권한 부여를 생성합니다. QuickSight 역할에 암호 해독 액세스 권한을 부여합니다. 마케팅 계정에서 QuickSight 권한을 업데이트하여 S3 버킷에 대한 액세스 권한을 부여합니다. D. 판매 계정에 IAM 역할을 생성하고 S3 버킷에 대한 액세스 권한을 부여합니다. 마케팅 계정에서 판매 계정의 IAM 역할을 맡아 S3 버킷에 액세스합니다. QuickSight 로트를 업데이트하여 영업 계정의 새 IAM 역할과 신뢰 관계를 만듭니다. Answer: D Explanation: Create an IAM role in the sales account and grant access to the S3 bucket. From the marketing account, assume the IAM role in the sales account to access the S3 bucket. Update the QuickSight role, to create a trust relationship with the new IAM role in the sales account. This approach is the most secure way to grant cross-account access to the data in the S3 bucket while minimizing operational overhead. By creating an IAM role in the sales account, the marketing team can assume the role in their own account, and have access to the S3 bucket. And updating the QuickSight role, to create a trust relationship with the new IAM role in the sales account will grant the marketing team to access the data in the S3 bucket and use it for data visualization using QuickSight. AWS Resource Access Manager (AWS RAM) also allows sharing of resources between accounts, but it would require additional management and configuration to set up the sharing, which would increase operational overhead. Using S3 replication would also replicate the data to the marketing account, but it would not provide the marketing team access to the original data, and also it would increase operational overhead with managing the replication process. IAM roles and policies, KMS grants and trust relationships are a powerful combination for managing cross-account access in a secure and efficient manner. Reference: AWS IAM Roles AWS KMS - Key Grants AWS RAM
QUESTION NO: 109 솔루션 설계자는 온프레미스 데이터 처리 애플리케이션을 AWS 클라우드로 마이그레이션하는 방법에 대해 회사에 조언해야 합니다. 현재 사용자는 웹 포털을 통해 입력 파일을 업로드합니다. 그런 다음 웹 서버는 업로드된 파일을 NAS에 저장하고 메시지 대기열을 통해 처리 서버에 메시지를 보냅니다. 각 미디어 파일을 처리하는 데 최대 1시간이 걸릴 수 있습니다. 회사는 처리 대기 중인 미디어 파일의 수가 업무 시간 동안 훨씬 더 많고 업무 시간 이후에는 파일 수가 급격히 감소한다는 사실을 확인했습니다. 가장 비용 효율적인 마이그레이션 권장 사항은 무엇입니까? A. Amazon SQS를 사용하여 대기열을 생성합니다. 새 대기열에 게시하도록 기존 웹 서버를 구성합니다. 대기열에 메시지가 있으면 AWS Lambda 함수를 호출하여 대기열에서 요청을 가져와 파일을 처리합니다. 처리된 파일을 Amazon S3 버킷에 저장합니다. B. Amazon M을 사용하여 대기열을 생성합니다. 새 대기열에 게시하도록 기존 웹 서버를 구성합니다. 대기열에 메시지가 있는 경우 새 Amazon EC2 인스턴스를 생성하여 대기열에서 요청을 끌어오고 파일을 처리합니다. 처리된 파일을 Amazon EFS에 저장합니다. 작업이 완료된 후 EC2 인스턴스를 종료합니다. C. Amazon MO를 사용하여 대기열을 생성합니다. 새 대기열에 게시하도록 기존 웹 서버를 구성합니다. 대기열에 메시지가 있으면 AWS Lambda 함수를 호출하여 대기열에서 요청을 가져와 파일을 처리합니다. 처리된 파일을 Amazon EFS에 저장합니다. D. Amazon SOS를 사용하여 대기열을 생성합니다. 새 대기열에 게시하도록 기존 웹 서버를 구성합니다. EC2 Auto Scaling 그룹의 Amazon EC2 인스턴스를 사용하여 대기열에서 요청을 가져오고 파일을 처리합니다. SOS 대기열 길이에 따라 EC2 인스턴스를 확장합니다. 처리된 파일을 Amazon S3 버킷에 저장합니다. Answer: D Explanation: https://aws.amazon.com/blogs/compute/operating-lambda-performance-optimization-part-1/
QUESTION NO: 110 회사는 모놀리식 애플리케이션을 배포된 최신 애플리케이션 또는 AWS로 리팩터링할 계획입니다. CLCD 파이프라인은 다음 요구 사항이 있는 응용 프로그램의 모뎀 설계를 지원하도록 업그레이드해야 합니다. * 매시간 여러 번 변경 사항을 릴리스할 수 있어야 합니다. * 가능한 한 빨리 변경 사항을 롤백할 수 있어야 합니다. 이러한 요구 사항을 충족하는 디자인은 무엇입니까? A. AMI를 통합하여 애플리케이션 및 해당 구성을 포함하는 Cl-CD 파이프라인을 배포합니다. Amazon EC2 인스턴스를 교체하여 애플리케이션을 배포합니다. B. 보조 환경에서 애플리케이션의 CI/CD 파이프라인에 대한 배포 대상으로 사용할 AWS Elastic Beanstak을 지정합니다. 스왑을 배포하려면 스테이징 및 프로덕션 환경 URL을 사용하십시오. C. AWS Systems Manager를 사용하여 각 배포에 대한 인프라를 다시 프로비저닝합니다. Amazon EC2 사용자 데이터를 업데이트하여 Amazon S3에서 최신 코드 아티팩트를 가져오고 Amazon Route 53 가중 라우팅을 사용하여 새 환경을 가리킵니다. D. 사전 구축된 AMI를 사용하여 Auto Scaling 이벤트의 일부로 애플리케이션 업데이트 시 롤아웃합니다. 새 버전의 AMI를 사용하여 인스턴스를 추가하고 배포 이벤트 중에 구성된 종료 정책과 함께 이전 AMI 버전을 사용하는 모든 인스턴스를 단계적으로 제거합니다. Answer: B Explanation: It is the fastest when it comes to rollback and deploying changes every hour
QUESTION NO: 111 회사는 온프레미스 환경에서 3계층 웹 애플리케이션을 호스팅하고 있습니다. 최근 트래픽 급증으로 인해 가동 중지 시간이 발생하고 재정적으로 상당한 영향을 받았기 때문에 회사 경영진은 애플리케이션을 AWS로 이전하도록 명령했습니다. 응용 프로그램은 .NET으로 작성되었으며 MySQL 데이터베이스에 종속되어 있습니다. 솔루션 설계자는 확장 가능하고 가용성이 높은 솔루션을 설계하여 일일 사용자 200,000명. 솔루션 설계자는 적절한 솔루션을 설계하기 위해 어떤 단계를 수행해야 합니까? A. AWS Elastic Beanstalk를 사용하여 웹 서버 환경 및 Amazon RDS MySQL 다중 AZ DB 인스턴스가 있는 새 애플리케이션 생성 환경은 Amazon EC2 Auto Scaling 그룹 앞에서 네트워크 로드 밸런서(NLB)를 시작해야 합니다. 가용 영역 Amazon Route 53 별칭 레코드를 사용하여 회사 도메인에서 NLB로 트래픽을 라우팅합니다. B. AWS CloudFormation을 사용하여 3개의 가용 영역에 걸쳐 있는 Amazon EC2 Auto Scaling 그룹 앞에 Application Load Balancer(ALB)가 포함된 스택을 시작합니다. 스택은 삭제 유지 정책을 사용하여 Amazon Aurora MySQL DB 클러스터의 다중 AZ 배포를 시작해야 합니다. Amazon Route 53 별칭 레코드를 사용하여 회사 도메인에서 ALB로 트래픽 라우팅 C. AWS Elastic Beanstalk를 사용하여 각 지역에 Application Load Balancer(ALB)가 있는 별도의 두 지역에 걸쳐 있는 자동 확장 웹 서버 환경을 만듭니다. 리전 간 읽기 전용 복제본이 있는 Amazon Aurora MySQL DB 클러스터의 다중 AZ 배포 생성 지리 근접 라우팅 정책과 함께 Amazon Route 53을 사용하여 두 리전 간에 트래픽을 라우팅합니다. D. AWS CloudFormation을 사용하여 3개의 가용 영역에 걸쳐 있는 스팟 인스턴스의 Amazon ECS 클러스터 앞에서 Application Load Balancer(ALB)가 포함된 스택을 시작합니다. 스택은 스냅샷 삭제 정책으로 Amazon RDS MySQL DB 인스턴스를 시작해야 합니다. 회사 도메인에서 ALB로 트래픽을 라우팅하는 Amazon Route 53 별칭 레코드 Answer: C Explanation: Using AWS CloudFormation to launch a stack with an Application Load Balancer (ALB) in front of an Amazon EC2 Auto Scaling group spanning three Availability Zones, a Multi-AZ deployment of an Amazon Aurora MySQL DB cluster with a Retain deletion policy, and an Amazon Route 53 alias record to route traffic from the company's domain to the ALB will ensure that
QUESTION NO: 112 회사의 공개 API는 Amazon Elastic Container Service(Amazon ECS)에서 작업으로 실행됩니다. 작업은 Application Load Balancer(ALB) 뒤에 있는 AWS Fargate에서 실행되며 CPU 사용률을 기반으로 작업에 대해 Service Auto Scaling으로 구성됩니다. 이 서비스는 몇 달 동안 잘 운영되고 있습니다. 최근에 API 성능이 느려지고 애플리케이션을 사용할 수 없게 되었습니다. 이 회사는 API에 대해 상당한 수의 SQL 주입 공격이 발생했으며 API 서비스가 최대 규모로 확장되었음을 발견했습니다. 솔루션 설계자는 SQL 인젝션 공격이 ECS API 서비스에 도달하지 못하도록 방지하는 솔루션을 구현해야 합니다. 솔루션은 합법적인 트래픽을 허용해야 하며 운영 효율성을 극대화해야 합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까? A. 새 AWS WAF 웹 ACL을 생성하여 ECS 작업 앞에서 ALB로 전달되는 HTTP 요청 및 HTTPS 요청을 모니터링합니다. B. 새 AWS WAF Bot Control 구현을 생성합니다. AWS WAF Bot Control 관리형 규칙 그룹에 규칙을 추가하여 트래픽을 모니터링하고 ECS 작업 앞의 ALB에 대한 적법한 트래픽만 허용합니다. C. 새 AWS WAF 웹 ACL을 생성합니다. SQL 데이터베이스 규칙 그룹과 일치하는 요청을 차단하는 새 규칙을 추가합니다. 해당 규칙과 일치하지 않는 다른 모든 트래픽을 허용하도록 웹 ACL을 설정합니다. ECS 작업 앞의 ALB에 웹 ACL을 연결합니다. D. 새 AWS WAF 웹 ACL을 생성합니다. AWS WAF에서 새로운 빈 IP 세트를 생성합니다. 새 IP 세트의 IP 주소에서 발생하는 요청을 차단하려면 웹 ACL에 새 규칙을 추가하십시오. SQL 주입 공격을 보내는 IP 주소에 대한 API 로그를 스크랩하는 AWS Lambda 함수를 생성하고 해당 IP 주소를 IP 세트에 추가합니다. ECS 작업 앞의 ALB에 웹 ACL을 연결합니다. Answer: C Explanation: The company should create a new AWS WAF web ACL. The company should add a new rule that blocks requests that match the SQL database rule group. The company should set the web ACL to allow all other traffic that does not match those rules. The company should attach the web ACL to the ALB in front of the ECS tasks. This solution will meet the requirements because AWS WAF is a web application firewall that lets you monitor and control web requests that are forwarded to your web applications. You can use AWS WAF to define customizable web security rules that control which traffic can access your web applications and which traffic should be blocked1. By creating a new AWS WAF web ACL, the company can create a collection of rules that define the conditions for allowing or blocking web requests. By adding a new rule that blocks requests that match the SQL database rule group, the company can prevent SQL injection attacks from reaching the ECS API service. The SQL database rule group is a managed rule group provided by AWS that contains rules to protect against common SQL injection attack patterns2. By setting the web ACL to allow all other traffic that does not match those rules, the company can ensure that legitimate traffic can access the API service. By attaching the web ACL to the ALB in front of the ECS tasks, the company can apply the web security rules to all requests that are forwarded by the load balancer. The other options are not correct because: Creating a new AWS WAF Bot Control implementation would not prevent SQL injection attacks from reaching the ECS API service. AWS WAF Bot Control is a feature that gives you visibility and control over common and pervasive bot traffic that can consume excess resources, skew metrics, cause downtime, or perform other undesired activities. However, it does not protect against SQL injection attacks, which are malicious attempts to execute unauthorized SQL statements against your database3. Creating a new AWS WAF web ACL to monitor the HTTP requests and HTTPS requests that are forwarded to the ALB in front of the ECS tasks would not prevent SQL injection attacks from reaching the ECS API service. Monitoring mode is a feature that enables you to evaluate how your rules would perform without actually blocking any requests. However, this mode does not provide any protection against attacks, as it only logs and counts requests that match your rules4. Creating a new AWS WAF web ACL and creating a new empty IP set in AWS WAF would not prevent SQL injection attacks from reaching the ECS API service. An IP set is a feature that enables you to specify a list of IP addresses or CIDR blocks that you want to allow or block based on their source IP address. However, this approach would not be effective or efficient against SQL injection attacks, as it would require constantly updating the IP set with new IP addresses of attackers, and it would not block attackers who use proxies or VPNs. Reference: https://aws.amazon.com/waf/ https://docs.aws.amazon.com/waf/latest/developerguide/aws-managed-rule-groupslist.html#sql-injection-rule-group https://docs.aws.amazon.com/waf/latest/developerguide/waf-bot-control.html https://docs.aws.amazon.com/waf/latest/developerguide/web-acl-monitoring-mode.html https://docs.aws.amazon.com/waf/latest/developerguide/waf-ip-sets.html
QUESTION NO: 113 회사는 AWS Cloud에서 처리 엔진을 실행합니다. 엔진은 지속 가능성 지수를 계산하기 위해 물류 센터의 환경 데이터를 처리합니다. 회사는 유럽 전역에 분산된 물류 센터에 수백만 대의 장치를 보유하고 있습니다. 장치는 RESTful API를 통해 처리 엔진에 정보를 전송합니다. API에서 예측할 수 없는 트래픽 폭증 발생 회사는 장치가 처리 엔진으로 보내는 모든 데이터를 처리하는 솔루션을 구현해야 합니다. 데이터 손실이 허용되지 않습니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. RESTful API용 ALB(Application Load Balancer) 생성 Amazon Simple Queue Service(Amazon SQS) 대기열 생성 ALB용 리스너 및 대상 그룹 생성 SQS 대기열을 대상으로 추가 다음에서 실행되는 컨테이너 사용 대기열의 메시지를 처리하기 위한 Fargate 시작 유형이 있는 Amazon Elastic Container Service(Amazon ECS) B. RESTful API를 구현하는 Amazon API Gateway HTTP API 생성 Amazon Simple Queue Service(Amazon SQS) 대기열 생성 SQS 대기열과 API Gateway 서비스 통합 생성 SQS 대기열의 메시지를 처리하는 AWS Lambda 함수 생성 C. RESTful API를 구현하는 Amazon API Gateway REST API 생성 Auto Scaling 그룹에 Amazon EC2 인스턴스 플릿 생성 API Gateway Auto Scaling 그룹 프록시 통합 생성 EC2 인스턴스를 사용하여 수신 데이터 처리 D. RESTful API용 Amazon CloudFront 배포 생성 Amazon Kinesis Data Streams에서 데이터 스트림 생성 데이터 스트림을 배포의 오리진으로 설정 데이터 스트림에서 데이터를 소비하고 처리하는 AWS Lambda 함수 생성 Answer: A Explanation: it will use the ALB to handle the unpredictable bursts of traffic and route it to the SQS queue. The SQS queue will act as a buffer to store incoming data temporarily and the container running in Amazon ECS with the Fargate launch type will process messages in the queue. This approach will ensure that all data is processed and prevent data loss.
QUESTION NO: 114 회사는 AWS 클라우드에서 다중 계정 설정을 위해 AWS Organizations를 사용합니다. 회사의 재무 팀에는 AWS Lambda 및 Amazon DynamoDB를 사용하는 데이터 처리 애플리케이션이 있습니다. 회사의 마케팅 팀은 DynamoDB 테이블에 저장된 데이터에 액세스하려고 합니다. DynamoDB 테이블에는 기밀 데이터가 포함되어 있습니다. 마케팅 팀은 DynamoDB 테이블에 있는 데이터의 특정 속성에만 액세스할 수 있습니다. 재무 팀과 마케팅 팀은 별도의 AWS 계정을 가지고 있습니다. DynamoDB 테이블에 대한 적절한 액세스 권한을 마케팅 팀에 제공하려면 솔루션 설계자가 무엇을 해야 합니까? A. 마케팅 팀의 AWS 계정에 DynamoDB 테이블의 특정 속성에 대한 액세스 권한을 부여하는 SCP를 생성합니다. 재무 팀의 OU에 SCP를 연결합니다. B. 특정 DynamoDB 속성(세밀한 액세스 제어)에 대한 IAM 정책 조건을 사용하여 재무 팀 계정에서 IAM 역할을 생성합니다. 마케팅 팀의 계정과 신뢰를 구축합니다. 마케팅 팀 계정에서 재무 팀 계정의 IAM 역할을 맡을 수 있는 권한이 있는 IAM 역할을 생성합니다. C. 특정 DynamoDB 속성(세분화된 액세스 제어)에 대한 조건을 포함하는 리소스 기반 IAM 정책을 생성합니다. 정책을 DynamoDB 테이블에 연결합니다. 마케팅 팀 계정에서 재무 팀 계정의 DynamoDB 테이블에 액세스할 수 있는 권한이 있는 IAM 역할을 생성합니다. D. Dyna-moDB 테이블에 액세스하기 위해 재무 팀의 계정에 IAM 역할을 생성합니다. IAM 권한 경계를 사용하여 특정 속성에 대한 액세스를 제한합니다. 마케팅 팀 계정에서 재무 팀 계정의 IAM 역할을 맡을 수 있는 권한이 있는 IAM 역할을 생성합니다. Answer: C Explanation: The company should create a resource-based IAM policy that includes conditions for specific DynamoDB attributes (fine-grained access control). The company should attach the policy to the DynamoDB table. In the marketing team's account, the company should create an IAM role that has permissions to access the DynamoDB table in the finance team's account. This solution will meet the requirements because a resource-based IAM policy is a policy that you attach to an AWS resource (such as a DynamoDB table) to control who can access that resource and what actions they can perform on it. You can use IAM policy conditions to specify fine-grained access control for DynamoDB items and attributes. For example, you can allow or deny access to specific attributes of all items in a table by matching on attribute names1. By creating a resource-based policy that allows access to only specific attributes of the DynamoDB table and attaching it to the table, the company can restrict access to confidential data. By creating an IAM role in the marketing team's account that has permissions to access the DynamoDB table in the finance team's account, the company can enable cross-account access. The other options are not correct because: Creating an SCP to grant the marketing team's AWS account access to the specific attributes of the DynamoDB table would not work because SCPs are policies that you can use with AWS Organizations to manage permissions in your organization's accounts. SCPs do not grant permissions; instead, they specify the maximum permissions that identities in an account can have2. SCPs cannot be used to specify fine-grained access control for DynamoDB items and attributes. Creating an IAM role in the finance team's account by using IAM policy conditions for specific DynamoDB attributes and establishing trust with the marketing team's account would not work because IAM roles are identities that you can create in your account that have specific permissions. You can use an IAM role to delegate access to users, applications, or services that don't normally have access to your AWS resources3. However, creating an IAM role in the finance team's account would not restrict access to specific attributes of the DynamoDB table; it would only allow cross-account access. The company would still need a resourcebased policy attached to the table to enforce fine-grained access control. Creating an IAM role in the finance team's account to access the DynamoDB table and using an IAM permissions boundary to limit the access to the specific attributes would not work because IAM permissions boundaries are policies that you use to delegate permissions management to other users. You can use permissions boundaries to limit the maximum permissions that an identity-based policy can grant to an IAM entity (user or role)4. Permissions boundaries cannot be used to specify fine-grained access control for DynamoDB items and attributes. Reference: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/specifyingconditions.html https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_boundaries.html
QUESTION NO: 115 회사는 새로 취득한 AWS 계정의 보안 태세를 감사해야 합니다. 회사의 데이터 보안 팀은 Amazon S3 버킷이 공개적으로 노출될 때만 알림을 요구합니다. 회사는 데이터 보안 팀의 이메일 주소가 구독된 Amazon Simple Notification Service(Amazon SNS) 주제를 이미 설정했습니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. isPublic 이벤트에 대해 모든 S3 버킷에서 S3 이벤트 알림을 생성합니다. 이벤트 알림 대상으로 SNS 주제를 선택합니다. B. AWS Identity and Access Management Access Analyzer에서 분석기를 생성합니다. "isPublic: true"에 대한 필터를 사용하여 이벤트 유형 "액세스 분석기 결과"에 대한 Amazon EventBridge 규칙을 생성합니다. EventBridge 규칙 대상으로 SNS 주제를 선택합니다. C. "PutBucketPolicy"에 대한 필터를 사용하여 이벤트 유형 "CloudTrail을 통한 버킷 수준 API 호출"에 대한 Amazon EventBridge 규칙을 생성합니다. EventBridge 규칙 대상으로 SNS 주제를 선택합니다. D. AWS Config를 활성화하고 cloudtrail-s3-dataevents-enabled 규칙을 추가합니다. "NON_COMPLIANT"에 대한 필터를 사용하여 "Config Rules Re-evaluation Status" 이벤트 유형에 대한 Amazon EventBridge 규칙을 생성합니다. EventBridge 규칙 대상으로 SNS 주제를 선택합니다. Answer: B Explanation: Access Analyzer is to assess the access policy. https://docs.aws.amazon.com/ja_jp/AmazonS3/latest/userguide/access-control-block-publicaccess.html
QUESTION NO: 116 회사에서 AWS 클라우드에 솔루션을 구축하고 있습니다. 수천 또는 장치가 솔루션에 연결하고 데이터를 보냅니다. 각 장치는 MQTT 프로토콜을 통해 실시간으로 데이터를 보내고 받을 수 있어야 합니다. 각 장치는 고유한 X.509 인증서를 사용하여 인증해야 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS loT Core를 설정합니다. 각 디바이스에 대해 해당 Amazon MQ 대기열을 생성하고 인증서를 프로비저닝합니다. 각 디바이스를 Amazon MQ에 연결합니다. B. NLB(Network Load Balancer)를 생성하고 AWS Lambda 권한 부여자로 구성합니다. Auto Scaling 그룹의 Amazon EC2 인스턴스에서 MQTT 브로커를 실행합니다. Auto Scaling 그룹을 NLB의 대상으로 설정합니다. 각 장치를 NLB에 연결합니다. C. AWS loT Core를 설정합니다. 각 디바이스에 대해 해당 AWS loT 사물을 생성하고 인증서를 프로비저닝합니다. 각 장치를 AWS loT Core에 연결합니다. D. Amazon API Gateway HTTP API 및 NLB(Network Load Balancer)를 설정합니다. API 게이트웨이와 NLB 간의 통합을 생성합니다. HTTP API에서 상호 TLS 인증 권한 부여자를 구성합니다. NLB가 대상으로 하는 Amazon EC2 인스턴스에서 MQTT 브로커를 실행합니다. 각 장치를 NLB에 연결합니다. Answer: D Explanation: This solution requires minimal operational overhead, as it only requires setting up AWS IoT Core and creating a thing for each device. (Reference: AWS Certified Solutions Architect - Professional Official Amazon Text Book, Page 537) AWS IoT Core is a fully managed service that enables secure, bi-directional communication between internet-connected devices and the AWS Cloud. It supports the MQTT protocol and includes built-in device authentication and access control. By using AWS IoT Core, the company can easily provision and manage the X.509 certificates for each device, and connect the devices to the service with minimal operational overhead.
QUESTION NO: 117 금융 회사는 현재 세대의 Linux EC2 인스턴스에서 비즈니스 크리티컬 애플리케이션을 실행하고 있습니다. 애플리케이션에는 많은 I/O 작업을 수행하는 자체 관리형 MySQL 데이터베이스가 포함되어 있습니다. 응용 프로그램은 한 달 동안 적당한 양의 트래픽을 처리할 수 있도록 잘 작동합니다. 그러나 회사가 증가하는 수요를 충족하기 위해 인프라 내에서 Elastic Load Balancer 및 Auto Scaling을 사용하고 있음에도 불구하고 월말 보고로 인해 매달 마지막 3일 동안 속도가 느려집니다. 다음 작업 중 데이터베이스가 성능에 미치는 영향을 최소화하면서 월말 로드를 처리할 수 있는 작업은 무엇입니까? A. 더 큰 인스턴스 유형을 사용하여 모든 Amazon EBS 볼륨을 GP2 볼륨으로 변경하는 Elastic Load Balancer를 사전 준비합니다. B. 데이터베이스 클러스터를 Amazon RDS로 일회성 마이그레이션 수행. 월말에 로드를 처리하기 위해 몇 개의 추가 읽기 복제본 생성 C. AWS Lambda와 함께 Amazon CioudWatch를 사용하여 유형을 변경합니다. 특정 CloudWatch 지표를 기반으로 클러스터에서 Amazon EBS 볼륨의 크기 또는 IOPS D. 월말 이전에 스냅샷을 생성하고 이후에 되돌려 기존의 모든 Amazon EBS 볼륨을 사용 가능한 최대 스토리지 크기 및 초당 I/O가 있는 새 PIOPS 볼륨으로 교체합니다. Answer: B Explanation: In this scenario, the Amazon EC2 instances are in an Auto Scaling group already which means that the database read operations is the possible bottleneck especially during the month-end wherein the reports are generated. This can be solved by creating RDS read replicas.
QUESTION NO: 118 회사에서 웹 애플리케이션을 개발했습니다. 회사는 Application Load Balancer 뒤의 Amazon EC2 인스턴스 그룹에서 애플리케이션을 호스팅하고 있습니다. 이 회사는 애플리케이션의 보안 태세를 개선하기를 원하며 AWS WAF 웹 ACL을 사용할 계획입니다. 솔루션은 애플리케이션에 대한 합법적인 트래픽에 부정적인 영향을 미치지 않아야 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 웹 ACL을 어떻게 구성해야 합니까? A. 웹 ACL 규칙의 동작을 Count로 설정합니다. AWS WAF 로깅 활성화 오탐에 대한 요청 분석 오탐을 방지하도록 규칙 수정 시간이 지남에 따라 웹 ACL 규칙의 작업을 개수에서 차단으로 변경합니다. B. 웹 ACL에서 비율 기반 규칙만 사용하십시오. 제한을 최대한 높게 설정하고 제한을 초과하는 모든 요청을 일시적으로 차단합니다. 속도 추적 범위를 좁히기 위해 중첩된 규칙을 정의합니다. C. 웹 ACL 규칙의 동작을 차단으로 설정합니다. 웹 ACL에서 AWS 관리형 규칙 그룹만 사용 AWS WAF 샘플링 요청 또는 AWS WAF 로그와 함께 Amazon CloudWatch 지표를 사용하여 규칙 그룹을 평가합니다. D. 웹 ACL에서 사용자 지정 규칙 그룹만 사용합니다. AWS WAF 로깅 활성화 요청을 분석하여 오탐을 방지하도록 규칙을 수정합니다. 시간이 지남에 따라 웹 ACL 규칙의 작업을 허용에서 차단으로 변경합니다. Answer: A Explanation: https://aws.amazon.com/premiumsupport/knowledge-center/waf-analyze-count-action-rules/
QUESTION NO: 119 디지털 마케팅 회사에는 다양한 팀에 속한 여러 AWS 계정이 있습니다. 크리에이티브 팀은 AWS 계정의 Amazon S3 버킷을 사용하여 회사 마케팅 캠페인의 콘텐츠로 사용되는 이미지와 미디어 파일을 안전하게 저장합니다. 크리에이티브 팀은 전략 팀이 개체를 볼 수 있도록 전략 팀과 S3 버킷을 공유하려고 합니다. 솔루션 설계자가 전략 계정에서 이름이 strategy_reviewer인 IAM 역할을 생성했습니다. 또한 솔루션 설계자는 Creative 계정에 사용자 지정 AWS Key Management Service(AWS KMS) 키를 설정하고 키를 S3 버킷과 연결했습니다. 그러나 전략 계정의 사용자가 IAM 역할을 맡고 S3 버킷의 객체에 액세스하려고 하면 계정을 받습니다. 솔루션 설계자는 전략 계정의 사용자가 S3 버킷에 액세스할 수 있는지 확인해야 합니다. 솔루션은 이러한 사용자에게 필요한 최소한의 권한만 제공해야 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까? (3개를 선택하세요.) A. S3 버킷에 대한 읽기 권한을 포함하는 버킷 정책을 생성합니다. 버킷 정책의 주체를 전략 계정의 계정 ID로 설정합니다. B. S3 버킷에 대한 전체 권한을 부여하고 사용자 지정 KMS 키에 대한 암호 해독 권한을 부여하도록 strategy_reviewer IAM 역할을 업데이트합니다. C. Creative 계정에서 사용자 지정 KMS 키 정책을 업데이트하여 strategy_reviewer IAM 역할에 암호 해독 권한을 부여합니다. D. S3 버킷에 대한 읽기 권한을 포함하는 버킷 정책을 생성합니다. 버킷 정책의 주체를 익명 사용자로 설정합니다. E. Creative 계정에서 사용자 지정 KMS 키 정책을 업데이트하여 strategy_reviewer IAM 역할에 암호화 권한을 부여합니다. F. S3 버킷에 대한 읽기 권한을 부여하고 사용자 지정 KMS 키에 대한 암호 해독 권한을 부여하도록 strategy_reviewer IAM 역할을 업데이트합니다. Answer: A,C,F Explanation: https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-deniederror-s3/
QUESTION NO: 120 회사에는 NAT 게이트웨이에 대해 활성화된 VPC 흐름 로그가 있습니다. 회사는 공용 IP 주소에서 오는 인바운드 트래픽에 대해 Action = ACCEPT를 보고 있습니다. 프라이빗 Amazon EC2 인스턴스로 향하는 198.51.100.2. 솔루션 설계자는 트래픽이 인터넷으로부터의 원치 않는 인바운드 연결을 나타내는지 여부를 결정해야 합니다. VPC CIDR 블록의 처음 두 옥텟은 203.0입니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 단계를 수행해야 합니까? A. AWS CloudTrail 콘솔을 엽니다. NAT 게이트웨이의 탄력적 네트워크 인터페이스와 프라이빗 인스턴스의 탄력적 네트워크 인터페이스가 포함된 로그 그룹을 선택합니다. 쿼리를 실행하여 "like 203.0"으로 설정된 대상 주소와 "like 198.51.100.2"로 설정된 소스 주소로 필터링합니다. stats 명령을 실행하여 소스 주소와 대상 주소가 전송한 바이트의 합계를 필터링합니다. B. Amazon CloudWatch 콘솔을 엽니다. NAT 게이트웨이의 탄력적 네트워크 인터페이스와 프라이빗 인스턴스의 탄력적 네트워크 인터페이스가 포함된 로그 그룹을 선택합니다. 쿼리를 실행하여 "like 203.0"으로 설정된 대상 주소와 "like 198.51.100.2"로 설정된 소스 주소로 필터링합니다. stats 명령을 실행하여 소스 주소와 대상 주소가 전송한 바이트의 합계를 필터링합니다. C. AWS CloudTrail 콘솔을 엽니다. NAT 게이트웨이의 탄력적 네트워크 인터페이스와 프라이빗 인스턴스의 탄력적 네트워크 인터페이스가 포함된 로그 그룹을 선택합니다. "like 198.51.100.2"로 설정된 대상 주소와 "like 203.0"으로 설정된 소스 주소로 필터링하려면 쿼리를 실행하십시오. stats 명령을 실행하여 소스 주소와 대상 주소가 전송한 바이트의 합계를 필터링합니다. D. Amazon CloudWatch 콘솔을 엽니다. NAT 게이트웨이의 탄력적 네트워크 인터페이스와 프라이빗 인스턴스의 탄력적 네트워크 인터페이스가 포함된 로그 그룹을 선택합니다. "like 198.51.100.2"로 설정된 대상 주소와 "like 203.0"으로 설정된 소스 주소로 필터링하려면 쿼리를 실행하십시오. stats 명령을 실행하여 소스 주소와 대상 주소가 전송한 바이트의 합계를 필터링합니다. Answer: D Explanation: https://aws.amazon.com/premiumsupport/knowledge-center/vpc-analyze-inbound-traffic-natgateway/ by Cloudxie says "select appropriate log"
QUESTION NO: 121 회사는 AWS에서 애플리케이션을 실행합니다. 회사는 여러 소스의 데이터를 관리합니다. 이 회사는 독점 알고리즘을 사용하여 데이터 변환 및 집계를 수행합니다. 회사에서 E TL 프로세스를 수행한 후 회사는 결과를 Amazon Redshift 테이블에 저장합니다. 회사는 이 데이터를 다른 회사에 판매합니다. 회사는 Amazon Redshift 테이블에서 데이터를 파일로 다운로드하고 FTP를 사용하여 여러 데이터 고객에게 파일을 전송합니다. 데이터 고객의 수가 크게 증가했습니다. 데이터 고객 관리가 어려워졌습니다. 회사는 AWS Data Exchange를 사용하여 회사가 고객과 데이터를 공유하는 데 사용할 수 있는 데이터 제품을 만들 것입니다. 회사는 회사가 데이터를 공유하기 전에 고객의 신원을 확인하려고 합니다. 또한 고객은 회사가 데이터를 게시할 때 최신 데이터에 액세스해야 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. API용 AWS Data Exchange를 사용하여 고객과 데이터를 공유하십시오. 구독 확인 구성 데이터를 생성하는 회사의 AWS 계정에서 Amazon Redshift와 Amazon API Gateway 데이터 API 서비스 통합을 생성합니다. 데이터 고객이 데이터 제품을 구독하도록 요구 데이터를 생산하는 회사의 AWS 계정에서 AWS Data Exchange를 Redshift에 연결하여 AWS Data Exchange 데이터 공유를 생성합니다. B. 클러스터. 구독 확인을 구성합니다. 데이터 고객이 데이터 제품을 구독하도록 요구합니다. C. Amazon Redshift 테이블에서 Amazon S3 버킷으로 데이터를 주기적으로 다운로드합니다. S3용 AWS Data Exchange를 사용하여 고객과 데이터를 공유하십시오. D. 구독 확인을 구성합니다. 데이터 고객이 데이터 제품을 구독하도록 요구합니다. Amazon Redshift 데이터를 AWS Data Exchange의 Open Data에 게시합니다. 고객이 AWS Data Exchange에서 데이터 제품을 구독하도록 요구합니다. 데이터를 생산하는 회사의 AWS 계정에서 IAM 리소스 기반 정책을 Amazon Redshift 테이블에 연결하여 확인된 AWS 계정에만 액세스할 수 있도록 합니다. Answer: C Explanation: The company should download the data from the Amazon Redshift tables to an Amazon S3 bucket periodically and use AWS Data Exchange for S3 to share data with customers. The company should configure subscription verification and require the data customers to subscribe to the data product. This solution will meet the requirements with the least operational overhead because AWS Data Exchange for S3 is a feature that enables data subscribers to access third-party data files directly from data providers' Amazon S3 buckets. Subscribers can easily use these files for their data analysis with AWS services without needing to create or manage data copies. Data providers can easily set up AWS Data Exchange for S3 on top of their existing S3 buckets to share direct access to an entire S3 bucket or specific prefixes and S3 objects. AWS Data Exchange automatically manages subscriptions, entitlements, billing, and payment1. The other options are not correct because: Using AWS Data Exchange for APIs to share data with customers would not work because AWS Data Exchange for APIs is a feature that enables data subscribers to access third-party APIs directly from data providers' AWS accounts. Subscribers can easily use these APIs for their data analysis with AWS services without needing to manage API keys or tokens. Data providers can easily set up AWS Data Exchange for APIs on top of their existing API Gateway resources to share direct access to an entire API or specific routes and stages2. However, this feature is not suitable for sharing data from Amazon Redshift tables, which are not exposed as APIs. Creating an Amazon API Gateway Data API service integration with Amazon Redshift would not work because the Data API is a feature that enables you to query your Amazon Redshift cluster using HTTP requests, without needing a persistent connection or a SQL client3. It is useful for building applications that interact with Amazon Redshift, but not for sharing data files with customers. Creating an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift cluster would not work because AWS Data Exchange does not support datashares for Amazon Redshift clusters. A datashare is a feature that enables you to share live and secure access to your Amazon Redshift data across your accounts or with third parties without copying or moving the underlying data4. It is useful for sharing query results and views with other users, but not for sharing data files with customers. Publishing the Amazon Redshift data to an Open Data on AWS Data Exchange would not work because Open Data on AWS Data Exchange is a feature that enables you to find and use free and public datasets from AWS customers and partners. It is useful for accessing open and free data, but not for confirming the identities of the customers or charging them for the data. Reference: https://aws.amazon.com/data-exchange/why-aws-data-exchange/s3/ https://aws.amazon.com/data-exchange/why-aws-data-exchange/api/ https://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html https://docs.aws.amazon.com/redshift/latest/dg/datashare-overview.html https://aws.amazon.com/data-exchange/open-data/
QUESTION NO: 122 한 회사가 온프레미스에서 AWS로 워크로드를 마이그레이션하려고 합니다. 워크로드는 Linux 및 Windows에서 실행됩니다. 이 회사는 수많은 애플리케이션을 호스팅하는 물리적 머신과 VM으로 구성된 대규모 온프레미스 내부 구조를 가지고 있습니다. 회사는 시스템 구성에 대한 세부 정보를 캡처해야 합니다. 시스템 성능. o의 프로세스 및 네트워크 coi.net을 실행합니다. - 구내, 보드에. 회사는 또한 AWS 마이그레이션을 위해 온프레미스 애플리케이션을 그룹으로 나누어야 합니다. 회사는 가장 비용 효율적인 방식으로 AWS에서 워크로드를 실행할 수 있도록 Amazon EC2 인스턴스 유형에 대한 권장 사항이 필요합니다. 이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 단계 조합을 수행해야 합니까? (3개를 선택하세요.) A. 물리적 머신과 VM에 AWS Application Discovery Agent를 설치하여 기존 애플리케이션을 평가합니다. B. 물리적 시스템 및 VM에 AWS Systems Manager 에이전트를 설치하여 기존 애플리케이션을 평가합니다. C. AWS Systems Manager Application Manager를 사용하여 서버를 마이그레이션할 애플리케이션으로 그룹화합니다. D. AWS Migration Hub를 사용하여 서버를 마이그레이션용 애플리케이션으로 그룹화합니다. E. AWS Migration Hub를 사용하여 권장 인스턴스 유형 및 관련 비용을 생성합니다. F. 서버 크기에 대한 데이터를 AWS Trusted Advisor로 가져옵니다. 비용 최적화를 위한 권장 사항을 따르십시오. Answer: A,D,E Explanation: https://docs.aws.amazon.com/application-discovery/latest/userguide/discovery-agent.html https://docs.aws.amazon.com/migrationhub/latest/ug/ec2-recommendations.html
QUESTION NO: 123 회사는 Amazon RDS for MySQL 데이터베이스가 있는 Application Load Balancer(ALB) 뒤에 있는 두 개의 Linux Amazon EC2 인스턴스에서 중요한 상태 저장 웹 애플리케이션을 실행하고 있습니다. 회사는 Amazon Route 53에서 애플리케이션에 대한 DNS 레코드를 호스팅합니다. 솔루션 설계자는 솔루션을 추천해야 합니다. 애플리케이션의 복원력을 개선하기 위해 솔루션은 다음 목표를 충족해야 합니다. * 애플리케이션 계층 RPO는 2분입니다. 30분의 RTO * 5분의 데이터베이스 계층 RPO 30분의 RTO 회사는 기존 애플리케이션 아키텍처를 크게 변경하고 싶지 않습니다. 회사는 장애 조치 후 최적의 대기 시간을 보장해야 합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까? A. AWS Elastic Disaster Recovery를 사용하도록 EC2 인스턴스 구성 RDS DB 인스턴스에 대한 교차 리전 읽기 복제본 생성 두 번째 AWS 리전에서 ALB 생성 AWS Global Accelerator 엔드포인트 생성 및 엔드포인트를 ALB 업데이트 DNS 레코드와 연결 Global Accelerator 끝점을 가리키도록 B. Amazon Data Lifecycle Manager(Amazon DLM)를 사용하여 EBS 볼륨의 스냅샷을 생성하도록 EC2 인스턴스 구성 RDS 자동 백업 구성 두 번째 AWS 리전에 대한 백업 복제 구성 두 번째 리전에 ALB 생성 AWS Global Accelerator 엔드포인트 생성 , 그리고 끝점을 ALB와 연결합니다. Global Accelerator 끝점을 가리키도록 DNS 레코드를 업데이트합니다. C. EC2 인스턴스 및 RDS DB 인스턴스에 대해 AWS Backup에서 백업 계획 생성 두 번째 AWS 리전에 대한 백업 복제 구성 두 번째 리전에 ALB 생성 ALB 앞에 Amazon CloudFront 배포 구성 다음을 가리키도록 DNS 레코드 업데이트 클라우드프론트 D. Amazon Data Lifecycle Manager(Amazon DLM)를 사용하여 EBS 볼륨의 스냅샷을 생성하도록 EC2 인스턴스 구성 RDS DB 인스턴스에 대한 교차 리전 읽기 복제본 생성 두 번째 AWS 리전에서 ALB 생성 AWS Global Accelerator 생성 끝점을 만들고 끝점을 ALB와 연결 Answer: B Explanation: This option meets the RPO and RTO requirements for both the application and database tiers and uses tools like Amazon DLM and RDS automated backups to create and manage the backups. Additionally, it uses Global Accelerator to ensure low latency after failover by directing traffic to the closest healthy endpoint.
QUESTION NO: 124 회사는 미국의 AWS 리전에서 판매 보고 애플리케이션을 실행합니다. 이 애플리케이션은 Amazon API Gateway 지역 API 및 AWS Lambda 함수를 사용하여 Amazon RDS for MySQL 데이터베이스의 데이터에서 온디맨드 보고서를 생성합니다. 애플리케이션의 프런트엔드는 Amazon S3에서 호스팅되며 Amazon CloudFront 배포를 통해 사용자가 액세스합니다. 이 회사는 Amazon Route 53을 도메인의 DNS 서비스로 사용하고 있습니다. Route 53은 트래픽을 API Gateway API로 라우팅하는 간단한 라우팅 정책으로 구성됩니다. 향후 6개월 내에 회사는 유럽으로 사업을 확장할 계획입니다. 데이터베이스 트래픽의 90% 이상이 읽기 전용 트래픽입니다. 회사는 이미 새 리전에 API Gateway API 및 Lambda 함수를 배포했습니다. 솔루션 설계자는 보고서를 다운로드하는 사용자의 대기 시간을 최소화하는 솔루션을 설계해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 전체 로드가 포함된 AWS Database Migration Service(AWS DMS) 작업을 사용하여 원래 리전의 기본 데이터베이스를 새 리전의 데이터베이스로 복제합니다. Route 53 레코드를 지연 시간 기반 라우팅으로 변경하여 API Gateway API에 연결합니다. B. 전체 로드와 변경 데이터 캡처(CDC)가 포함된 AWS Database Migration Service(AWS DMS) 작업을 사용하여 원래 리전의 기본 데이터베이스를 새 리전의 데이터베이스로 복제합니다. Route 53 레코드를 지리적 위치 라우팅으로 변경하여 API Gateway API에 연결합니다. C. 새 리전의 RDS 데이터베이스에 대한 리전 간 읽기 전용 복제본을 구성합니다. Route 53 레코드를 지연 시간 기반 라우팅으로 변경하여 API Gateway API에 연결합니다. D. 새 리전의 RDS 데이터베이스에 대한 리전 간 읽기 전용 복제본을 구성합니다. Route 53 레코드를 지리적 위치 라우팅으로 변경하여 API에 연결 Answer: C Explanation: The company should configure a cross-Region read replica for the RDS database in the new Region. The company should change the Route 53 record to latency-based routing to connect to the API Gateway API. This solution will meet the requirements because a cross-Region read replica is a feature that enables you to create a MariaDB, MySQL, Oracle, PostgreSQL, or SQL Server read replica in a different Region from the source DB instance. You can use cross-Region read replicas to improve availability and disaster recovery, scale out globally, or migrate an existing database to a new Region1. By creating a cross-Region read replica for the RDS database in the new Region, the company can have a standby copy of its primary database that can serve read-only traffic from users in Europe. A latency-based routing policy is a feature that enables you to route traffic based on the latency between your users and your resources. You can use latency-based routing to route traffic to the resource that provides the best latency2. By changing the Route 53 record to latency-based routing, the company can minimize latency for users who download reports by connecting them to the API Gateway API in the Region that provides the best response time. The other options are not correct because: Using AWS Database Migration Service (AWS DMS) to replicate the primary database in the original Region to the database in the new Region would not be as cost-effective or simple as using a cross-Region read replica. AWS DMS is a service that enables you to migrate relational databases, data warehouses, NoSQL databases, and other types of data stores. You can use AWS DMS to perform one-time migrations or continuous data replication with high availability and consolidate databases into a petabyte-scale data warehouse3. However, AWS DMS requires more configuration and management than creating a cross-Region read replica, which is fully managed by Amazon RDS. AWS DMS also incurs additional charges for replication instances and tasks. Creating an Amazon API Gateway Data API service integration with Amazon Redshift would not help with disaster recovery or minimizing latency. The Data API is a feature that enables you to query your Amazon Redshift cluster using HTTP requests, without needing a persistent connection or a SQL client. It is useful for building applications that interact with Amazon Redshift, but not for replicating or recovering data from an RDS database. Creating an AWS Data Exchange datashare by connecting AWS Data Exchange to the Redshift cluster would not help with disaster recovery or minimizing latency. AWS Data Exchange is a service that makes it easy for AWS customers to exchange data in the cloud. You can use AWS Data Exchange to subscribe to a diverse selection of third-party data products or offer your own data products to other AWS customers. A datashare is a feature that enables you to share live and secure access to your Amazon Redshift data across your accounts or with third parties without copying or moving the underlying data. It is useful for sharing query results and views with other users, but not for replicating or recovering data from an RDS database. Reference: https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/Concepts.RDS_Fea_Regions_ DB-eng.Feature.CrossRegionReadReplicas.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-policy.html#routingpolicy-latency https://aws.amazon.com/dms/ https://docs.aws.amazon.com/redshift/latest/mgmt/data-api.html https://aws.amazon.com/data-exchange/ https://docs.aws.amazon.com/redshift/latest/dg/datashare-overview.html
QUESTION NO: 125 회사에는 Amazon DynamoDB를 스토리지 백엔드로 사용하는 지연 시간에 민감한 거래 플랫폼이 있습니다. 회사는 온디맨드 용량 모드를 사용하도록 DynamoDB 테이블을 구성했습니다. 솔루션 설계자는 거래 플랫폼의 성능을 개선하기 위한 솔루션을 설계해야 합니다. 새로운 솔루션은 거래 플랫폼의 고가용성을 보장해야 합니다. 최소 대기 시간으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 2노드 DynamoDB Accelerator(DAX) 클러스터 생성 DAX를 사용하여 데이터를 읽고 쓰도록 애플리케이션을 구성합니다. B. 3노드 DynamoDB Accelerator(DAX) 클러스터를 생성합니다. DAX를 사용하여 데이터를 읽고 DynamoDB 테이블에 직접 데이터를 쓰도록 애플리케이션을 구성합니다. C. 3노드 DynamoDB Accelerator(DAX) 클러스터를 생성합니다. DynamoDB 테이블에서 직접 데이터를 읽고 DAX를 사용하여 데이터를 쓰도록 애플리케이션을 구성합니다. D. 단일 노드 DynamoD8 Accelerator(DAX) 클러스터를 생성합니다. DAX를 사용하여 데이터를 읽고 DynamoD8 테이블에 직접 데이터를 쓰도록 애플리케이션을 구성합니다. Answer: B Explanation: A DAX cluster can be deployed with one or two nodes for development or test workloads. One- and two-node clusters are not fault-tolerant, and we don't recommend using fewer than three nodes for production use. If a one- or two-node cluster encounters software or hardware errors, the cluster can become unavailable or lose cached data.A DAX cluster can be deployed with one or two nodes for development or test workloads. One- and two-node clusters are not fault-tolerant, and we don't recommend using fewer than three nodes for production use. If a one- or two-node cluster encounters software or hardware errors, the cluster can become unavailable or lose cached data. https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/DAX.concepts.cluste r.html
QUESTION NO: 126 출판 회사의 디자인 팀은 전자 상거래 웹 애플리케이션에서 사용하는 아이콘 및 기타 정적 자산을 업데이트합니다. 회사는 회사의 프로덕션 계정에서 호스팅되는 Amazon S3 버킷의 아이콘과 자산을 제공합니다. 회사는 또한 디자인 팀 구성원이 액세스할 수 있는 개발 계정을 사용합니다. 디자인 팀이 개발 계정에서 정적 자산을 테스트한 후 디자인 팀은 프로덕션 계정의 S3 버킷에 자산을 로드해야 합니다. 솔루션 설계자는 원치 않는 변경의 위험에 웹 애플리케이션의 다른 부분을 노출시키지 않고 프로덕션 계정에 대한 액세스 권한을 디자인 팀에 제공해야 합니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (3개를 선택하세요.) A. 프로덕션 계정에서 S3 버킷에 대한 읽기 및 쓰기 액세스를 허용하는 새 IAM 정책을 생성합니다. B. 개발 계정에서 S3 버킷에 대한 읽기 및 쓰기 액세스를 허용하는 새 IAM 정책을 생성합니다. C. 프로덕션 계정에서 역할을 생성합니다. 새 정책을 역할에 연결합니다. 개발 계정을 신뢰할 수 있는 엔터티로 정의합니다. D. 개발 계정에서 역할을 생성합니다. 새 정책을 역할에 연결합니다. 프로덕션 계정을 신뢰할 수 있는 엔터티로 정의합니다. E. 개발 계정에서 디자인 팀의 모든 IAM 사용자를 포함하는 그룹을 만듭니다. 프로덕션 계정의 역할에 대한 sts:AssumeRole 작업을 허용하려면 그룹에 다른 IAM 정책을 연결합니다. F. 개발 계정에서 디자인 팀의 모든 tfje IAM 사용자를 포함하는 그룹을 만듭니다. 개발 계정의 역할에 대한 sts;AssumeRole 작업을 허용하려면 그룹에 다른 IAM 정책을 연결합니다. Answer: A,C,E Explanation: 1. In the production account, create a new IAM policy that allows read and write access to the S3 bucket. The policy grants the necessary permissions to access the assets in the production S3 bucket. 2. In the production account, create a role. Attach the new policy to the role. Define the development account as a trusted entity. By creating a role and attaching the policy, and then defining the development account as a trusted entity, the development account can assume the role and access the production S3 bucket with the read and write permissions. 3. In the development account, create a group that contains all the IAM users of the design team. Attach a different IAM policy to the group to allow the sts:AssumeRole action on the role in the production account. The IAM policy attached to the group allows the design team members to assume the role created in the production account, thereby giving them access to the production S3 bucket. Step 1: Create a role in the Production Account; create the role in the Production account and specify the Development account as a trusted entity. You also limit the role permissions to only read and write access to the productionapp bucket. Anyone granted permission to use the role can read and write to the productionapp bucket. Step 2: Grant access to the role Sign in as an administrator in the Development account and allow the AssumeRole action on the UpdateApp role in the Production account. So, recap, production account you create the policy for S3, and you set development account as a trusted entity. Then on the development account you allow the sts:assumeRole action on the role in production account. https://docs.aws.amazon.com/IAM/latest/UserGuide/tutorial_cross-account-with-roles.html
QUESTION NO: 127 한 회사에서 AWS Elastic Beanstalk 및 Jav a를 사용하여 파일럿 애플리케이션을 개발했습니다. 개발 중 비용을 절감하기 위해 회사의 개발 팀은 애플리케이션을 단일 인스턴스 환경에 배포했습니다. 최근 테스트에 따르면 애플리케이션이 예상보다 더 많은 CPU를 사용하는 것으로 나타났습니다. CPU 사용률은 정기적으로 85%보다 높으며 이로 인해 일부 성능 병목 현상이 발생합니다. 솔루션 설계자는 회사가 응용 프로그램을 프로덕션으로 시작하기 전에 성능 문제를 완화해야 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 새 Elastic Beanstalk 애플리케이션을 만듭니다. 부하 분산 환경 유형을 선택합니다. 모든 가용 영역을 선택합니다. 최대 CPU 사용률이 5분 동안 85%를 초과하는 경우 실행될 확장 규칙을 추가합니다. B. 두 번째 Elastic Beanstalk 환경을 생성합니다. 트래픽 분할 배포 정책을 적용합니다. 평균 CPU 사용률이 5분 동안 85% 이상인 경우 새 환경으로 보낼 들어오는 트래픽의 백분율을 지정합니다. C. 부하 분산 환경 유형을 사용하도록 기존 환경의 용량 구성을 수정합니다. 모든 가용 영역을 선택합니다. 평균 CPU 사용률이 5분 동안 85%를 초과하는 경우 실행될 확장 규칙을 추가합니다. D. 로드 밸런싱 옵션으로 환경 재구축 작업을 선택합니다. 가용 영역을 선택합니다. 총 CPU 사용률이 5분 동안 85%를 초과하는 경우 실행할 스케일 아웃 규칙을 추가합니다. Answer: C Explanation: This solution will meet the requirements with the least operational overhead because it allows the company to modify the existing environment's capacity configuration, so it becomes a load-balanced environment type. By selecting all availability zones, the company can ensure that the application is running in multiple availability zones, which can help to improve the availability and scalability of the application. The company can also add a scale-out rule that will run if the average CPU utilization is over 85% for 5 minutes, which can help to mitigate the performance issues. This solution does not require creating new Elastic Beanstalk environments or rebuilding the existing one, which reduces the operational overhead. You can refer to the AWS Elastic Beanstalk documentation for more information on how to use this service: https://aws.amazon.com/elasticbeanstalk/ You can refer to the AWS documentation for more information on how to use autoscaling: https://aws.amazon.com/autoscaling/
QUESTION NO: 128 회사에서 대량의 보관 문서를 저장하고 회사 인트라넷을 통해 직원이 문서를 사용할 수 있도록 할 계획입니다. 직원은 VPC에 연결된 클라이언트 VPN 서비스를 통해 연결하여 시스템에 액세스합니다. 데이터는 대중이 접근할 수 없어야 합니다. 회사가 저장하고 있는 문서는 다른 곳의 물리적 매체에 보관된 데이터의 사본입니다. 요청 수가 적습니다. 가용성과 검색 속도는 회사의 관심사가 아닙니다. 최저 비용으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Amazon S3 버킷을 생성합니다. S3 One Zone-Infrequent Access(S3 One Zone-IA) 스토리지 클래스를 기본값으로 사용하도록 S3 버킷을 구성합니다. 웹 사이트 호스팅을 위해 S3 버킷을 구성합니다. S3 인터페이스 엔드포인트를 생성합니다. 해당 엔드포인트를 통해서만 액세스를 허용하도록 S3 버킷을 구성합니다. B. 웹 서버를 실행하는 Amazon EC2 인스턴스를 시작합니다. Amazon Elastic File System(Amazon EFS) 파일 시스템을 연결하여 보관된 데이터를 EFS One Zone-Infrequent Access(EFS One Zone-IA) 스토리지 클래스에 저장합니다. 프라이빗 네트워크에서만 액세스할 수 있도록 인스턴스 보안 그룹을 구성합니다. C. 웹 서버를 실행하는 Amazon EC2 인스턴스를 시작합니다. Amazon Elastic Block Store(Amazon EBS) 볼륨을 연결하여 아카이브된 데이터를 저장합니다. Cold HDD(sc1) 볼륨 유형을 사용합니다. 사설 네트워크에서만 액세스를 허용하도록 인스턴스 보안 그룹을 구성합니다. D. Amazon S3 버킷을 생성합니다. S3 Glacier Deep Archive 스토리지 클래스를 기본값으로 사용하도록 S3 버킷을 구성합니다. 웹 사이트 호스팅을 위해 S3 버킷을 구성합니다. S3 인터페이스 엔드포인트를 생성합니다. 해당 엔드포인트를 통해서만 액세스를 허용하도록 S3 버킷을 구성합니다. Answer: D Explanation: The S3 Glacier Deep Archive storage class is the lowest-cost storage class offered by Amazon S3, and it is designed for archival data that is accessed infrequently and for which retrieval time of several hours is acceptable. S3 interface endpoint for the VPC ensures that access to the bucket is only from resources within the VPC and this will meet the requirement of not being accessible to the public. And also, S3 bucket can be configured for website hosting, and this will allow employees to access the documents through the corporate intranet. Using an EC2 instance and a file system or block store would be more expensive and unnecessary because the number of requests to the data will be low and availability and speed of retrieval are not concerns. Additionally, using Amazon S3 bucket will provide durability, scalability and availability of data.
QUESTION NO: 129 회사에 Amazon CloudFront, Amazon API Gateway 및 AWS Lambda 기능으로 구성된 서버리스 애플리케이션이 있습니다. 애플리케이션 코드의 현재 배포 프로세스는 Lambda 함수의 새 버전 번호를 생성하고 AWS CLI 스크립트를 실행하여 업데이트하는 것입니다. 새 함수 버전에 오류가 있는 경우 다른 CLI 스크립트는 함수의 이전 작업 버전을 배포하여 되돌립니다. 회사는 Lambda 함수에서 제공하는 애플리케이션 로직의 새 버전을 배포하는 시간을 줄이고 오류가 식별되었을 때 이를 감지하고 되돌리는 시간도 줄이고자 합니다. 이것이 어떻게 이루어질 수 있습니까? A. AWS CloudFront 배포 및 API Gateway로 구성된 상위 스택과 Lambda 함수를 포함하는 하위 스택으로 중첩된 AWS CloudFormation 스택을 생성하고 배포합니다. Lambda를 변경하려면 AWS CloudFormation 변경 세트를 생성하고 배포하십시오. 오류가 트리거되면 AWS CloudFormation 변경 세트를 이전 버전으로 되돌립니다. B. AWS SAM 및 기본 제공 AWS CodeDeploy를 사용하여 새 Lambda 버전을 배포하고 트래픽을 새 버전으로 점진적으로 전환하고 트래픽 전 및 트래픽 후 테스트 기능을 사용하여 코드를 확인합니다. Amazon CloudWatch 경보가 트리거되면 롤백합니다. C. AWS CLI 스크립트를 새 Lambda 버전을 배포하는 단일 스크립트로 리팩터링합니다. 배포가 완료되면 스크립트 테스트가 실행됩니다. 오류가 감지되면 이전 Lambda 버전으로 되돌립니다. D. 새 Lambda 버전을 참조하는 새 API 게이트웨이 엔드포인트로 구성된 AWS CloudFormation 스택을 생성하고 배포합니다. CloudFront 오리진을 새 API 게이트웨이 엔드포인트로 변경하고 오류를 모니터링하며 감지된 경우 AWS CloudFront 오리진을 이전 API 게이트웨이 엔드포인트로 변경합니다. Answer: B Explanation: https://aws.amazon.com/about-aws/whats-new/2017/11/aws-lambda-supports-traffic-shiftingand-phased-deployments-with-aws-codedeploy/
QUESTION NO: 130 회사는 데이터를 처리하기 위해 Amazon EC2 인스턴스에서 Python 스크립트를 실행합니다. 스크립트는 10분마다 실행됩니다. 스크립트는 Amazon S3 버킷에서 파일을 수집하고 파일을 처리합니다. 평균적으로 스크립트는 각 파일을 처리하는 데 약 5분이 걸립니다. 스크립트는 스크립트가 이미 처리한 파일을 다시 처리하지 않습니다. 이 회사는 Amazon CloudWatch 지표를 검토한 결과 파일 처리 속도 때문에 EC2 인스턴스가 시간의 약 40% 동안 유휴 상태임을 확인했습니다. 회사는 워크로드의 가용성과 확장성을 높이길 원합니다. 회사는 또한 장기적인 관리 오버헤드를 줄이기를 원합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까? A. 데이터 처리 스크립트를 AWS Lambda 함수로 마이그레이션합니다. 회사에서 객체를 업로드할 때 S3 이벤트 알림을 사용하여 Lambda 함수를 호출하여 객체를 처리합니다. B. Amazon Simple Queue Service(Amazon SQS) 대기열을 생성합니다. 이벤트 알림을 SQS 대기열로 보내도록 Amazon S3를 구성합니다. 최소 크기가 인스턴스 1개인 EC2 Auto Scaling 그룹을 생성합니다. 데이터 처리 스크립트를 업데이트하여 SQS 대기열을 폴링합니다. SQS 메시지가 식별하는 S3 객체를 처리합니다. C. 데이터 처리 스크립트를 컨테이너 이미지로 마이그레이션합니다. EC2 인스턴스에서 데이터 처리 컨테이너를 실행합니다. 새 객체에 대해 S3 버킷을 폴링하고 결과 객체를 처리하도록 컨테이너를 구성합니다. D. 데이터 처리 스크립트를 AWS Fargate의 Amazon Elastic Container Service(Amazon ECS)에서 실행되는 컨테이너 이미지로 마이그레이션합니다. 컨테이너가 파일을 처리할 때 Fargate RunTaskAPI 작업을 호출하는 AWS Lambda 함수를 생성합니다. S3 이벤트 알림을 사용하여 Lambda 함수를 호출합니다. Answer: D Explanation: migrating the data processing script to an AWS Lambda function and using an S3 event notification to invoke the Lambda function to process the objects when the company uploads the objects. This solution meets the company's requirements of high availability and scalability, as well as reducing long-term management overhead, and is likely to be the most cost-effective option.
QUESTION NO: 131 회사에는 사용자가 짧은 동영상을 업로드할 수 있는 웹 애플리케이션이 있습니다. 동영상은 Amazon EBS 볼륨에 저장되며 분류를 위해 사용자 정의 인식 소프트웨어로 분석됩니다. 이 웹사이트에는 특정 달에 피크가 있는 가변 트래픽이 있는 stat c 콘텐츠가 포함되어 있습니다. 아키텍처는 웹 애플리케이션용 Auto Scaling 그룹에서 실행되는 Amazon EC2 인스턴스와 Amazon SQS 대기열을 처리하기 위해 Auto Scaling 그룹에서 실행되는 EC2 인스턴스로 구성됩니다. 가능하고 타사 소프트웨어에 대한 종속성을 제거합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 웹 애플리케이션에는 Amazon ECS 컨테이너를 사용하고 SQS 대기열을 처리하는 Auto Scaling 그룹에는 스팟 인스턴스를 사용합니다. 사용자 지정 소프트웨어를 Amazon Recognition으로 교체하여 비디오를 분류합니다. B. 업로드된 비디오를 Amazon EFS에 저장하고 웹 애플리케이션용 EC2 인스턴스에 파일 시스템을 탑재합니다. 비디오를 분류하기 위해 Amazon Rekognition API를 호출하는 AWS Lambda 함수로 SOS 대기열을 처리합니다. C. Amazon S3에서 웹 애플리케이션을 호스팅합니다. 업로드된 동영상을 Amazon S3에 저장합니다. S3 이벤트 알림을 사용하여 이벤트를 SQS 대기열에 게시합니다. 비디오를 분류하기 위해 Amazon Rekognition API를 호출하는 AWS Lambda 함수로 SQS 대기열을 처리합니다. D. AWS Elastic Beanstalk를 사용하여 웹 애플리케이션용 Auto Scaling 그룹에서 EC2 인스턴스를 시작하고 작업자 환경을 시작하여 SQS 대기열을 처리합니다. 사용자 지정 소프트웨어를 Amazon Rekognition으로 교체하여 비디오를 분류합니다. Answer: C Explanation: Option C is correct because hosting the web application in Amazon S3, storing the uploaded videos in Amazon S3, and using S3 event notifications to publish events to the SQS queue reduces the operational overhead of managing EC2 instances and EBS volumes. Amazon S3 can serve static content such as HTML, CSS, JavaScript, and media files directly from S3 buckets. Amazon S3 can also trigger AWS Lambda functions through S3 event notifications when new objects are created or existing objects are updated or deleted. AWS Lambda can process the SQS queue with an AWS Lambda function that calls the Amazon Rekognition API to categorize the videos. This solution eliminates the need for custom recognition software and third-party dependencies345
QUESTION NO: 132 모험 회사가 모바일 앱에 새로운 기능을 출시했습니다. 사용자는 이 기능을 사용하여 언제든지 하이킹 및 래팅 사진과 비디오를 업로드할 수 있습니다. 사진과 비디오는 S3 버킷의 Amazon S3 Standard 스토리지에 저장되며 Amazon CloudFront를 통해 제공됩니다. 회사는 스토리지 비용을 최적화해야 합니다. 솔루션 설계자는 업로드된 사진과 비디오의 대부분이 30일 후에 드물게 액세스된다는 것을 발견했습니다. 다만, 업로드된 사진과 영상 중 일부는 30일 이후에 자주 접속되는 경우가 있습니다. 솔루션 설계자는 가능한 가장 낮은 비용으로 사진 및 비디오의 밀리초 검색 가용성을 유지하는 솔루션을 구현해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. S3 버킷에서 S3 Intelligent-Tiering을 구성합니다. B. 30일 후에 S3 Standard에서 S3 Glacier Deep Archive로 이미지 객체 및 비디오 객체를 전환하도록 S3 수명 주기 정책을 구성합니다. C. Amazon S3를 Amazon EC2 인스턴스에 탑재된 Amazon Elastic File System(Amazon EFS) 파일 시스템으로 교체합니다. D. S3 이미지 객체 및 S3 비디오 객체에 Cache-Control: max-age 헤더를 추가합니다. 헤더를 30일로 설정합니다. Answer: B Explanation: Amazon S3 Intelligent-Tiering is a storage class that automatically moves objects between two access tiers based on changing access patterns. Objects that are accessed frequently are stored in the frequent access tier and objects that are accessed infrequently are stored in the infrequent access tier. This allows for cost optimization without requiring manual intervention. This makes it an ideal solution for the scenario described, as it can automatically move objects that are infrequently accessed after 30 days to a lower-cost storage tier while still maintaining millisecond retrieval availability.
QUESTION NO: 133 한 회사가 us-east-1 리전의 Amazon RDS for MySQL DB 인스턴스에 데이터베이스를 배포했습니다. 회사는 유럽의 고객이 데이터를 사용할 수 있도록 해야 합니다. 유럽의 고객은 미국(US)의 고객과 동일한 데이터에 액세스할 수 있어야 하며 높은 애플리케이션 대기 시간 또는 오래된 데이터를 허용하지 않습니다. 유럽 고객과 미국 고객은 데이터베이스에 기록해야 합니다. 두 고객 그룹 모두 다른 그룹의 업데이트를 실시간으로 확인해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. RDS for MySQL DB 인스턴스의 Amazon Aurora MySQL 복제본을 생성합니다. RDS DB 인스턴스에 대한 애플리케이션 쓰기를 일시 중지합니다. Aurora 복제본을 독립 실행형 DB 클러스터로 승격합니다. Aurora 데이터베이스를 사용하도록 애플리케이션을 재구성하고 쓰기를 재개하십시오. eu-west-1을 보조 리전으로 추가합니다. 06 클러스터. DB 클러스터에서 쓰기 전달을 활성화합니다. eu-west-1에 애플리케이션을 배포합니다. eu-west-1에서 Aurora MySQL 엔드포인트를 사용하도록 애플리케이션을 구성합니다. B. RDS for MySQL DB 인스턴스에 대해 eu-west-1에 교차 리전 복제본을 추가합니다. 쓰기 쿼리를 다시 기본 DB 인스턴스로 복제하도록 복제본을 구성합니다. eu-west-1에 애플리케이션을 배포합니다. eu-west-1에서 RDS for MySQL 엔드포인트를 사용하도록 애플리케이션을 구성합니다. C. RDS for MySQL DB 인스턴스에서 eu-west-1로 가장 최근 스냅샷을 복사합니다. 스냅샷에서 eu-west-1에 새 RDS for MySQL DB 인스턴스를 생성합니다. us-east-1에서 euwest-1로 MySQL 논리적 복제를 구성합니다. DB 클러스터에서 쓰기 전달을 활성화합니다. eu-west-1에 애플리케이션을 배포합니다. eu-west-1에서 RDS for MySQL 엔드포인트를 사용하도록 애플리케이션을 구성합니다. D. RDS for MySQL DB 인스턴스를 Amazon Aurora MySQL DB 클러스터로 변환합니다. euwest-1을 DB 클러스터에 보조 리전으로 추가합니다. DB 클러스터에서 쓰기 전달을 활성화합니다. eu-west-1에 애플리케이션을 배포합니다. eu-west-1에서 Aurora MySQL 엔드포인트를 사용하도록 애플리케이션을 구성합니다. Answer: D Explanation: The company should use AWS Amplify to create a static website for uploads of media files. The company should use Amplify Hosting to serve the website through Amazon CloudFront. The company should use Amazon S3 to store the uploaded media files. The company should use Amazon Cognito to authenticate users. This solution will meet the requirements with the least operational overhead because AWS Amplify is a complete solution that lets frontend web and mobile developers easily build, ship, and host full-stack applications on AWS, with the flexibility to leverage the breadth of AWS services as use cases evolve. No cloud expertise needed1. By using AWS Amplify, the company can refactor the application to a serverless architecture that reduces operational complexity and costs. AWS Amplify offers the following features and benefits: Amplify Studio: A visual interface that enables you to build and deploy a full-stack app quickly, including frontend UI and backend. Amplify CLI: A local toolchain that enables you to configure and manage an app backend with just a few commands. Amplify Libraries: Open-source client libraries that enable you to build cloud-powered mobile and web apps. Amplify UI Components: Open-source design system with cloud-connected components for building feature-rich apps fast. Amplify Hosting: Fully managed CI/CD and hosting for fast, secure, and reliable static and server-side rendered apps. By using AWS Amplify to create a static website for uploads of media files, the company can leverage Amplify Studio to visually build a pixel-perfect UI and connect it to a cloud backend in clicks. By using Amplify Hosting to serve the website through Amazon CloudFront, the company can easily deploy its web app or website to the fast, secure, and reliable AWS content delivery network (CDN), with hundreds of points of presence globally. By using Amazon S3 to store the uploaded media files, the company can benefit from a highly scalable, durable, and cost-effective object storage service that can handle any amount of data2. By using Amazon Cognito to authenticate users, the company can add user sign-up, sign-in, and access control to its web app with a fully managed service that scales to support millions of users3. The other options are not correct because: Using AWS Application Migration Service to migrate the application server to Amazon EC2 instances would not refactor the application or accelerate development. AWS Application Migration Service (AWS MGN) is a service that enables you to migrate physical servers, virtual machines (VMs), or cloud servers from any source infrastructure to AWS without requiring agents or specialized tools. However, this would not address the challenges of overutilization and data uploads failures. It would also not reduce operational overhead or costs compared to a serverless architecture. Creating a static website for uploads of media files and using AWS AppSync to create an API would not be as simple or fast as using AWS Amplify. AWS AppSync is a service that enables you to create flexible APIs for securely accessing, manipulating, and combining data from one or more data sources. However, this would require more configuration and management than using Amplify Studio and Amplify Hosting. It would also not provide authentication features like Amazon Cognito. Setting up AWS IAM Identity Center (AWS Single Sign-On) to give users the ability to sign in to the application would not be as suitable as using Amazon Cognito. AWS Single Sign-On (AWS SSO) is a service that enables you to centrally manage SSO access and user permissions across multiple AWS accounts and business applications. However, this service is designed for enterprise customers who need to manage access for employees or partners across multiple resources. It is not intended for authenticating end users of web or mobile apps. Reference: https://aws.amazon.com/amplify/ https://aws.amazon.com/s3/ https://aws.amazon.com/cognito/ https://aws.amazon.com/mgn/ https://aws.amazon.com/appsync/ https://aws.amazon.com/single-sign-on/
QUESTION NO: 134 한 글로벌 제조 회사는 대부분의 애플리케이션을 AWS로 마이그레이션할 계획입니다. 그러나 회사는 데이터 규정 요구 사항 또는 한 자릿수 밀리초의 대기 시간 요구 사항으로 인해 특정 국가 또는 회사의 중앙 온프레미스 데이터 센터에 유지해야 하는 애플리케이션에 대해 우려하고 있습니다. 회사는 또한 제한된 네트워크 인프라가 존재하는 일부 공장 사이트에서 호스팅하는 애플리케이션에 대해 우려하고 있습니다. 회사는 개발자가 애플리케이션을 한 번 빌드하고 온프레미스, 클라우드 또는 하이브리드 아키텍처에 배포할 수 있도록 일관된 개발자 경험을 원합니다. 개발자는 자신에게 익숙한 동일한 도구, API 및 서비스를 사용할 수 있어야 합니다. 이러한 요구 사항을 충족하기 위해 일관된 하이브리드 환경을 제공하는 솔루션은 무엇입니까? A. 모든 애플리케이션을 규정을 준수하는 가장 가까운 AWS 리전으로 마이그레이션합니다. 중앙 온프레미스 데이터 센터와 AWS 간에 AWS Direct Connect 연결을 설정합니다. Direct Connect 게이트웨이를 배포합니다. B. 데이터 규제 요구 사항 또는 한 자릿수 밀리초의 지연 시간 요구 사항이 있는 애플리케이션에는 AWS Snowball Edge Storage Optimized 장치를 사용하십시오. 장치를 온프레미스에 보관하십시오. AWS Wavelength를 배포하여 공장 사이트에서 워크로드를 호스팅합니다. C. 데이터 규제 요구 사항 또는 한 자릿수 밀리초의 지연 시간 요구 사항이 있는 애플리케이션을 위해 AWS Outposts를 설치합니다. AWS Snowball Edge Compute Optimized 디바이스를 사용하여 공장 사이트에서 워크로드를 호스팅합니다. D. 데이터 규제 요구 사항 또는 한 자릿수 밀리초의 지연 시간 요구 사항이 있는 애플리케이션을 AWS 로컬 영역으로 마이그레이션합니다. AWS Wavelength를 배포하여 공장 사이트에서 워크로드를 호스팅합니다. Answer: C Explanation: Installing AWS Outposts for the applications that have data regulatory requirements or requirements for latency of single-digit milliseconds will provide a fully managed service that extends AWS infrastructure, services, APIs, and tools to customer premises1. AWS Outposts allows customers to run some AWS services locally and connect to a broad range of services available in the local AWS Region1. Using AWS Snowball Edge Compute Optimized devices to host the workloads in the factory sites will provide local compute and storage resources for locations with limited network infrastructure2. AWS Snowball Edge devices can run Amazon EC2 instances and AWS Lambda functions locally and sync data with AWS when network connectivity is available2.
QUESTION NO: 135 회사는 VPC의 AWS에서 이미지 처리 서비스를 호스팅하고 있습니다. VPC는 두 가용 영역에 걸쳐 확장됩니다. 각 가용 영역에는 퍼블릭 서브넷 1개와 프라이빗 서브넷 1개가 포함됩니다. 이 서비스는 프라이빗 서브넷의 Amazon EC2 인스턴스에서 실행됩니다. 퍼블릭 서브넷의 Application Load Balancer는 서비스 앞에 있습니다. 서비스는 인터넷과 통신해야 하며 두 개의 NAT 게이트웨이를 통해 통신합니다. 이 서비스는 이미지 저장을 위해 Amazon S3를 사용합니다. EC2 인스턴스는 매일 S3 버킷에서 약 1TB의 데이터를 검색합니다. 회사는 서비스를 매우 안전하다고 홍보했습니다. 솔루션 설계자는 서비스의 보안 태세를 손상시키거나 진행 중인 작업에 소요되는 시간을 늘리지 않으면서 클라우드 지출을 최대한 줄여야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. NAT 게이트웨이를 NAT 인스턴스로 교체합니다. VPC 경로 테이블에서 프라이빗 서브넷에서 NAT 인스턴스로의 경로를 만듭니다. B. EC2 인스턴스를 퍼블릭 서브넷으로 이동합니다. NAT 게이트웨이를 제거합니다. C. VPC에서 S3 게이트웨이 VPC 엔드포인트를 설정합니다. 엔드포인트에 엔드포인트 정책을 연결하여 S3 버킷에서 필요한 작업을 허용합니다. D. Amazon Elastic File System(Amazon EFS) 볼륨을 EC2 인스턴스에 연결합니다. EFS 볼륨에서 이미지를 호스팅합니다. Answer: C Explanation: Create Amazon S3 gateway endpoint in the VPC and add a VPC endpoint policy. This VPC endpoint policy will have a statement that allows S3 access only via access points owned by the organization.
QUESTION NO: 136 회사에서 각 엔지니어링 팀을 위해 AWS Organizations에 OU를 생성했습니다. 각 OU는 여러 AWS 계정을 소유합니다. 조직에는 수백 개의 AWS 계정이 있습니다. 솔루션 설계자는 각 OU가 AWS 계정 전체의 사용 비용 분석을 볼 수 있도록 솔루션을 설계해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS Resource Access Manager를 사용하여 각 OU에 대한 AWS 비용 및 사용 보고서(CUR)를 생성합니다. 각 팀이 Amazon QuickSight 대시보드를 통해 CUR을 시각화할 수 있도록 합니다. B. AWS Organizations 관리 계정에서 AWS 비용 및 사용 보고서(CUR) 생성 - 각 팀이 Amazon QuickSight 대시보드를 통해 CUR을 시각화하도록 허용 C. 각 AWS Organizations 멤버 계정에서 AWS 비용 및 사용 보고서(CUR) 생성 각 팀이 Amazon QuickSight 대시보드를 통해 CUR을 시각화하도록 허용합니다. D. AWS Systems Manager를 사용하여 AWS 비용 및 사용 보고서(CUR) 생성 각 팀이 Systems Manager OpsCenter 대시보드를 통해 CUR을 시각화하도록 허용 Answer: B Explanation: https://docs.aws.amazon.com/cur/latest/userguide/billing-cur-limits.html
QUESTION NO: 137 회사에는 AWS Organizations에 많은 AWS 계정이 있는 조직이 있습니다. 솔루션 설계자는 회사가 조직의 AWS 계정에 대한 공통 보안 그룹 규칙을 관리하는 방법을 개선해야 합니다. 회사는 회사의 온프레미스 네트워크에 대한 액세스를 허용하기 위해 각 AWS 계정의 허용 목록에 공통 IP CIDR 범위 세트를 가지고 있습니다. 각 계정 내의 개발자는 보안 그룹에 새 IP CIDR 범위를 추가할 책임이 있습니다. 보안 팀에는 자체 AWS 계정이 있습니다. 현재 보안 팀은 허용 목록이 변경되면 다른 AWS 계정의 소유자에게 알립니다. 솔루션 설계자는 CIDR 범위의 공통 집합을 모든 계정에 배포하는 솔루션을 설계해야 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 보안 팀의 AWS 계정에서 Amazon Simple Notification Service(Amazon SNS) 주제를 설정합니다. 각 AWS 계정에 AWS Lambda 함수를 배포합니다. SNS 주제가 메시지를 수신할 때마다 실행되도록 Lambda 함수를 구성합니다. IP 주소를 입력으로 받아 계정의 보안 그룹 목록에 추가하도록 Lambda 함수를 구성합니다. SNS 주제에 메시지를 게시하여 변경 사항을 배포하도록 보안 팀에 지시합니다. B. 조직 내의 각 AWS 계정에 새로운 고객 관리 접두사 목록을 만듭니다. 모든 내부 CIDR 범위로 각 계정의 접두사 목록을 채웁니다. 보안 그룹의 계정에서 새로운 고객 관리형 접두사 목록 ID를 허용하도록 각 AWS 계정의 소유자에게 알립니다. 각 AWS 계정 소유자와 업데이트를 공유하도록 보안 팀에 지시합니다. C. 보안 팀의 AWS 계정에 새로운 고객 관리 접두사 목록을 생성합니다. 모든 내부 CIDR 범위로 고객 관리 접두사 목록을 채웁니다. AWS Resource Access Manager를 사용하여 고객 관리 접두사 목록을 조직과 공유하십시오. 보안 그룹에서 새로운 고객 관리형 접두사 목록 ID를 허용하도록 각 AWS 계정의 소유자에게 알립니다. D. 조직의 각 계정에서 IAM 역할을 생성합니다. 보안 그룹을 업데이트할 수 있는 권한을 부여합니다. 보안 팀의 AWS 계정에 AWS Lambda 함수를 배포합니다. 내부 IP 주소 목록을 입력으로 사용하고, 각 조직 계정에서 역할을 수임하고, 각 계정의 보안 그룹에 IP 주소 목록을 추가하도록 Lambda 함수를 구성합니다. Answer: C Explanation: Create a new customer-managed prefix list in the security team's AWS account. Populate the customer-managed prefix list with all internal CIDR ranges. Share the customer-managed prefix list with the organization by using AWS Resource Access Manager. Notify the owner of each AWS account to allow the new customer-managed prefix list ID in their security groups. This solution meets the requirements with the least amount of operational overhead as it requires the security team to create and maintain a single customer-managed prefix list, and share it with the organization using AWS Resource Access Manager. The owners of each AWS account are then responsible for allowing the prefix list in their security groups, which eliminates the need for the security team to manually notify each account owner when changes are made. This solution also eliminates the need for a separate AWS Lambda function in each account, reducing the overall complexity of the solution.
QUESTION NO: 138 회사에는 Amazon Elastic Kubernetes Service(Amazon EKS) 클러스터에서 여러 포드의 ReplicaSet으로 실행되는 애플리케이션이 있습니다. EKS 클러스터에는 여러 가용 영역에 노드가 있습니다. 응용 프로그램은 응용 프로그램의 실행 중인 모든 인스턴스에서 액세스할 수 있어야 하는 많은 작은 파일을 생성합니다. 회사는 파일을 백업하고 1년 동안 백업을 보관해야 합니다. 가장 빠른 스토리지 성능을 제공하면서 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Amazon Elastic File System(Amazon EFS) 파일 시스템과 EKS 클러스터의 노드를 포함하는 각 서브넷에 대한 탑재 대상을 생성합니다. 파일 시스템을 마운트하도록 ReplicaSet를 구성합니다. 파일 시스템에 파일을 저장하도록 애플리케이션에 지시합니다. 데이터 사본을 백업하고 1년 동안 보관하도록 AWS Backup을 구성합니다. B. Amazon Elastic Block Store(Amazon EBS) 볼륨을 생성합니다. EBS 다중 연결 기능을 활성화합니다. EBS 볼륨을 마운트하도록 ReplicaSet를 구성합니다. EBS 볼륨에 파일을 저장하도록 애플리케이션에 지시합니다. 데이터 사본을 백업하고 1년 동안 보관하도록 AWS Backup을 구성합니다. C. Amazon S3 버킷을 생성합니다. S3 버킷을 마운트하도록 ReplicaSet를 구성합니다. S3 버킷에 파일을 저장하도록 애플리케이션에 지시합니다. 데이터 사본을 유지하도록 S3 버전 관리를 구성합니다. 1년 후 객체를 삭제하도록 S3 수명 주기 정책을 구성합니다. D. 실행 중인 각 애플리케이션 포드에서 사용 가능한 스토리지를 사용하여 파일을 로컬에 저장하도록 ReplicaSet를 구성합니다. 타사 도구를 사용하여 1년 동안 EKS 클러스터를 백업합니다. Answer: A Explanation: In the past, EBS can be attached only to one ec2 instance but not anymore but there are limitations like - it works only on io1/io2 instance types and many others as described here. https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-volumes-multi.html EFS has shareable storage In terms of performance, Amazon EFS is optimized for workloads that require high levels of aggregate throughput and IOPS, whereas EBS is optimized for lowlatency, random access I/O operations. Amazon EFS is designed to scale throughput and capacity automatically as your storage needs grow, while EBS volumes can be resized on demand.
QUESTION NO: 139 솔루션 설계자는 회사에서 곧 출시할 새 애플리케이션의 데이터 저장 및 검색 아키텍처를 설계하고 있습니다. 이 애플리케이션은 전 세계 디바이스에서 분당 수백만 개의 작은 레코드를 수집하도록 설계되었습니다. 각 레코드는 크기가 4KB 미만이며 짧은 대기 시간으로 검색할 수 있는 내구성 있는 위치에 저장해야 합니다. 데이터는 일시적이며 회사는 데이터를 120일 동안만 저장해야 하며 그 이후에는 데이터를 삭제할 수 있습니다. 솔루션 설계자는 1년 동안 스토리지 요구 사항이 약 10-15TB가 될 것이라고 계산합니다. 가장 비용 효율적이고 설계 요구 사항을 충족하는 스토리지 전략은 무엇입니까? A. 각 수신 레코드를 Amazon S3 버킷에 단일 .csv 파일로 저장하여 인덱스 검색이 가능하도록 애플리케이션을 설계하십시오. 120일보다 오래된 데이터를 삭제하도록 수명 주기 정책을 구성합니다. B. 각 수신 레코드를 규모에 맞게 구성된 Amazon DynamoDB 테이블에 저장하도록 애플리케이션을 설계합니다. 120일보다 오래된 레코드를 삭제하도록 DynamoOB TTL(Time to Live) 기능을 구성합니다. C. 각 수신 레코드를 Amazon RDS MySQL 데이터베이스의 단일 테이블에 저장하도록 애플리케이션을 설계합니다. 쿼리를 실행하는 야간 cron 작업을 실행하여 120일보다 오래된 레코드를 삭제합니다. D. 수신 레코드를 Amazon S3 버킷에 쓰기 전에 일괄 처리하도록 애플리케이션을 설계합니다. 배치의 레코드 목록을 포함하도록 객체의 메타데이터를 업데이트하고 Amazon S3 메타데이터 검색 기능을 사용하여 데이터를 검색합니다. Answer: B a. Configure a lifecycle policy to delete the data after 120 days. Explanation: DynamoDB with TTL, cheaper for sustained throughput of small items + suited for fast retrievals. S3 cheaper for storage only, much higher costs with writes. RDS not designed for this use case.
QUESTION NO: 140 솔루션 설계자가 이벤트를 처리하는 솔루션을 설계하고 있습니다. 솔루션에는 솔루션이 수신하는 이벤트 수에 따라 확장 및 축소할 수 있는 기능이 있어야 합니다. 처리 오류가 발생하면 이벤트는 검토를 위해 별도의 대기열로 이동해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Amazon Simple Notification Service(Amazon SNS) 주제로 이벤트 세부 정보를 보냅니다. 이벤트를 처리할 SNS 주제에 대한 구독자로 AWS Lambda 함수를 구성합니다. 기능에 실패 시 대상을 추가합니다. Amazon Simple Queue Service(Amazon SQS) 대기열을 대상으로 설정합니다. B. Amazon Simple Queue Service(Amazon SQS) 대기열에 이벤트를 게시합니다. Amazon EC2 Auto Scaling 그룹을 생성합니다. 대기열의 ApproximateAgeOfOldestMessage 지표를 기반으로 확장 및 축소하도록 Auto Scaling 그룹을 구성합니다. 배달 못한 편지 큐에 실패한 메시지를 쓰도록 애플리케이션을 구성합니다. C. Amazon DynamoDB 테이블에 이벤트를 씁니다. 테이블에 대한 DynamoDB 스트림을 구성합니다. AWS Lambda 함수를 호출하도록 스트림을 구성합니다. 이벤트를 처리하도록 Lambda 함수를 구성합니다. D. Amazon EventBridge 이벤트 버스에 이벤트를 게시합니다. Auto Scaling 그룹이 있는 Amazon EC2 인스턴스에서 애플리케이션 생성 및 실행 Answer: A behind an Application Load Balancer (ALB). Set the ALB as the event bus target. Configure the event bus to retry events. Write messages to a dead-letter queue if the application cannot process the messages. Explanation: Amazon Simple Notification Service (Amazon SNS) is a fully managed pub/sub messaging service that enables users to send messages to multiple subscribers1. Users can send event details to an Amazon SNS topic and configure an AWS Lambda function as a subscriber to the SNS topic to process the events. Lambda is a serverless compute service that runs code in response to events and automatically manages the underlying compute resources2. Users can add an on-failure destination to the function and set an Amazon Simple Queue Service (Amazon SQS) queue as the target. Amazon SQS is a fully managed message queuing service that enables users to decouple and scale microservices, distributed systems, and serverless applications3. This way, if a processing error occurs, the event will move into the separate queue for review. Option B is incorrect because publishing events to an Amazon SQS queue and creating an Amazon EC2 Auto Scaling group will not have the ability to scale in and out based on the number of events that the solution receives. Amazon EC2 is a web service that provides secure, resizable compute capacity in the cloud. Auto Scaling is a feature that helps users maintain application availability and allows them to scale their EC2 capacity up or down automatically according to conditions they define. However, for this use case, using SQS and EC2 will not take advantage of the serverless capabilities of Lambda and SNS. Option C is incorrect because writing events to an Amazon DynamoDB table and configuring a DynamoDB stream for the table will not have the ability to move events into a separate queue for review if a processing error occurs. Amazon DynamoDB is a fully managed keyvalue and document database that delivers single-digit millisecond performance at any scale. DynamoDB Streams is a feature that captures data modification events in DynamoDB tables. Users can configure the stream to invoke a Lambda function, but they cannot configure an on-failure destination for the function. Option D is incorrect because publishing events to an Amazon EventBridge event bus and setting an Application Load Balancer (ALB) as the event bus target will not have the ability to move events into a separate queue for review if a processing error occurs. Amazon EventBridge is a serverless event bus service that makes it easy to connect applications with data from a variety of sources. An ALB is a load balancer that distributes incoming application traffic across multiple targets, such as EC2 instances, containers, IP addresses, Lambda functions, and virtual appliances. Users can configure EventBridge to retry events, but they cannot configure an on-failure destination for the ALB.
QUESTION NO: 141 회사는 Auto Scaling 그룹에 있는 Amazon EC2 스팟 인스턴스를 사용하여 컴퓨팅 워크로드를 실행하고 있습니다. 시작 템플릿은 두 개의 배치 그룹과 단일 인스턴스 유형을 사용합니다. 최근 모니터링 시스템에서 시스템 사용자의 대기 시간이 길어지는 Auto Scaling 인스턴스 시작 실패를 보고했습니다. 회사는 워크로드의 전반적인 안정성을 개선해야 합니다. 이 요구 사항을 충족하는 솔루션은 무엇입니까? A. 시작 템플릿을 시작 구성으로 교체하여 속성 기반 인스턴스 유형 선택을 사용하는 Auto Scaling 그룹을 사용합니다. B. 속성 기반 인스턴스 유형 선택을 사용하는 새로운 시작 템플릿 버전을 생성합니다. 새 시작 템플릿 버전을 사용하도록 Auto Scaling 그룹을 구성합니다. C. 시작 템플릿 Auto Scaling 그룹을 업데이트하여 배치 그룹 수를 늘립니다. D. 더 큰 인스턴스 유형을 사용하도록 시작 템플릿을 업데이트합니다. Answer: C Explanation: https://docs.aws.amazon.com/autoscaling/ec2/userguide/create-asg-instance-typerequirements.html#use-attribute-based-instance-type-selection-prerequisites
QUESTION NO: 142 한 회사가 AWS에서 웹 애플리케이션을 호스팅할 계획이며 Amazon EC2 인스턴스 그룹 전체에서 트래픽 로드 밸런싱을 위해 노력하고 있습니다. 보안 요구 사항 중 하나는 클라이언트와 웹 서버 간에 전송되는 종단 간 암호화를 활성화하는 것입니다. 이 요구 사항을 충족하는 솔루션은 무엇입니까? A. ALB(Application Load Balancer) 뒤에 EC2 인스턴스 배치 AWS Certificate Manager(ACM)를 사용하여 SSL 인증서를 프로비저닝하고 SSL 인증서를 ALB와 연결합니다. SSL 인증서를 내보내고 각 EC2 인스턴스에 설치합니다. 포트 443에서 수신하고 인스턴스의 포트 443으로 트래픽을 전달하도록 ALB를 구성합니다. B. EC2 인스턴스를 대상 그룹과 연결합니다. AWS Certificate Manager(ACM)를 사용하여 SSL 인증서를 프로비저닝합니다. Amazon CloudFront 배포를 생성하고 SSL 인증서를 사용하도록 구성합니다. 대상 그룹을 오리진 서버로 사용하도록 CloudFront 설정 C. Application Load Balancer(ALB) 뒤에 EC2 인스턴스를 배치합니다. AWS Certificate Manager(ACM)를 사용하여 SSL 인증서를 프로비저닝하고 SSL 인증서를 ALB와 연결합니다. 타사 SSL 인증서를 프로비저닝하고 각 EC2 인스턴스에 설치합니다. 포트 443에서 수신하고 인스턴스의 포트 443으로 트래픽을 전달하도록 ALB를 구성합니다. D. NLB(Network Load Balancer) 뒤에 EC2 인스턴스를 배치합니다. 타사 SSL 인증서를 프로비저닝하고 NLB 및 각 EC2 인스턴스에 설치합니다. 포트 443에서 수신하고 인스턴스의 포트 443으로 트래픽을 전달하도록 NLB를 구성합니다. Answer: A Explanation: Option A is correct because placing the EC2 instances behind an Application Load Balancer (ALB) and associating an SSL certificate from AWS Certificate Manager (ACM) with the ALB enables encryption in transit between the client and the ALB. Exporting the SSL certificate and installing it on each EC2 instance enables encryption in transit between the ALB and the web server. Configuring the ALB to listen on port 443 and to forward traffic to port 443 on the instances ensures that HTTPS is used for both connections. This solution achieves end-toend encryption in transit for the web application12
QUESTION NO: 143 회사에는 Amazon EC2 인스턴스에서 실행되는 애플리케이션이 있습니다. 솔루션 설계자는 애플리케이션이 Amazon Aurora DB 클러스터에 액세스해야 하는 AWS 리전에서 VPC 인프라를 설계하고 있습니다. EC2 인스턴스는 모두 동일한 보안 그룹과 연결됩니다. DB 클러스터는 자체 보안 그룹과 연결됩니다. 솔루션 설계자는 애플리케이션에 DB 클러스터에 대한 최소 권한 액세스를 제공하기 위해 보안 그룹에 규칙을 추가해야 합니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개를 선택하세요.) A. EC2 인스턴스의 보안 그룹에 인바운드 규칙을 추가합니다. 기본 Aurora 포트를 통해 DB 클러스터의 보안 그룹을 소스로 지정합니다. B. EC2 인스턴스의 보안 그룹에 아웃바운드 규칙을 추가합니다. DB 클러스터의 보안 그룹을 기본 Aurora 포트를 통한 대상으로 지정합니다. C. DB 클러스터의 보안 그룹에 인바운드 규칙을 추가합니다. 기본 Aurora 포트를 통해 EC2 인스턴스의 보안 그룹을 소스로 지정합니다. D. DB 클러스터의 보안 그룹에 아웃바운드 규칙을 추가합니다. EC2 인스턴스의 보안 그룹을 기본 Aurora 포트를 통한 대상으로 지정합니다. E. DB 클러스터의 보안 그룹에 아웃바운드 규칙을 추가합니다. 임시 포트를 통한 대상으로 EC2 인스턴스의 보안 그룹을 지정합니다. Answer: A,B Explanation: 1. Add an outbound rule to the EC2 instances' security group. Specify the DB cluster's security group as the destination over the default Aurora port. This allows the instances to make outbound connections to the DB cluster on the default Aurora port. C. Add an inbound rule to the DB cluster's security group. Specify the EC2 instances' security group as the source over the default Aurora port. This allows connections to the DB cluster from the EC2 instances on the default Aurora port.
QUESTION NO: 144 회사는 AWS Organizations를 사용하여 여러 AWS 계정을 관리합니다. 루트 OU 아래. 회사에는 두 개의 OU(Research 및 DataOps)가 있습니다. 규정 요구 사항으로 인해 회사에서 조직에 배포하는 모든 리소스는 ap-northeast-1 리전에 있어야 합니다. 추가로. 회사가 DataOps OU에 배포하는 EC2 인스턴스는 미리 정의된 인스턴스 유형 목록을 사용해야 합니다. 솔루션 설계자는 이러한 제한 사항을 적용하는 솔루션을 구현해야 합니다. 솔루션은 운영 효율성을 극대화하고 지속적인 유지 관리를 최소화해야 합니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개 선택) A. DataOps OU 아래의 한 계정에서 IAM 역할을 생성합니다. 역할에 대한 인라인 정책의 ec2 인스턴스 유형 조건 키를 사용하여 특정 인스턴스 유형에 대한 액세스를 제한합니다. B. 루트 OU 아래의 모든 계정에서 IAM 사용자 생성 각 사용자에 대한 인라인 정책에서 aws RequestedRegion 조건 키를 사용하여 ap-northeast-1을 제외한 모든 AWS 리전에 대한 액세스를 제한합니다. C. SCP 생성 aws:RequestedRegion 조건 키를 사용하여 ap-northeast-1을 제외한 모든 AWS 리전에 대한 액세스를 제한합니다. 루트 OU에 SCP를 적용합니다. D. SCP 생성 ec2Region 조건 키를 사용하여 ap-northeast-1을 제외한 모든 AWS 지역에 대한 액세스를 제한합니다. 루트 OU에 SCP를 적용합니다. DataOps OU. 그리고 연구 OU. E. SCP 생성 ec2:lnstanceType 조건 키를 사용하여 특정 인스턴스 유형에 대한 액세스를 제한합니다. DataOps OU에 SCP를 적용합니다. Answer: C,E Explanation: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_deny -requested-region.html https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_policies_scps_ex amples_ec2.html
QUESTION NO: 145 회사에는 회사의 비즈니스에 중요한 레거시 모놀리식 애플리케이션이 있습니다. 회사는 Amazon Linux 2를 실행하는 Amazon EC2 인스턴스에서 애플리케이션을 호스팅합니다. 회사의 애플리케이션 팀은 법무 부서로부터 인스턴스의 암호화된 Amazon Elastic Block Store(Amazon EBS) 볼륨에서 Amazon S3 버킷으로 데이터를 백업하라는 지시를 받습니다. . 애플리케이션 팀에 인스턴스에 대한 관리 SSH 키 쌍이 없습니다. 애플리케이션은 계속해서 사용자에게 서비스를 제공해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Amazon S3에 쓰기 권한이 있는 인스턴스에 역할을 연결합니다. AWS Systems Manager Session Manager 옵션을 사용하여 인스턴스에 대한 액세스 권한을 얻고 Amazon S3에 데이터를 복사하는 명령을 실행합니다. B. 재부팅 옵션이 켜진 인스턴스의 이미지를 생성합니다. 이미지에서 새 EC2 인스턴스를 시작합니다. Amazon S3에 쓰기 권한이 있는 역할을 새 인스턴스에 연결합니다. Amazon S3에 데이터를 복사하는 명령을 실행합니다. C. Amazon Data Lifecycle Manager(Amazon DLM)를 사용하여 EBS 볼륨의 스냅샷을 만듭니다. 데이터를 Amazon S3에 복사합니다. D. 인스턴스의 이미지를 생성합니다. 이미지에서 새 EC2 인스턴스를 시작합니다. Amazon S3에 쓰기 권한이 있는 역할을 새 인스턴스에 연결합니다. Amazon S3에 데이터를 복사하는 명령을 실행합니다. Answer: C Explanation: Taking a snapshot of the EBS volume using Amazon Data Lifecycle Manager (DLM) will meet the requirements because it allows you to create a backup of the volume without the need to access the instance or its SSH key pair. Additionally, DLM allows you to schedule the backups to occur at specific intervals and also enables you to copy the snapshots to an S3 bucket. This approach will not impact the running application as the backup is performed on the EBS volume level.
QUESTION NO: 146 회사는 eu-west-1 리전에서 애플리케이션을 실행하고 각 환경 개발, 테스트 및 프로덕션에 대해 하나의 계정을 가지고 있습니다. 모든 환경은 상태 저장 Amazon EC2 인스턴스 및 Amazon RDS를 사용하여 연중무휴로 실행됩니다. for MySQL 데이터베이스 데이터베이스 크기는 500GB에서 800GB 사이입니다. 개발 팀과 테스트 팀은 업무 시간 중에 업무일에 작업하지만 프로덕션 환경은 하루 24시간 운영됩니다. 주 7일. 회사는 비용 절감을 원합니다. AH 리소스에는 개발, 테스트 또는 프로덕션을 키로 하는 환경 태그가 지정됩니다. 최소한의 운영 노력으로 비용을 절감하기 위해 솔루션 설계자는 무엇을 해야 합니까? A. 매일 저녁에 실행되는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. 태그를 기반으로 인스턴스를 종료하는 AWS Lambda 함수를 호출하도록 규칙을 구성합니다. 매일 실행되는 두 번째 EventBridge(CloudWatch Events) 규칙을 생성합니다. 영업일 오전 태그를 기반으로 마지막 백업에서 인스턴스를 복원하는 다른 Lambda 함수를 호출하도록 두 번째 규칙을 구성합니다. B. 매일 저녁에 실행되는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 만듭니다. 태그를 기반으로 인스턴스를 중지하는 AWS Lambda 함수를 호출하도록 규칙을 구성합니다. 매일 아침에 실행되는 두 번째 EventBridge(CloudWatch 이벤트) 규칙을 생성합니다. 태그를 기반으로 인스턴스를 시작하는 다른 Lambda 함수를 호출하도록 두 번째 규칙을 구성합니다. C. 매일 한 번 실행되는 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. 태그 요일 및 시간에 따라 인스턴스를 시작하거나 중지하는 하나의 AWS Lambda 함수를 호출하도록 규칙을 구성합니다. D. 매시간 실행되는 Amazon EventBridge 규칙을 생성합니다. 태그를 기반으로 마지막 백업에서 인스턴스를 종료하거나 복원하는 하나의 AWS Lambda 함수를 호출하도록 규칙을 구성합니다. 요일, 시간. Answer: B Explanation: Creating an Amazon EventBridge rule that runs every business day in the evening to stop instances and another rule that runs every business day in the morning to start instances based on the tag will reduce costs with the least operational effort. This approach allows for instances to be stopped during non-business hours when they are not in use, reducing the costs associated with running them. It also allows for instances to be started again in the morning when the development and testing teams need to use them.
QUESTION NO: 147 회사는 온프레미스 데이터 분석 플랫폼을 사용합니다. 이 시스템은 회사 데이터 센터의 12개 서버에 걸쳐 완전히 중복된 구성으로 가용성이 높습니다. 시스템은 사용자의 일회성 요청 외에도 매시간 및 매일 예약된 작업을 실행합니다. 예약된 작업은 실행을 완료하는 데 20분에서 2시간이 소요될 수 있으며 엄격한 SLA가 있습니다. 예약된 작업은 시스템 사용량의 65%를 차지합니다. 사용자 작업은 일반적으로 5분 이내에 실행이 완료되며 SLA가 없습니다. 사용자 작업은 시스템 사용량의 35%를 차지합니다. 시스템 오류가 발생하는 동안 예약된 작업은 계속해서 SLA를 충족해야 합니다. 그러나 사용자 작업이 지연될 수 있습니다. 솔루션 설계자는 시스템을 Amazon EC2 인스턴스로 이동하고 소비 기반 모델을 채택하여 장기 약정 없이 비용을 절감해야 합니다. 솔루션은 고가용성을 유지해야 하며 SLA에 영향을 미치지 않아야 합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까? A. 선택한 AWS 리전에서 2개의 가용 영역에 걸쳐 12개의 인스턴스를 분할합니다. 용량 예약이 있는 온디맨드 인스턴스로 각 가용 영역에서 두 개의 인스턴스를 실행합니다. 각 가용 영역에서 4개의 인스턴스를 스팟 인스턴스로 실행합니다. B. 선택한 AWS 리전에서 3개의 가용 영역에 걸쳐 12개의 인스턴스를 분할합니다. 가용 영역 중 하나에서 4개의 인스턴스를 모두 용량 예약이 있는 온디맨드 인스턴스로 실행합니다. 나머지 인스턴스를 스팟 인스턴스로 실행합니다. C. 선택한 AWS 리전에서 3개의 가용 영역에 걸쳐 12개의 인스턴스를 분할합니다. Savings Plan이 있는 온디맨드 인스턴스로 각 가용 영역에서 두 개의 인스턴스를 실행합니다. 각 가용 영역에서 두 개의 인스턴스를 스팟 인스턴스로 실행합니다. D. 선택한 AWS 리전에서 3개의 가용 영역에 걸쳐 12개의 인스턴스를 분할합니다. 각 가용 영역에서 용량 예약이 있는 온디맨드 인스턴스로 3개의 인스턴스를 실행합니다. 각 가용 영역에서 하나의 인스턴스를 스팟 인스턴스로 실행합니다. Answer: D Explanation: By splitting the 12 instances across three Availability Zones, the system can maintain high availability and availability of resources in case of a failure. Option D also uses a combination of On-Demand Instances with Capacity Reservations and Spot Instances, which allows for scheduled jobs to be run on the On-Demand instances with guaranteed capacity, while also taking advantage of the cost savings from Spot Instances for the user jobs which have lower SLA requirements.
QUESTION NO: 148 한 회사에서 AWS CloudFormation 스택에 배포된 AWS Lambda를 기반으로 애플리케이션을 구축했습니다. 웹 애플리케이션의 마지막 프로덕션 릴리스에서 몇 분 동안 중단되는 문제가 발생했습니다. 솔루션 설계자는 카나리아 릴리스를 지원하도록 배포 프로세스를 조정해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 새로 배포된 Lambda 함수 버전마다 별칭을 만듭니다. 라우팅 구성 파라미터와 함께 AWS CLI update-alias 명령을 사용하여 로드를 분산합니다. B. 애플리케이션을 새 CloudFormation 스택에 배포합니다. Amazon Route 53 가중 라우팅 정책을 사용하여 로드를 분산합니다. C. 새로 배포된 모든 Lambda 함수에 대한 버전을 생성합니다. 라우팅 구성 파라미터와 함께 AWS CLI update-function-configuration 명령을 사용하여 로드를 분산합니다. D. AWS CodeDeploy를 구성하고 배포 구성에서 CodeDeployDefault.OneAtATime을 사용하여 로드를 분산합니다. Answer: A Explanation: https://aws.amazon.com/blogs/compute/implementing-canary-deployments-of-aws-lambdafunctions-with-alias-traffic-shifting/ https://docs.aws.amazon.com/lambda/latest/dg/configuration-aliases.html
QUESTION NO: 149 회사는 Windows 파일 서버에 온프레미스로 데이터를 저장하고 있습니다. 회사는 매일 5GB의 새로운 데이터를 생성합니다. 이 회사는 Windows 기반 워크로드의 일부를 AWS로 마이그레이션했으며 클라우드의 파일 시스템에서 데이터를 사용할 수 있어야 합니다. 회사는 이미 온프레미스 네트워크와 AWS 간에 AWS Direct Connect 연결을 설정했습니다. 회사는 어떤 데이터 마이그레이션 전략을 사용해야 합니까? A. AWS Storage Gateway의 파일 게이트웨이 옵션을 사용하여 기존 Windows 파일 서버를 교체하고 기존 파일 공유가 새 파일 게이트웨이를 가리키도록 합니다. B. AWS DataSync를 사용하여 온프레미스 Windows 파일 서버와 Amazon FSx 간에 데이터를 복제하는 일일 작업을 예약합니다. C. AWS Data Pipeline을 사용하여 온프레미스 Windows 파일 서버와 Amazon Elastic File System(Amazon EFS) 간에 데이터를 복제하는 일일 작업을 예약합니다. D. AWS DataSync를 사용하여 온프레미스 Windows 파일 서버와 Amazon Elastic File System(Amazon EFS) 간에 데이터를 복제하는 일일 작업을 예약합니다. Answer: B Explanation: https://aws.amazon.com/storagegateway/file/ https://docs.aws.amazon.com/fsx/latest/WindowsGuide/migrate-files-to-fsx-datasync.html https://docs.aws.amazon.com/systems-manager/latest/userguide/prereqs-operatingsystems.html#prereqs-os-windows-server
QUESTION NO: 150 회사에서 타사 SaaS(Software-as-a-Service) 애플리케이션을 사용하려고 합니다. 타사 SaaS 애플리케이션은 여러 API 호출을 통해 사용됩니다. 타사 SaaS 애플리케이션도 VPC 내부의 AWS에서 실행됩니다. 회사는 VPC 내부에서 타사 SaaS 애플리케이션을 사용합니다. 회사에는 인터넷을 통과하지 않는 개인 연결 사용을 의무화하는 내부 보안 정책이 있습니다. 회사 VPC에서 실행되는 리소스는 회사 VPC 외부에서 액세스할 수 없습니다. 모든 권한은 최소 권한 원칙을 준수해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS PrivateLink 인터페이스 VPC 엔드포인트를 생성합니다. 타사 SaaS 애플리케이션이 제공하는 엔드포인트 서비스에 이 엔드포인트를 연결합니다. 엔드포인트에 대한 액세스를 제한하는 보안 그룹을 생성합니다. 보안 그룹을 엔드포인트와 연결합니다. B. 타사 SaaS 애플리케이션과 회사 VPC 간에 AWS Site-to-Site VPN 연결을 생성합니다. VPN 터널을 통한 액세스를 제한하도록 네트워크 ACL을 구성합니다. C. 피어링 연결에 필요한 경로를 추가하여 타사 SaaS 애플리케이션과 회사 VPUpdate 경로 테이블 간에 VPC 피어링 연결을 생성합니다. D. AWS PrivateLink 엔드포인트 서비스를 생성합니다. 타사 SaaS 공급자에게 이 엔드포인트 서비스에 대한 인터페이스 VPC 엔드포인트를 생성하도록 요청하십시오. 타사 SaaS 공급자의 특정 계정에 끝점 서비스에 대한 권한을 부여합니다. Answer: A Explanation: Reference architecture - https://docs.aws.amazon.com/vpc/latest/privatelink/privatelink -access-saas.html Note from documentation that Interface Endpoint is at client side
QUESTION NO: 151 한 회사가 AWS 클라우드에서 온프레미스 주문 처리 플랫폼을 리팩토링하고 있습니다. 이 플랫폼에는 프런트 엔드를 백엔드에 연결하기 위해 일련의 VM RabbitMQ에서 호스팅되는 웹 프런트 엔드와 컨테이너화된 백엔드 시스템을 실행하여 주문을 처리하는 Kubernetes 클러스터가 포함됩니다. 회사는 애플리케이션을 크게 변경하고 싶지 않습니다. 운영 오버헤드를 최소화하면서 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 웹 서버 VM의 AMI 생성 AMI 및 Application Load Balancer를 사용하는 Amazon EC2 Auto Scaling 그룹 생성 Amazon MQ를 설정하여 온프레미스 메시징 대기열을 교체 Amazon Elastic Kubernetes Service(Amazon EKS) 구성 주문 처리 백엔드 호스팅 B. 웹 서버 환경을 모방하는 사용자 지정 AWS Lambda 런타임 생성 프런트 엔드 웹 서버를 대체할 Amazon API Gateway API 생성 온프레미스 메시징 대기열을 대체하도록 Amazon MQ 설정 Amazon Elastic Kubernetes Service(Amazon EKS) 구성 ) 주문 처리 백엔드 호스팅 C. 웹 서버 VM의 AMI 생성 AMI 및 Application Load Balancer를 사용하는 Amazon EC2 Auto Scaling 그룹 생성 Amazon MQ를 설정하여 온프레미스 메시징 대기열을 대체 주문 처리 백엔드 호스팅 D. 웹 서버 VM의 AMI 생성 AMI 및 Application Load Balancer를 사용하는 Amazon EC2 Auto Scaling 그룹 생성 온프레미스 메시징 대기열을 대체할 Amazon Simple Queue Service(Amazon SQS) 대기열 설정 Amazon 구성 주문 처리 백엔드를 호스팅하는 Elastic Kubernetes Service(Amazon EKS) Answer: A Explanation: https://aws.amazon.com/about-aws/whats-new/2020/11/announcing-amazon-mq-rabbitmq/
QUESTION NO: 152 회사는 온프레미스 데이터 센터에서 Git 리포지토리를 호스팅합니다. 이 회사는 웹후크를 사용하여 AWS 클라우드에서 실행되는 기능을 호출합니다. 회사는 Application Load Balancer(ALB)의 대상으로 설정한 Auto Scaling 그룹의 Amazon EC2 인스턴스 세트에서 웹후크 로직을 호스팅합니다. Git 서버는 구성된 웹후크에 대해 ALB를 호출합니다. 회사는 솔루션을 서버리스 아키텍처로 이동하려고 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 각 Webhook에 대해 AWS Lambda 함수 URL을 생성하고 구성합니다. 개별 Lambda 함수 URL을 호출하도록 Git 서버를 업데이트합니다. B. Amazon API Gateway HTTP API를 생성합니다. 별도의 AWS Lambda 함수에서 각 Webhook 로직을 구현합니다. API Gateway 엔드포인트를 호출하도록 Git 서버를 업데이트합니다. C. Webhook 로직을 AWS App Runner에 배포합니다. ALB를 생성하고 App Runner를 대상으로 설정합니다. ALB 엔드포인트를 호출하도록 Git 서버를 업데이트합니다. D. 웹후크 로직을 컨테이너화합니다. Amazon Elastic Container Service(Amazon ECS) 클러스터를 생성하고 AWS Fargate에서 웹후크 로직을 실행합니다. Amazon API Gateway REST API를 생성하고 Fargate를 대상으로 설정합니다. API Gateway 엔드포인트를 호출하도록 Git 서버를 업데이트합니다. Answer: B Explanation: https://aws.amazon.com/solutions/implementations/git-to-s3-using-webhooks/ https://medium.com/mindorks/building-webhook-is-easy-using-aws-lambda-and-api-gateway-56f5e5c3a596
QUESTION NO: 153 회사는 월별 AWS 청구서에서 각 애플리케이션 또는 팀에 귀속되는 비용을 결정해야 합니다. 회사는 또한 지난 12개월의 비용을 비교하고 향후 12개월의 비용을 예측하는 데 도움이 되는 보고서를 생성할 수 있어야 합니다. 솔루션 설계자는 이러한 비용 보고서를 제공하는 AWS Billing and Cost Management 솔루션을 추천해야 합니다. 이러한 요구 사항을 충족하는 작업 조합은 무엇입니까? (3개를 선택하세요.) A. 애플리케이션과 팀을 나타내는 사용자 정의 비용 할당 태그를 활성화합니다. B. 애플리케이션과 팀을 나타내는 AWS 생성 비용 할당 태그를 활성화합니다. C. Billing and Cost Management에서 각 애플리케이션에 대한 비용 범주를 생성합니다. D. Billing and Cost Management에 대한 IAM 액세스를 활성화합니다. E. 비용 예산을 만듭니다. F. 비용 탐색기를 활성화합니다. Answer: A,C,F Explanation: https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-cost-categories.html https://aws.amazon.com/premiumsupport/knowledge-center/cost-explorer-analyze-spendingand-usage/ https://docs.aws.amazon.com/awsaccountbilling/latest/aboutv2/manage-cost-categories.html https://docs.aws.amazon.com/cost-management/latest/userguide/ce-enable.html The best combination of actions to meet the company's requirements is Options A, C, and F. Option A involves activating the user-defined cost allocation tags that represent the application and the team. This will allow the company to assign costs to different applications or teams, and will allow them to be tracked in the monthly AWS bill. Option C involves creating a cost category for each application in Billing and Cost Management. This will allow the company to easily identify and compare costs across different applications and teams. Option F involves enabling Cost Explorer. This will allow the company to view the costs of their AWS resources over the last 12 months and to create forecasts for the next 12 months. These recommendations are in line with the official Amazon Textbook and Resources for the AWS Certified Solutions Architect - Professional certification. In particular, the book states that "You can use cost allocation tags to group your costs by application, team, or other categories" (Source: https://d1.awsstatic.com/training-and-certification/docs-sa-pro/AWS_Certified_Solutions_Architect_Professional_Exam_Guide_EN_v1.5.pdf). Additionally, the book states that "Cost Explorer enables you to view the costs of your AWS resources over the last 12 months and to create forecasts for the next 12 months" (Source: https://d1.awsstatic.com/training-and-certification/docs-sapro/ AWS_Certified_Solutions_Architect_Professional_Exam_Guide_EN_v1.5.pdf).
QUESTION NO: 154 회사는 재무 정보에 대한 규제 감사를 받습니다. 단일 AWS 계정을 사용하는 외부 감사자는 회사의 AWS 계정에 액세스해야 합니다. 솔루션 설계자는 감사자에게 회사의 AWS 계정에 대한 안전한 읽기 전용 액세스 권한을 제공해야 합니다. 솔루션은 AWS 보안 모범 사례를 준수해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 회사의 AWS 계정에서 계정의 모든 리소스에 대한 리소스 정책을 생성하여 감사자의 AWS 계정에 대한 액세스 권한을 부여합니다. 리소스 정책에 고유한 외부 ID를 할당합니다. B. 회사의 AWS 계정에서 감사자의 AWS 계정을 신뢰하는 IAM 역할을 생성합니다. 필요한 권한이 있는 IAM 정책을 생성합니다. 정책을 역할에 연결합니다. 역할의 신뢰 정책에 고유한 외부 ID를 할당합니다. C. 회사의 AWS 계정에서 IAM 사용자를 생성합니다. 필요한 IAM 정책을 IAM 사용자에게 연결합니다. IAM 사용자에 대한 API 액세스 키를 생성합니다. 감사자와 액세스 키를 공유합니다. D. 회사의 AWS 계정에서 필요한 권한을 가진 下午 7:08 22/1/2023】、8 IAM 그룹을 생성합니다. 회사 계정에서 각 감사자에 대한 IAM 사용자를 생성합니다. IAM 그룹에 IAM 사용자를 추가합니다. Answer: B Explanation: This solution will allow the external auditors to have read-only access to the company's AWS account while being compliant with AWS security best practices. By creating an IAM role, which is a secure and flexible way of granting access to AWS resources, and trusting the auditors' AWS account, the company can ensure that the auditors only have the permissions that are required for their role and nothing more. Assigning a unique external ID to the role's trust policy, it will ensure that only the auditors' AWS account can assume the role. Reference: AWS IAM Roles documentation: https://aws.amazon.com/iam/features/roles/ AWS IAM Best practices: https://aws.amazon.com/iam/security-best-practices/
QUESTION NO: 155 회사는 AWS 클라우드에서 다중 계정 설정을 위해 AWS Organizations를 사용합니다. 이 회사는 거버넌스를 위해 AWS Control Tower를 사용하고 계정 간 VPC 연결을 위해 AWS Transit Gateway를 사용합니다. AWS 애플리케이션 계정에서 회사의 애플리케이션 팀은 AWS Lambda 및 Amazon RDS를 사용하는 웹 애플리케이션을 배포했습니다. 회사의 데이터베이스 관리자는 별도의 DBA 계정을 가지고 있으며 이 계정을 사용하여 조직 전체의 모든 데이터베이스를 중앙에서 관리합니다. 데이터베이스 관리자는 DBA 계정에 배포된 Amazon EC2 인스턴스를 사용하여 애플리케이션 계정에 배포된 RDS 데이터베이스에 액세스합니다. 애플리케이션 팀은 데이터베이스 자격 증명을 애플리케이션 계정의 AWS Secrets Manager에 비밀로 저장했습니다. 애플리케이션 팀은 비밀을 데이터베이스 관리자와 수동으로 공유하고 있습니다. 비밀은 애플리케이션 계정의 Secrets Manager에 대한 기본 AWS 관리형 키로 암호화됩니다. 솔루션 설계자는 데이터베이스 관리자에게 데이터베이스에 대한 액세스 권한을 부여하고 암호를 수동으로 공유할 필요가 없도록 하는 솔루션을 구현해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS Resource Access Manager(AWS RAM)를 사용하여 애플리케이션 계정의 암호를 DBA 계정과 공유합니다. DBA 계정에서 DBA-Admin이라는 IAM 역할을 생성합니다. 공유 암호에 액세스하는 데 필요한 권한을 역할에 부여합니다. 교차 계정 암호에 액세스하려면 DBA-Admin 역할을 EC2 인스턴스에 연결합니다. B. 애플리케이션 계정에서 DBA-Secret이라는 IAM 역할을 생성합니다. 암호에 액세스하는 데 필요한 권한을 역할에 부여합니다. DBA 계정에서 DBA-Admin이라는 IAM 역할을 생성합니다. 애플리케이션 계정에서 DBA-Secret 역할을 맡는 데 필요한 권한을 DBA-Admin 역할에 부여합니다. 교차 계정 암호에 액세스하려면 DBA-Admin 역할을 EC2 인스턴스에 연결합니다. C. DBA 계정에서 DBA-Admin이라는 IAM 역할을 생성합니다. 애플리케이션 계정의 암호 및 기본 AWS 관리형 키에 액세스하는 데 필요한 권한을 역할에 부여합니다. 애플리케이션 계정에서 리소스 기반 정책을 키에 연결하여 DBA 계정의 액세스를 허용합니다. 교차 계정 암호에 액세스하려면 DBA-Admin 역할을 EC2 인스턴스에 연결합니다. D. DBA 계정에서 DBA-Admin이라는 IAM 역할을 생성합니다. 애플리케이션 계정의 암호에 액세스하는 데 필요한 권한을 역할에 부여합니다. DBA 계정에서 암호에 액세스할 수 있도록 애플리케이션 계정에 SCP를 연결합니다. 교차 계정 암호에 액세스하려면 DBA-Admin 역할을 EC2 인스턴스에 연결합니다. Answer: B Explanation: Option B is correct because creating an IAM role in the application account that has permissions to access the secrets and creating an IAM role in the DBA account that has permissions to assume the role in the application account eliminates the need to manually share the secrets. This approach uses cross-account IAM roles to grant access to the secrets in the application account. The database administrators can assume the role in the application account from their EC2 instance in the DBA account and retrieve the secrets without having to store them locally or share them manually2
QUESTION NO: 156 한 회사가 VPC의 퍼블릭 서브넷에 있는 5개의 Amazon EC2 인스턴스에서 모바일 앱용 모놀리식 REST 기반 API를 호스팅하고 있습니다. 모바일 클라이언트는 Amazon Route 53에서 호스팅되는 도메인 이름을 사용하여 API에 연결합니다. 회사는 모든 EC2 인스턴스의 IP 주소로 Route 53 다중값 응답 라우팅 정책을 생성했습니다. 최근 이 앱은 갑작스러운 대규모 트래픽 증가에 압도당했습니다. 앱이 트래픽을 따라잡지 못했습니다. 솔루션 설계자는 앱이 새롭고 다양한 로드를 처리할 수 있도록 솔루션을 구현해야 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. API를 개별 AWS Lambda 함수로 분리합니다. 백엔드에 대한 Lambda 통합으로 Amazon API Gateway REST API를 구성합니다. API Gateway API를 가리키도록 Route 53 레코드를 업데이트합니다. B. API 로직을 컨테이너화합니다. Amazon Elastic Kubernetes Service(Amazon EKS) 클러스터를 생성합니다. Amazon EC2를 사용하여 클러스터에서 컨테이너를 실행합니다. Kubernetes 인그레스를 만듭니다. Kubernetes 수신을 가리키도록 Route 53 레코드를 업데이트합니다. C. Auto Scaling 그룹을 생성합니다. Auto Scaling 그룹에 모든 EC2 인스턴스를 배치합니다. CPU 사용률을 기반으로 조정 작업을 수행하도록 Auto Scaling 그룹을 구성합니다. Auto Scaling 그룹 변경에 반응하고 Route 53 레코드를 업데이트하는 AWS Lambda 함수를 생성합니다. D. API 앞에 Application Load Balancer(ALB)를 생성합니다. EC2 인스턴스를 VPC의 프라이빗 서브넷으로 이동합니다. ALB의 대상으로 EC2 인스턴스를 추가합니다. ALB를 가리키도록 Route 53 레코드를 업데이트합니다. Answer: D Explanation: By breaking down the monolithic API into individual Lambda functions and using API Gateway to handle the incoming requests, the solution can automatically scale to handle the new and varying load without the need for manual scaling actions. Additionally, this option will automatically handle the traffic without the need of having EC2 instances running all the time and only pay for the number of requests and the duration of the execution of the Lambda function. By updating the Route 53 record to point to the API Gateway, the solution can handle the traffic and also it will direct the traffic to the correct endpoint.
QUESTION NO: 157 회사가 온프레미스에서 AWS로 애플리케이션을 마이그레이션했습니다. 애플리케이션 프런트엔드는 Application Load Balancer(ALB) 뒤에 있는 두 개의 Amazon EC2 인스턴스에서 실행되는 정적 웹 사이트입니다. 애플리케이션 백엔드는 다른 ALB 뒤에 있는 3개의 EC2 인스턴스에서 실행되는 Python 애플리케이션입니다. EC2 인스턴스는 애플리케이션의 최대 사용량에 대한 온프레미스 사양을 충족하도록 크기가 조정된 대형 범용 온디맨드 인스턴스입니다. 애플리케이션은 매월 평균 수십만 건의 요청을 처리합니다. 그러나 애플리케이션은 주로 점심 시간에 사용되며 나머지 시간에는 트래픽이 최소화됩니다. 솔루션 설계자는 애플리케이션 가용성에 부정적인 영향을 미치지 않으면서 애플리케이션의 인프라 비용을 최적화해야 합니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개 선택하세요.) A. 모든 EC2 인스턴스를 기존 EC2 인스턴스와 동일한 수의 코어가 있는 컴퓨팅 최적화 인스턴스로 변경합니다. B. 애플리케이션 프런트엔드를 Amazon S3에서 호스팅되는 정적 웹사이트로 이동합니다. C. AWS Elastic Beanstalk를 사용하여 애플리케이션 프런트엔드를 배포합니다. 노드에 대해 동일한 인스턴스 유형을 사용하십시오. D. 모든 백엔드 EC2 인스턴스를 스팟 인스턴스로 변경합니다. E. 백엔드 Python 애플리케이션을 기존 EC2 인스턴스와 동일한 수의 코어가 있는 버스트 가능한 범용 EC2 인스턴스에 배포합니다. Answer: B,D Explanation: Moving the application frontend to a static website that is hosted on Amazon S3 will save cost as S3 is cheaper than running EC2 instances. Using Spot instances for the backend EC2 instances will also save cost, as they are significantly cheaper than On-Demand instances. This will be suitable for the application, as it has minimal traffic during the rest of the day, and the availability of spot instances will not negatively affect the application's availability. Reference: Amazon S3 pricing: https://aws.amazon.com/s3/pricing/ Amazon EC2 Spot Instances documentation: https://aws.amazon.com/ec2/spot/ AWS Elastic Beanstalk documentation: https://aws.amazon.com/elasticbeanstalk/ Amazon Elastic Compute Cloud (EC2) pricing: https://aws.amazon.com/ec2/pricing/
QUESTION NO: 158 한 회사가 AWS에서 SaaS(Software-as-a-Service) 솔루션을 구축하고 있습니다. 이 회사는 여러 AWS 리전과 동일한 프로덕션 계정에 AWS Lambda 통합과 함께 Amazon API Gateway REST API를 배포했습니다. 이 회사는 고객이 초당 특정 수의 API 호출을 할 수 있는 용량에 대해 비용을 지불할 수 있는 계층형 가격을 제공합니다. 프리미엄 계층은 초당 최대 3,000개의 호출을 제공하며 고객은 고유한 API 키로 식별됩니다. 다양한 리전의 여러 프리미엄 계층 고객은 사용량이 가장 많은 시간 동안 여러 API 메서드에서 429개의 너무 많은 요청 오류 응답을 받았다고 보고합니다. 로그는 Lambda 함수가 호출되지 않았음을 나타냅니다. 이러한 고객에게 표시되는 오류 메시지의 원인은 무엇입니까? A. Lambda 함수가 동시성 제한에 도달했습니다. B. Lambda는 동시성에 대한 리전 제한 기능을 합니다. C. 회사가 API 게이트웨이 계정의 초당 호출 한도에 도달했습니다. D. 회사는 초당 호출에 대한 API 게이트웨이 기본 메서드별 제한에 도달했습니다. Answer: C Explanation: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-requestthrottling.html#apig-request-throttling-account-level-limits
QUESTION NO: 159 회사는 데이터 분석 환경을 온프레미스에서 AWS로 마이그레이션하려고 합니다. 환경은 두 개의 간단한 Node js 애플리케이션으로 구성됩니다. 애플리케이션 중 하나는 센서 데이터를 수집하여 MySQL 데이터베이스에 로드합니다. 다른 애플리케이션은 데이터를 보고서로 집계합니다. 집계 작업이 실행될 때 . 일부 로드 작업이 제대로 실행되지 않음 회사는 데이터 로드 문제를 해결해야 합니다. 회사는 또한 회사 고객을 위해 중단이나 변경 없이 마이그레이션이 수행되어야 합니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까? A. Amazon Aurora MySQL 데이터베이스를 온프레미스 데이터베이스의 복제 대상으로 설정 Aurora MySQL 데이터베이스에 대한 Aurora 복제본을 생성하고 집계 작업을 이동하여 Aurora 복제본에 대해 실행 수집 엔드포인트를 AWS Lambda 함수로 설정 NLB(Network Load Balancer) 뒤에 있습니다. 그리고 Amazon RDS 프록시를 사용하여 Aurora MySQL 데이터베이스에 연결합니다. 데이터베이스가 동기화되면 복제 작업을 비활성화하고 Aurora 복제본을 기본 인스턴스로 다시 시작합니다. 수집기 DNS 레코드가 NLB를 가리키도록 합니다. B. Amazon Aurora MySQL 데이터베이스 설정 AWS DMS(AWS Database Migration Service)를 사용하여 온프레미스 데이터베이스에서 Aurora로 연속 데이터 복제 수행 집계 작업을 이동하여 Aurora MySQL 데이터베이스에 대해 실행 Auto Scaling 그룹의 Amazon EC2 인스턴스인 Application Load Balancer(ALB) 데이터베이스가 동기화되면 수집기 DNS 레코드가 ALB를 가리키도록 합니다. 온프레미스에서 AWS로 컷오버한 후 AWS DMS 동기화 작업을 비활성화합니다. C. Amazon Aurora MySQL 데이터베이스 설정 AWS DMS(AWS Database Migration Service)를 사용하여 온프레미스 데이터베이스에서 Aurora로 연속 데이터 복제 수행 Aurora MySQL 데이터베이스용 Aurora 복제본 생성 및 실행할 집계 작업 이동 Aurora 복제본 수집 엔드포인트를 Application Load Balancer(ALB) 뒤에 있는 AWS Lambda 함수로 설정하고 Amazon RDS Proxy를 사용하여 Aurora MySQL 데이터베이스에 쓰기 데이터베이스가 동기화되면 수집기 DNS 레코드를 ALB로 지정 AWS DMS 동기화 비활성화 온프레미스에서 AWS로 컷오버 후 작업 D. Amazon Aurora MySQL 데이터베이스 설정 Aurora MySQL 데이터베이스용 Aurora 복제본 생성 및 집계 작업을 이동하여 Aurora 복제본에 대해 실행 컬렉션 엔드포인트를 Amazon Kinesis 데이터 스트림으로 설정 Amazon Kinesis Data Firehose를 사용하여 데이터 복제 데이터베이스가 동기화되면 복제 작업을 비활성화하고 Aurora 복제본을 기본 인스턴스로 다시 시작합니다. 수집기 DNS 레코드가 Kinesis 데이터 스트림을 가리키도록 합니다. Answer: C Explanation: Set up an Amazon Aurora MySQL database. Use AWS Database Migration Service (AWS DMS) to perform continuous data replication from the on-premises database to Aurora. Create an Aurora Replica for the Aurora MySQL database, and move the aggregation jobs to run against the Aurora Replica. Set up collection endpoints as AWS Lambda functions behind an Application Load Balancer (ALB), and use Amazon RDS Proxy to write to the Aurora MySQL database. When the databases are synced, point the collector DNS record to the ALB. Disable the AWS DMS sync task after the cutover from on premises to AWS. Amazon RDS Proxy allows applications to pool and share connections established with the database, improving database efficiency and application scalability. With RDS Proxy, failover times for Aurora and RDS databases are reduced by up to 66%
QUESTION NO: 160 솔루션 설계자가 Amazon RDS DB 인스턴스의 스냅샷을 만드는 회사의 프로세스를 검토하고 있습니다. 회사는 매일 자동 스냅샷을 찍고 7일 동안 스냅샷을 보관합니다. 솔루션 설계자는 6시간마다 스냅샷을 만들고 30일 동안 스냅샷을 유지하는 솔루션을 추천해야 합니다. 이 회사는 AWS Organizations를 사용하여 모든 AWS 계정을 관리합니다. 회사는 RDS 스냅샷의 상태에 대한 통합 보기가 필요합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS 백업에서 교차 계정 관리 기능을 켭니다. 빈도 및 보존 요구 사항을 지정하는 백업 계획을 만듭니다. DB 인스턴스에 태그를 추가합니다. 태그를 사용하여 백업 계획을 적용합니다. AWS Backup을 사용하여 백업 상태를 모니터링합니다. B. Amazon RDS에서 교차 계정 관리 기능을 켭니다. 빈도 및 보존 요구 사항을 지정하는 스냅샷 글로벌 정책을 만듭니다. 관리 계정의 RDS 콘솔을 사용하여 백업 상태를 모니터링합니다. C. AWS CloudFormation에서 교차 계정 관리 기능을 켭니다. 마스터 계정에서 빈도 및 보존 요구 사항을 지정하는 AWS Backup의 백업 계획이 포함된 CloudFormation 스택 세트를 배포합니다. 마스터 계정에서 AWS Lambda 함수를 생성하여 백업 상태를 모니터링합니다. 각 계정에서 Amazon EventBridge 규칙을 생성하여 일정에 따라 Lambda 함수를 실행합니다. D. 각 계정에서 AWS 백업을 구성합니다. 빈도 및 보존 요구 사항을 지정하는 Amazon Data Lifecycle Manager 수명 주기 정책을 생성합니다. 대상 리소스로 DB 인스턴스를 지정합니다. 각 멤버 계정에서 Amazon Data Lifecycle Manager 콘솔을 사용하여 백업 상태를 모니터링합니다. Answer: A Explanation: Turning on the cross-account management feature in AWS Backup will enable managing and monitoring backups across multiple AWS accounts that belong to the same organization in AWS Organizations1. Creating a backup plan that specifies the frequency and retention requirements will enable taking snapshots every 6 hours and retaining them for 30 days2. Adding a tag to the DB instances will enable applying the backup plan by using tags2. Using AWS Backup to monitor the status of the backups will enable having a consolidated view of the health of the RDS snapshots1.
QUESTION NO: 161 회사는 웹 호스팅, 데이터베이스 API 서비스 및 비즈니스 로직을 위해 현재 로드 밸런싱된 Amazon EC2 인스턴스 플릿이 있는 소매 주문 웹 애플리케이션을 리팩터링하려고 합니다. 회사는 실패한 주문을 유지하는 동시에 운영 비용을 최소화하는 메커니즘을 갖춘 분리되고 확장 가능한 아키텍처를 만들어야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 데이터베이스 API 서비스를 위해 Amazon API Gateway와 함께 웹 호스팅에 Amazon S3를 사용합니다. 주문 대기열에 Amazon Simple Queue Service(Amazon SQS)를 사용합니다. 실패한 주문을 유지하기 위해 Amazon SQS 긴 폴링과 함께 비즈니스 로직에 Amazon Elastic Container Service(Amazon ECS)를 사용합니다. B. 데이터베이스 API 서비스를 위해 Amazon API Gateway와 함께 웹 호스팅에 AWS Elastic Beanstalk를 사용합니다. 주문 대기열에 Amazon MQ를 사용하십시오. 실패한 주문을 유지하려면 Amazon S3 Glacier Deep Archive와 함께 비즈니스 로직에 AWS Step Functions를 사용하십시오. C. 데이터베이스 API 서비스를 위해 AWS AppSync와 함께 웹 호스팅에 Amazon S3를 사용합니다. 주문 대기열에 Amazon Simple Queue Service(Amazon SQS)를 사용합니다. 실패한 주문을 유지하기 위해 Amazon SQS 데드 레터 대기열과 함께 비즈니스 로직에 AWS Lambda를 사용합니다. D. 데이터베이스 API 서비스를 위해 AWS AppSync와 함께 웹 호스팅에 Amazon Lightsail을 사용합니다. 주문 대기열에 Amazon Simple Email Service(Amazon SES)를 사용합니다. 실패한 주문을 유지하기 위해 Amazon OpenSearch Service와 함께 비즈니스 로직에 Amazon Elastic Kubernetes Service(Amazon EKS)를 사용합니다. Answer: C Explanation: * Use Amazon S3 for web hosting with AWS AppSync for database API services. Use Amazon Simple Queue Service (Amazon SQS) for order queuing. Use AWS Lambda for business logic with an Amazon SQS dead-letter queue for retaining failed orders. This solution will allow you to: * Host a static website on Amazon S3 without provisioning or managing servers1. * Use AWS AppSync to create a scalable GraphQL API that connects to your database and other data sources1. * Use Amazon SQS to decouple and scale your order processing microservices1. * Use AWS Lambda to run code for your business logic without provisioning or managing servers1. * Use an Amazon SQS dead-letter queue to retain messages that can't be processed by your Lambda function1.
QUESTION NO: 162 회사는 VPC에 연결된 AWS Lambda 함수에서 실행되는 서버리스 애플리케이션을 구축하고 있습니다. 회사는 애플리케이션을 외부 공급자의 새로운 서비스와 통합해야 합니다. 외부 공급자는 허용 목록에 있는 퍼블릭 IPv4 주소에서 오는 요청만 지원합니다. 회사는 애플리케이션이 새 서비스를 사용하기 전에 외부 공급자에게 단일 공용 IP 주소를 제공해야 합니다. 애플리케이션이 새 서비스에 액세스할 수 있는 기능을 제공하는 솔루션은 무엇입니까? A. NAT 게이트웨이를 배포합니다. 탄력적 IP 주소를 NAT 게이트웨이와 연결합니다. NAT 게이트웨이를 사용하도록 VPC를 구성합니다. B. 외부 전용 인터넷 게이트웨이를 배포합니다. 탄력적 IP 주소를 외부 전용 인터넷 게이트웨이와 연결합니다. 외부 전용 인터넷 게이트웨이를 사용하도록 Lambda 함수에서 탄력적 네트워크 인터페이스를 구성합니다. C. 인터넷 게이트웨이를 배포합니다. 탄력적 IP 주소를 인터넷 게이트웨이와 연결합니다. 인터넷 게이트웨이를 사용하도록 Lambda 함수를 구성합니다. D. 인터넷 게이트웨이를 배포합니다. 탄력적 IP 주소를 인터넷 게이트웨이와 연결합니다. 퍼블릭 VPC 라우팅 테이블에서 인터넷 게이트웨이를 사용하도록 기본 라우팅을 구성합니다. Answer: A Explanation: This solution will give the Lambda function access to the internet by routing its outbound traffic through the NAT gateway, which has a public Elastic IP address. This will allow the external provider to whitelist the single public IP address associated with the NAT gateway, and enable the application to access the new service Deploying a NAT gateway and associating an Elastic IP address with it, and then configuring the VPC to use the NAT gateway, will give the application the ability to access the new service. This is because the NAT gateway will be the single public IP address that the external provider needs for the allow list. The NAT gateway will allow the application to access the service, while keeping the underlying Lambda functions private. When configuring NAT gateways, you should ensure that the route table associated with the NAT gateway has a route to the internet gateway with a target of the internet gateway. Additionally, you should ensure that the security group associated with the NAT gateway allows outbound traffic from the Lambda functions. Reference: AWS Certified Solutions Architect Professional Official Amazon Text Book [1], page 456 https://docs.aws.amazon.com/vpc/latest/userguide/VPC_NAT_Gateway.html
QUESTION NO: 163 SaaS(Software-as-a-Service) 공급자는 ALB(Application Load Balancer)를 통해 API를 노출합니다. ALB는 us-east-I 지역에 배포된 Amazon Elastic Kubernetes Service(Amazon EKS) 클러스터에 연결됩니다. 노출된 API에는 LINK, UNLINK, LOCK 및 UNLOCK과 같은 몇 가지 비표준 REST 메서드 사용이 포함되어 있습니다. 미국 이외 지역의 사용자는 이러한 API에 대해 길고 일관성 없는 응답 시간을 보고하고 있습니다. 솔루션 설계자는 운영 오버헤드를 최소화하는 솔루션으로 이 문제를 해결해야 합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까? A. Amazon CloudFront 배포를 추가합니다. ALB를 원본으로 구성합니다. B. Amazon API Gateway 엣지 최적화 API 엔드포인트를 추가하여 API를 노출합니다. ALB를 대상으로 구성합니다. C. AWS Global Accelerator에 액셀러레이터를 추가합니다. ALB를 원본으로 구성합니다. D. 두 개의 추가 AWS 지역인 eu-west-l 및 ap-southeast-2에 API를 배포합니다. Amazon Route 53에 지연 시간 기반 라우팅 레코드를 추가합니다. Answer: C Explanation: Adding an accelerator in AWS Global Accelerator will enable improving the performance of the APIs for local and global users1. AWS Global Accelerator is a service that uses the AWS global network to route traffic to the optimal regional endpoint based on health, client location, and policies1. Configuring the ALB as the origin will enable connecting the accelerator to the ALB that exposes the APIs2. AWS Global Accelerator supports nonstandard REST methods such as LINK, UNLINK, LOCK, and UNLOCK3.
QUESTION NO: 164 소매 회사는 AWS Organizations에서 조직의 일부가 되도록 AWS 계정을 구성했습니다. 회사는 통합 청구를 설정하고 부서를 다음 OU에 매핑했습니다. 재무. 매상. 인사부 <인사>. 마케팅 및 운영. 각 OU에는 부서 내의 각 환경에 대해 하나씩 여러 AWS 계정이 있습니다. 이러한 환경은 개발, 테스트, 사전 프로덕션 및 프로덕션입니다. HR 부서는 3개월 내에 출시할 새로운 시스템을 출시하고 있습니다. 준비 과정에서 HR 부서는 프로덕션 AWS 계정에서 여러 예약 인스턴스(RI)를 구입했습니다. HR 부서에서 이 계정에 새 애플리케이션을 설치합니다. HR 부서는 다른 부서가 R1 할인을 공유할 수 없도록 하려고 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. HR 부서 프로덕션 계정의 AWS Billing and Cost Management 콘솔에서 R1 공유를 끕니다. B. 조직에서 HR 부서의 프로덕션 AWS 계정을 제거합니다. 통합 청구 구성에만 계정을 추가하십시오. C. AWS Billing and Cost Management 콘솔에서 조직의 마스터 계정을 사용하여 HR 부서의 프로덕션 AWS 계정에 대한 R1 공유를 끕니다. D. 조직에 SCP를 생성하여 RI에 대한 액세스를 제한합니다. 다른 부서의 OU에 SCP를 적용합니다. Answer: C Explanation: You can use the management account of the organization in AWS Billing and Cost Management console to turn off RI sharing for the HR department's production AWS account. This will prevent other departments from sharing the RI discounts and ensure that only the HR department can use the RIs purchased in their production account.
QUESTION NO: 165 회사는 하이브리드 DNS 솔루션을 설계해야 합니다. 이 솔루션은 VPC 내에 저장된 리소스에 대해 도메인 cloud.example.com에 대해 Amazon Route 53 프라이빗 호스팅 영역을 사용합니다. 회사에는 다음과 같은 DNS 확인 요구 사항이 있습니다. * 온프레미스 시스템은 cloud.example.com을 확인하고 연결할 수 있어야 합니다. * 모든 VPC는 cloud.example.com을 확인할 수 있어야 합니다. 온프레미스 회사 네트워크와 AWS Transit Gateway 사이에 이미 AWS Direct Connect 연결이 있습니다. 회사는 최고의 성능으로 이러한 요구 사항을 충족하기 위해 어떤 아키텍처를 사용해야 합니까? A. 프라이빗 호스팅 영역을 모든 VPC에 연결합니다. 공유 서비스 VPC에서 Route 53 인바운드 해석기를 생성합니다. 모든 VPC를 전송 게이트웨이에 연결하고 인바운드 해석기를 가리키는 cloud.example.com에 대한 온프레미스 DNS 서버에서 전달 규칙을 생성합니다. B. 프라이빗 호스팅 영역을 모든 VPC에 연결합니다. 공유 서비스 VPC에 Amazon EC2 조건부 전달자를 배포합니다. 모든 VPC를 transit gateway에 연결하고 조건부 전달자를 가리키는 cloud.example.com에 대한 온프레미스 DNS 서버에서 전달 규칙을 생성합니다. C. 프라이빗 호스팅 영역을 공유 서비스 VPC에 연결합니다. 공유 서비스 VPC에서 Route 53 아웃바운드 해석기를 생성합니다. 모든 VPC를 전송 게이트웨이에 연결하고 아웃바운드 해석기를 가리키는 cloud.example.com에 대한 온프레미스 DNS 서버에서 전달 규칙을 생성합니다. D. 프라이빗 호스팅 영역을 공유 서비스 VPC에 연결합니다. 공유 서비스 VPC에서 Route 53 인바운드 해석기를 생성합니다. 공유 서비스 VPC를 transit gateway에 연결하고 인바운드 해석기를 가리키는 cloud.example.com에 대한 온프레미스 DNS 서버에서 전달 규칙을 생성합니다. Answer: A Explanation: Amazon Route 53 Resolver is a managed DNS resolver service from Route 53 that helps to create conditional forwarding rules to redirect query traffic1. By associating the private hosted zone to all the VPCs, the solutions architect can enable DNS resolution for cloud.example.com within the VPCs. By creating a Route 53 inbound resolver in the shared services VPC, the solutions architect can enable DNS resolution for cloud.example.com from on-premises systems. By attaching all VPCs to the transit gateway, the solutions architect can enable connectivity between the VPCs and the on-premises network through AWS Direct Connect. By creating forwarding rules in the on-premises DNS server for cloud.example.com that point to the inbound resolver, the solutions architect can direct DNS queries for cloud.example.com to the Route 53 Resolver endpoint in AWS. This solution will provide the highest performance as it leverages Route 53 Resolver's optimized routing and caching capabilities.
QUESTION NO: 166 AWS 파트너 회사는 org라는 조직을 사용하여 AWS Organizations에서 서비스를 구축하고 있습니다. 이 서비스를 사용하려면 파트너 회사가 org2라는 별도의 조직에 있는 고객 계정의 AWS 리소스에 액세스할 수 있어야 합니다. 회사는 API 또는 명령줄 도구를 사용하여 고객 계정에 대한 최소 권한 보안 액세스를 설정해야 합니다. 가장 안전한 방법은 무엇입니까? org1이 리소스 h org2에 액세스하도록 허용하려면? A. 고객은 로그인하여 필요한 작업을 수행하기 위해 파트너사에 AWS 계정 액세스 키를 제공해야 합니다. B. 고객은 IAM 사용자를 생성하고 필요한 권한을 IAM 사용자에게 할당해야 합니다. 그런 다음 고객은 파트너 회사에 자격 증명을 제공하여 로그인하고 필요한 작업을 수행해야 합니다. C. 고객은 IAM 역할을 생성하고 IAM 역할에 필요한 권한을 할당해야 합니다. 그런 다음 파트너 회사는 필요한 작업을 수행하기 위해 액세스를 요청할 때 IAM 로테의 Amazon 리소스 이름(ARN)을 사용해야 합니다. D. 고객은 IAM 로트를 생성하고 필요한 권한을 IAM 로트에 할당해야 합니다. 그런 다음 파트너 회사는 IAM 로테의 Amazon 리소스 이름(ARN)을 사용해야 합니다. 필요한 작업을 수행하기 위해 액세스 권한을 요청할 때 IAM 역할의 trust pokey에 외부 ID 포함 Answer: C Explanation: https://docs.aws.amazon.com/IAM/latest/UserGuide/confused-deputy.html This is the most secure way to allow org1 to access resources in org2 because it allows for least privilege security access. The customer should create an IAM role and assign the required permissions to the IAM role. The partner company should then use the IAM role's Amazon Resource Name (ARN) and include the external ID in the IAM role's trust policy when requesting access to perform the required tasks. This ensures that the partner company can only access the resources that it needs and only from the specific customer account.
QUESTION NO: 167 회사에 AWS Organizations의 조직 구성원인 50개의 AWS 계정이 있습니다. 각 계정에는 여러 개의 VPC가 포함되어 있습니다. 회사는 AWS Transit Gateway를 사용하여 각 구성원 계정의 VPC 간에 연결을 설정하려고 합니다. 새 VPC 및 전송 게이트웨이 연결을 생성하는 프로세스를 자동화합니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (2개 선택) A. 마스터 계정에서 AWS Resource Access Manager를 사용하여 회원 계정과 전송 게이트웨이를 공유합니다. B. 마스터 계정 프롬, AWS Organizations SCP를 사용하여 회원 계정과 전송 게이트웨이 공유 C. 멤버 계정에 새 VPC 및 VPC 전송 게이트웨이 연결을 자동으로 생성하는 마스터 계정에서 AWS CloudFormation 스택 세트를 시작합니다. 전송 게이트웨이 ID를 사용하여 마스터 계정의 전송 게이트웨이와 연결을 연결합니다. D. 멤버 계정에 새 VPC와 피어링 전송 게이트웨이 연결을 자동으로 생성하는 마스터 계정에서 AWS CloudFormation 스택 세트를 시작합니다. 전송 게이트웨이 서비스 연결 역할을 사용하여 마스터 계정의 전송 게이트웨이와 연결을 공유합니다. E. 마스터 계정에서 AWS Service Catalog를 사용하여 회원 계정과 전송 게이트웨이를 공유합니다. Answer: A,C Explanation: https://aws.amazon.com/blogs/mt/self-service-vpcs-in-aws-control-tower-using-aws-servicecatalog/ https://docs.aws.amazon.com/vpc/latest/tgw/tgw-transit-gateways.html https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-resource-ec2- transitgatewayattachment.html
QUESTION NO: 168 회사에서 문서 처리 워크로드를 AWS로 마이그레이션하고 있습니다. 이 회사는 기본적으로 Amazon S3 API를 사용하여 처리 서버가 초당 약 5개의 문서 속도로 생성하는 문서를 저장, 검색 및 수정하도록 많은 애플리케이션을 업데이트했습니다. 문서 처리가 완료되면 고객은 Amazon S3에서 직접 문서를 다운로드할 수 있습니다. 마이그레이션 과정에서 회사는 S3 API를 지원하기 위해 많은 문서를 생성하는 처리 서버를 즉시 업데이트할 수 없음을 발견했습니다. 서버는 Linux에서 실행되며 서버가 생성하고 수정하는 파일에 대한 빠른 로컬 액세스가 필요합니다. 서버가 처리를 마치면 파일을 30분 이내에 다운로드할 수 있도록 대중이 사용할 수 있어야 합니다. 최소한의 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 애플리케이션을 AWS Lambda 함수로 마이그레이션합니다. Java용 AWS SDK를 사용하여 회사에서 Amazon S3에 직접 저장하는 파일을 생성, 수정 및 액세스합니다. B. Amazon S3 파일 게이트웨이를 설정하고 문서 저장소에 연결된 파일 공유를 구성합니다. NFS를 사용하여 Amazon EC2 인스턴스에 파일 공유를 탑재합니다. Amazon S3에서 변경 사항이 발생하면 RefreshCache API 호출을 시작하여 S3 파일 게이트웨이를 업데이트합니다. C. 가져오기 및 내보내기 정책으로 Amazon FSx for Lustre를 구성합니다. 새 파일 시스템을 S3 버킷에 연결합니다. NFS를 사용하여 Lustre 클라이언트를 설치하고 문서 저장소를 Amazon EC2 인스턴스에 마운트합니다. D. Amazon EC2 인스턴스에 연결하도록 AWS DataSync를 구성합니다. 생성된 파일을 Amazon S3와 동기화하도록 작업을 구성합니다. Answer: C Explanation: Amazon FSx for Lustre is a fully managed service that provides cost-effective, highperformance, scalable storage for compute workloads. Powered by Lustre, the world's most popular high-performance file system, FSx for Lustre offers shared storage with sub-ms latencies, up to terabytes per second of throughput, and millions of IOPS. FSx for Lustre file systems can also be linked to Amazon Simple Storage Service (S3) buckets, allowing you to access and process data concurrently from both a high-performance file system and from the S3 API.
QUESTION NO: 169 회사는 온프레미스에서 인트라넷 애플리케이션을 실행합니다. 회사는 애플리케이션의 클라우드 백업을 구성하려고 합니다. 회사는 이 솔루션을 위해 AWS Elastic Disaster Recovery를 선택했습니다. 회사는 복제 트래픽이 공용 인터넷을 통해 이동하지 않도록 요구합니다. 응용 프로그램은 또한 인터넷에서 액세스할 수 없어야 합니다. 회사는 다른 애플리케이션에 대역폭이 필요하기 때문에 이 솔루션이 사용 가능한 모든 네트워크 대역폭을 소비하는 것을 원하지 않습니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (3개를 선택하세요.) A. 최소 2개의 프라이빗 서브넷, 2개의 NAT 게이트웨이 및 가상 프라이빗 게이트웨이가 있는 VPC를 생성합니다. B. 최소 2개의 퍼블릭 서브넷, 가상 프라이빗 게이트웨이 및 인터넷 게이트웨이가 있는 VPC를 생성합니다. C. 온프레미스 네트워크와 대상 AWS 네트워크 간에 AWS Site-to-Site VPN 연결을 생성합니다. D. 온프레미스 네트워크와 대상 AWS 네트워크 간에 AWS Direct Connect 연결 및 Direct Connect 게이트웨이를 생성합니다. E. 복제 서버를 구성하는 동안 데이터 복제에 사설 IP 주소를 사용하는 옵션을 선택합니다. F. 대상 서버에 대한 시작 설정을 구성하는 동안 복구 인스턴스의 개인 IP 주소가 원본 서버의 개인 IP 주소와 일치하는지 확인하는 옵션을 선택합니다. Answer: B,D,E Explanation: AWS Elastic Disaster Recovery (AWS DRS) is a service that minimizes downtime and data loss with fast, reliable recovery of on-premises and cloud-based applications using affordable storage, minimal compute, and point-in-time recovery1. Users can set up AWS DRS on their source servers to initiate secure data replication to a staging area subnet in their AWS account, in the AWS Region they select. Users can then launch recovery instances on AWS within minutes, using the most up-to-date server state or a previous point in time. To configure a cloud backup of the application with AWS DRS, users need to create a VPC that has at least two public subnets, a virtual private gateway, and an internet gateway. A VPC is a logically isolated section of the AWS Cloud where users can launch AWS resources in a virtual network that they define2. A public subnet is a subnet that has a route to an internet gateway3. A virtual private gateway is the VPN concentrator on the Amazon side of the Site-to-Site VPN connection4. An internet gateway is a horizontally scaled, redundant, and highly available VPC component that allows communication between instances in the VPC and the internet. Users need to create at least two public subnets for redundancy and high availability. Users need to create a virtual private gateway and attach it to the VPC to enable VPN connectivity between the on-premises network and the target AWS network. Users need to create an internet gateway and attach it to the VPC to enable internet access for the replication servers. To ensure that replication traffic does not travel through the public internet, users need to create an AWS Direct Connect connection and a Direct Connect gateway between the onpremises network and the target AWS network. AWS Direct Connect is a service that establishes a dedicated network connection from an on-premises network to one or more VPCs. A Direct Connect gateway is a globally available resource that allows users to connect multiple VPCs across different Regions to their on-premises networks using one or more Direct Connect connections. Users need to create an AWS Direct Connect connection between their on-premises network and an AWS Region. Users need to create a Direct Connect gateway and associate it with their VPC and their Direct Connect connection. To ensure that the application is not accessible from the internet, users need to select the option to use private IP addresses for data replication during configuration of the replication servers. This option configures the replication servers with private IP addresses only, without assigning any public IP addresses or Elastic IP addresses. This way, the replication servers can only communicate with other resources within the VPC or through VPN connections. Option A is incorrect because creating a VPC that has at least two private subnets, two NAT gateways, and a virtual private gateway is not necessary or cost-effective. A private subnet is a subnet that does not have a route to an internet gateway3. A NAT gateway is a highly available, managed Network Address Translation (NAT) service that enables instances in a private subnet to connect to the internet or other AWS services, but prevents the internet from initiating connections with those instances. Users do not need to create private subnets or NAT gateways for this use case, as they can use public subnets with private IP addresses for data replication. Option C is incorrect because creating an AWS Site-to-Site VPN connection between the onpremises network and the target AWS network will not ensure that replication traffic does not travel through the public internet. A Site-to-Site VPN connection consists of two VPN tunnels between an on-premises customer gateway device and a virtual private gateway in your VPC4. The VPN tunnels are encrypted using IPSec protocols, but they still use public IP addresses for communication. Users need to use AWS Direct Connect instead of Site-to-Site VPN for this use case. Option F is incorrect because selecting the option to ensure that the Recovery instance's private IP address matches the source server's private IP address during configuration of the launch settings for the target servers will not ensure that the application is not accessible from the internet. This option configures the Recovery instance with an identical private IP address as its source server when launched in drills or recovery mode. However, this option does not prevent assigning public IP addresses or Elastic IP addresses to the Recovery instance. Users need to select the option to use private IP addresses for data replication instead.
QUESTION NO: 170 회사에서 Amazon OpenSearch Service를 사용하여 데이터를 분석하고 있습니다. ㅏ. 회사는 S3 Standard 스토리지를 사용하는 Amazon S3 버킷에서 10개의 데이터 노드가 있는 OpenSearch Service 클러스터로 데이터를 로드합니다. 데이터는 읽기 전용 분석을 위해 1개월 동안 클러스터에 상주합니다. 1개월 후 회사는 데이터가 포함된 인덱스를 클러스터에서 삭제합니다. 규정 준수를 위해 회사는 모든 입력 데이터의 사본을 보관해야 합니다. 회사는 지속적인 비용에 대해 우려하고 있으며 솔루션 설계자에게 새로운 솔루션을 추천해 달라고 요청합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까? A. 모든 데이터 노드를 UltraWarm 노드로 교체하여 예상 용량을 처리합니다. 회사에서 데이터를 클러스터에 로드할 때 입력 데이터를 S3 Standard에서 S3 Glacier Deep Archive로 전환합니다. B. 클러스터의 데이터 노드 수를 2개로 줄입니다. UltraWarm 노드를 추가하여 예상 용량을 처리합니다. OpenSearch Service가 데이터를 수집할 때 UltraWarm으로 전환하도록 인덱스를 구성합니다. S3 수명 주기 정책을 사용하여 1개월 후에 입력 데이터를 S3 Glacier Deep Archive로 전환합니다. C. 클러스터의 데이터 노드 수를 2로 줄입니다. UltraWarm 노드를 추가하여 예상 용량을 처리합니다. OpenSearch Service가 데이터를 수집할 때 UltraWarm으로 전환하도록 인덱스를 구성합니다. 클러스터에 콜드 스토리지 노드 추가 인덱스를 UltraWarm에서 콜드 스토리지로 전환합니다. S3 수명 주기 정책을 사용하여 1개월 후 S3 버킷에서 입력 데이터를 삭제합니다. D. 클러스터의 데이터 노드 수를 2로 줄입니다. 인스턴스 지원 데이터 노드를 추가하여 예상 용량을 처리합니다. 회사에서 데이터를 클러스터에 로드할 때 입력 데이터를 S3 Standard에서 S3 Glacier Deep Archive로 전환합니다. Answer: B Explanation: By reducing the number of data nodes in the cluster to 2 and adding UltraWarm nodes to handle the expected capacity, the company can reduce the cost of running the cluster. Additionally, configuring the indexes to transition to UltraWarm when OpenSearch Service ingests the data will ensure that the data is stored in the most cost-effective manner. Finally, transitioning the input data to S3 Glacier Deep Archive after 1 month by using an S3 Lifecycle policy will ensure that the data is retained for compliance purposes, while also reducing the ongoing costs.
QUESTION NO: 171 회사는 두 개의 개별 비즈니스 단위로 구성됩니다. 각 사업부는 AWS Organizations의 단일 조직 내에 자체 AWS 계정을 가지고 있습니다. 비즈니스 단위는 정기적으로 중요한 문서를 서로 공유합니다. 공유를 용이하게 하기 위해 회사는 각 계정에 Amazon S3 버킷을 생성하고 S3 버킷 간에 양방향 복제를 구성했습니다. S3 버킷에는 수백만 개의 객체가 있습니다. 최근 보안 감사에서 두 S3 버킷 모두 유휴 암호화가 활성화되어 있지 않은 것으로 확인되었습니다. 회사 정책에 따라 모든 문서는 저장 시 암호화된 상태로 저장되어야 합니다. 회사는 Amazon S3 관리형 암호화 키(SSE-S3)로 서버 측 암호화를 구현하려고 합니다. 이러한 요구 사항을 충족하는 운영상 가장 효율적인 솔루션은 무엇입니까? A. 두 S3 버킷에서 SSE-S3를 켭니다. S3 배치 작업을 사용하여 동일한 위치에서 객체를 복사하고 암호화합니다. B. 각 계정에서 AWS Key Management Service(AWS KMS) 키를 생성합니다. 해당 AWS 계정에서 해당 KMS 키를 사용하여 각 S3 버킷에서 AWS KMS 키(SSE-KMS)로 서버 측 암호화를 켭니다. S3 배치 작업을 사용하여 객체를 동일한 위치에 복사합니다. C. 각 계정에서 AWS Key Management Service(AWS KMS) 키를 생성합니다. 해당 AWS 계정에서 해당 KMS 키를 사용하여 각 S3 버킷에서 AWS KMS 키(SSE-KMS)로 서버 측 암호화를 켭니다. AWS CLI에서 S3 복사 명령을 사용하여 기존 객체를 암호화합니다. D. 두 S3 버킷에서 SSE-S3를 켭니다. AWS CLI에서 S3 복사 명령을 사용하여 기존 객체를 암호화합니다. Answer: A Explanation: "The S3 buckets have millions of objects" If there are million of objects then you should use Batch operations. https://aws.amazon.com/blogs/storage/encrypting-objects-with-amazon-s3- batch-operations/
QUESTION NO: 172 회사에서 여러 AWS 계정을 사용하고 있습니다. DNS 레코드는 계정 A의 Amazon Route 53용 프라이빗 호스팅 영역에 저장되어 있습니다. 회사의 애플리케이션과 데이터베이스는 계정 B에서 실행 중입니다. 솔루션 설계자가 새 VPC에서 2네트 애플리케이션을 배포하는 데 성공했습니다. 구성을 단순화하기 위해 Amazon RDS 엔드포인트에 대한 db.example com CNAME 레코드 세트가 Amazon Route 53용 프라이빗 호스팅 영역에 생성되었습니다. 배포 중에 응용 프로그램을 시작하지 못했습니다. 문제 해결 결과 Amazon EC2 인스턴스에서 db.example com을 확인할 수 없음이 밝혀졌습니다. 솔루션 설계자는 Route 53에서 레코드 세트가 올바르게 생성되었음을 확인했습니다. 솔루션 설계자는 이 문제를 해결하기 위해 어떤 단계 조합을 수행해야 합니까? (2개 선택) A. 새 VPC의 별도 EC2 인스턴스에 데이터베이스 배포 프라이빗 호스팅 영역에서 인스턴스의 프라이빗 IP에 대한 레코드 세트 생성 B. SSH를 사용하여 애플리케이션 계층 EC2 인스턴스에 연결합니다. RDS 엔드포인트 IP 주소를 /eto/resolv.conf 파일 C. 계정 A의 프라이빗 호스팅 영역을 계정 B의 새 VPC와 연결하기 위한 권한 부여 생성 D. example.com 도메인 m 계정 B에 대한 프라이빗 호스팅 영역 생성 AWS 계정 간에 Route 53 복제 구성 E. 계정 B의 새 VPC를 계정 A의 호스팅 영역과 연결합니다. 계정 A에서 연결 승인을 삭제합니다. Answer: C,E Explanation: https://aws.amazon.com/premiumsupport/knowledge-center/private-hosted-zone-differentaccount/
QUESTION NO: 173 회사는 단일 Amazon EC2 인스턴스에서 실행되는 Grafana 데이터 시각화 솔루션을 사용하여 회사의 AWS 워크로드 상태를 모니터링합니다. 회사는 회사가 보존하고 싶은 대시보드를 만들기 위해 시간과 노력을 투자했습니다. 대시보드는 가용성이 높아야 하며 10분 이상 다운될 수 없습니다. 회사는 지속적인 유지 관리를 최소화해야 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Amazon CloudWatch 대시보드로 마이그레이션합니다. 기존 Grafana 대시보드와 일치하도록 대시보드를 다시 만듭니다. 가능한 경우 자동 대시보드를 사용하십시오. B. Amazon 관리형 Grafana 작업 공간을 생성합니다. 새 Amazon CloudWatch 데이터 원본을 구성합니다. 기존 Grafana 인스턴스에서 대시보드를 내보냅니다. 대시보드를 새 작업 공간으로 가져옵니다. C. Grafana가 사전 설치된 AMI를 생성합니다. Amazon Elastic File System(Amazon EFS)에 기존 대시보드를 저장합니다. 새 AMI를 사용하는 Auto Scaling 그룹을 생성합니다. Auto Scaling 그룹의 최소, 희망, 최대 인스턴스 수를 하나로 설정합니다. 두 개 이상의 가용 영역을 제공하는 Application Load Balancer를 생성합니다. D. 매시간 한 번씩 Grafana를 실행하는 EC2 인스턴스를 백업하도록 AWS Backup을 구성합니다. 필요한 경우 대체 가용 영역의 최신 스냅샷에서 EC2 인스턴스를 복원합니다. Answer: C Explanation: By creating an AMI that has Grafana pre-installed and storing the existing dashboards in Amazon Elastic File System (Amazon EFS) it allows for faster and more efficient scaling, and by creating an Auto Scaling group that uses the new AMI and setting the Auto Scaling group's minimum, desired, and maximum number of instances to one and creating an Application Load Balancer that serves at least two Availability Zones, it ensures high availability and minimized downtime.
QUESTION NO: 174 회사는 Production이라는 단일 OU가 있는 AWS Organizations를 사용하여 여러 계정을 관리합니다. 모든 계정은 Production OU의 구성원입니다. 관리자는 조직의 루트에 있는 거부 목록 SCP를 사용하여 제한된 서비스에 대한 액세스를 관리합니다. 이 회사는 최근에 새 비즈니스 단위를 인수하고 새 비즈니스 단위의 기존 AWS 계정을 조직에 초대했습니다. 온보딩 후 새 비즈니스 단위의 관리자는 회사 정책을 충족하기 위해 기존 AWS Config 규칙을 업데이트할 수 없음을 발견했습니다. 관리자가 추가 장기 유지 관리 없이 현재 정책을 변경하고 계속 시행할 수 있는 옵션은 무엇입니까? A. AWS Config에 대한 액세스를 제한하는 조직의 루트 SCP를 제거합니다. 회사의 표준 AWS Config 규칙에 대한 AWS Service Catalog 제품을 생성하고 새 계정을 포함하여 조직 전체에 배포합니다. B. 새 계정에 대해 Onboarding이라는 임시 OU를 생성합니다. 온보딩 OU에 SCP를 적용하여 AWS Config 작업을 허용합니다. AWS Config 조정이 완료되면 새 계정을 프로덕션 OU로 이동합니다. C. 조직의 루트 SCP를 거부 목록 SCP에서 허용 목록 SCP로 변환하여 필요한 서비스만 허용하도록 SCP를 일시적으로 새 계정의 보안 주체에 대해서만 AWS Config 작업을 허용하는 조직의 루트에 적용합니다. D. 새 계정에 대해 온보딩이라는 임시 OU를 생성합니다. 온보딩 OU에 SCP를 적용하여 AWS Config 작업을 허용합니다. 조직의 루트 SCP를 프로덕션 OU로 이동합니다. AWS Config 조정이 완료되면 새 계정을 프로덕션 OU로 이동합니다. Answer: D Explanation: An SCP at a lower level can't add a permission after it is blocked by an SCP at a higher level. SCPs can only filter; they never add permissions. SO you need to create a new OU for the new account assign an SCP, and move the root SCP to Production OU. Then move the new account to production OU when AWS config is done.
QUESTION NO: 175 금융 회사는 Amazon S3에서 데이터 레이크를 호스팅합니다. 회사는 SFTP를 통해 매일 밤 여러 제3자로부터 재무 데이터 기록을 받습니다. 회사는 VPC의 퍼블릭 서브넷에 있는 Amazon EC2 인스턴스에서 자체 SFTP 서버를 실행합니다. 파일이 업로드된 후 동일한 인스턴스에서 실행되는 cron 작업에 의해 데이터 레이크로 이동됩니다. SFTP 서버는 Amazon Route를 사용하여 DNS sftp.examWe.com에서 연결할 수 있습니다. 53. 솔루션 설계자는 SFTP 솔루션의 안정성과 확장성을 개선하기 위해 무엇을 해야 합니까? A. EC2 인스턴스를 Auto Scaling 그룹으로 이동합니다. ALB(Application Load Balancer) 뒤에 EC2 인스턴스를 배치합니다. ALB를 가리키도록 Route 53의 DNS 레코드 sftp.example.com을 업데이트합니다. B. SFTP 서버를 SFTP용 AWS 전송으로 마이그레이션합니다. Route에서 DNS 레코드 sftp.example.com 업데이트 53은 서버 끝점 호스트 이름을 가리킵니다. C. SFTP 서버를 AWS Storage Gateway의 파일 게이트웨이로 마이그레이션합니다. 파일 게이트웨이 엔드포인트를 가리키도록 Route 53의 DNS 레코드 sflp.example.com을 업데이트합니다. D. NLB(Network Load Balancer) 뒤에 EC2 인스턴스를 배치합니다. NLB를 가리키도록 Route 53의 DNS 레코드 sftp.example.com을 업데이트합니다. Answer: B Explanation: https://aws.amazon.com/aws-transfer-family/faqs/ https://docs.aws.amazon.com/transfer/latest/userguide/what-is-aws-transfer-family.html https://aws.amazon.com/about-aws/whats-new/2018/11/aws-transfer-for-sftp-fully-managedsftp-for-s3/?nc1=h_ls
QUESTION NO: 176 회사는 단일 AWS 리전에서 서버리스 애플리케이션을 실행합니다. 애플리케이션은 외부 URL에 액세스하고 해당 사이트에서 메타데이터를 추출합니다. 회사는 Amazon Simple Notification Service(Amazon SNS) 주제를 사용하여 URL을 Amazon Simple Queue Service(Amazon SQS) 대기열에 게시합니다. AWS Lambda 함수는 대기열을 이벤트 소스로 사용하고 대기열의 URL을 처리합니다. 결과는 Amazon S3 버킷 회사는 사이트 현지화의 가능한 차이점을 비교하기 위해 다른 지역의 각 URL을 처리하려고 합니다. URL은 기존 지역에서 게시되어야 합니다. 결과는 현재 리전의 기존 S3 버킷에 기록되어야 합니다. 이러한 요구 사항을 충족하는 다중 리전 배포를 생성하는 변경 조합은 무엇입니까? (2개를 선택하세요.) A. Lambda 함수를 사용하여 SOS 대기열을 다른 지역에 배포합니다. B. 각 지역의 SNS 주제를 SQS 대기열에 구독합니다. C. 각 지역의 SQS 대기열을 각 지역의 SNS 주제에 구독합니다. D. 각 리전의 SNS 주제에 URL을 게시하도록 SQS 대기열을 구성합니다. E. SNS 주제와 Lambda 함수를 다른 지역에 배포합니다. Answer: A,C Explanation: https://docs.aws.amazon.com/sns/latest/dg/sns-cross-region-delivery.html
QUESTION NO: 177 회사에서 여러 AWS 계정을 관리하기 위해 AWS Organizations를 사용하고 있습니다. 보안을 위해 회사는 모든 조직 구성원 계정에서 타사 알림 시스템과 통합할 수 있는 Amazon Simple Notification Service(Amazon SNS) 항목을 생성해야 합니다. 솔루션 아키텍트 AWS CloudFormation 템플릿을 사용하여 SNS 주제 및 스택 세트를 생성하여 CloudFormation 스택 배포를 자동화했습니다. 조직에서 신뢰할 수 있는 액세스가 활성화되었습니다. 솔루션 설계자는 모든 AWS 계정에 CloudFormation StackSets를 배포하기 위해 무엇을 해야 합니까? A. 조직 구성원 계정에 스택 세트를 생성합니다. 서비스 관리 권한을 사용합니다. 조직에 배포할 배포 옵션을 설정합니다. CloudFormation StackSets 드리프트 감지를 사용합니다. B. 조직 구성원 계정에 스택을 생성합니다. 셀프 서비스 권한을 사용합니다. 조직에 배포할 배포 옵션을 설정합니다. CloudFormation StackSets 자동 배포를 활성화합니다. C. 조직 관리 계정 서비스 관리 권한 사용에서 스택 세트를 생성합니다. 조직에 배포할 배포 옵션을 설정합니다. CloudFormation StackSets 자동 배포를 활성화합니다. D. 조직 마스터 계정에서 스택을 생성합니다. 서비스 관리 권한을 사용합니다. 조직에 배포할 배포 옵션을 설정합니다. CloudFormation StackSets 드리프트 감지를 활성화합니다. Answer: C Explanation: https://aws.amazon.com/blogs/aws/use-cloudformation-stacksets-to-provision-resourcesacross-multiple-aws-accounts-and-regions/
QUESTION NO: 178 스마트 자동차를 제조하는 회사입니다. 회사는 맞춤형 애플리케이션을 사용하여 차량 데이터를 수집합니다. 차량은 MQTT 프로토콜을 사용하여 애플리케이션에 연결합니다. 회사는 5분 간격으로 데이터를 처리합니다. 그런 다음 회사는 차량 텔레매틱스 데이터를 온프레미스 스토리지에 복사합니다. 맞춤형 애플리케이션은 이 데이터를 분석하여 이상을 감지합니다. 데이터를 전송하는 차량의 수는 지속적으로 증가하고 있습니다. 최신 차량은 많은 양의 데이터를 생성합니다. 온프레미스 스토리지 솔루션은 피크 트래픽에 맞게 확장할 수 없으므로 데이터 손실이 발생합니다. 회사는 솔루션을 현대화하고 솔루션을 AWS로 마이그레이션하여 확장 문제를 해결해야 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS IOT Greengrass를 사용하여 차량 데이터를 Amazon Managed Streaming for Apache Kafka(Amazon MSK)로 보냅니다. Amazon S3에 데이터를 저장할 Apache Kafka 애플리케이션을 생성합니다. Amazon SageMaker에서 사전 훈련된 모델을 사용하여 이상을 감지합니다. B. AWS IOT Core를 사용하여 차량 데이터를 수신합니다. Amazon S3에 데이터를 저장하는 Amazon Kinesis Data Firehose 전송 스트림으로 데이터를 라우팅하도록 규칙을 구성합니다. 이상을 감지하기 위해 전송 스트림에서 읽는 Amazon Kinesis Data Analytics 애플리케이션을 생성합니다. C. AWS IOT FleetWise를 사용하여 차량 데이터를 수집합니다. 데이터를 Amazon Kinesis 데이터 스트림으로 보냅니다. Amazon Kinesis Data Firehose 전송 스트림을 사용하여 Amazon S3에 데이터를 저장합니다. AWS Glue에 내장된 기계 학습 변환을 사용하여 이상을 감지합니다. D. RabbitMQ용 Amazon MQ를 사용하여 차량 데이터를 수집합니다. 데이터를 Amazon Kinesis Data Firehose 전송 스트림으로 보내 Amazon S3에 데이터를 저장합니다. Amazon Lookout for Metrics를 사용하여 이상 징후를 탐지하십시오. Answer: B Explanation: Using AWS IoT Core to receive the vehicle data will enable connecting the smart vehicles to the cloud using the MQTT protocol1. AWS IoT Core is a platform that enables you to connect devices to AWS Services and other devices, secure data and interactions, process and act upon device data, and enable applications to interact with devices even when they are offline2. Configuring rules to route data to an Amazon Kinesis Data Firehose delivery stream that stores the data in Amazon S3 will enable processing and storing the vehicle data in a scalable and reliable way3. Amazon Kinesis Data Firehose is a fully managed service that delivers real-time streaming data to destinations such as Amazon S3. Creating an Amazon Kinesis Data Analytics application that reads from the delivery stream to detect anomalies will enable analyzing the vehicle data using SQL queries or Apache Flink applications. Amazon Kinesis Data Analytics is a fully managed service that enables you to process and analyze streaming data using SQL or Java.
QUESTION NO: 179 제조 회사에서 공장용 검사 솔루션을 구축하고 있습니다. 이 회사는 각 조립 라인 끝에 IP 카메라가 있습니다. 이 회사는 Amazon SageMaker를 사용하여 기계 학습(ML) 모델을 교육하여 스틸 이미지에서 일반적인 결함을 식별했습니다. 회사는 결함이 감지되면 공장 작업자에게 현지 피드백을 제공하려고 합니다. 회사는 공장의 인터넷 연결이 끊어진 경우에도 이 피드백을 제공할 수 있어야 합니다. 회사에는 작업자에게 로컬 피드백을 제공하는 API를 호스팅하는 로컬 Linux 서버가 있습니다. 회사는 이러한 요구 사항을 충족하기 위해 ML 모델을 어떻게 배포해야 합니까? A. 각 IP 카메라에서 AWS로 Amazon Kinesis 비디오 스트림을 설정합니다. Amazon EC2 인스턴스를 사용하여 스트림의 스틸 이미지를 찍습니다. 이미지를 Amazon S3 버킷에 업로드합니다. ML 모델을 사용하여 SageMaker 엔드포인트를 배포합니다. 새 이미지가 업로드될 때 추론 엔드포인트를 호출하도록 AWS Lambda 함수를 호출합니다. 결함이 감지되면 로컬 API를 호출하도록 Lambda 함수를 구성합니다. B. 로컬 서버에 AWS IoT Greengrass를 배포합니다. ML 모델을 Greengrass 서버에 배포합니다. Greengrass 구성 요소를 생성하여 카메라에서 스틸 이미지를 가져오고 추론을 실행합니다. 결함이 감지되면 로컬 API를 호출하도록 구성 요소를 구성합니다. C. AWS Snowball 디바이스를 주문합니다. SageMaker 엔드포인트, ML 모델 및 Amazon EC2 인스턴스를 Snowball 디바이스에 배포합니다. 카메라에서 스틸 이미지를 가져옵니다. EC2 인스턴스에서 추론을 실행합니다. 결함이 감지되면 로컬 API를 호출하도록 인스턴스를 구성합니다. D. 각 IP 카메라에 Amazon Monitron 장치를 배포합니다. 온프레미스에 Amazon Monitron Gateway를 배포합니다. ML 모델을 Amazon Monitron 디바이스에 배포합니다. 결함이 감지되면 Amazon Monitron 상태 경보를 사용하여 AWS Lambda 함수에서 로컬 API를 호출합니다. Answer: B Explanation: The company should use AWS IoT Greengrass to deploy the ML model to the local server and provide local feedback to the factory workers. AWS IoT Greengrass is a service that extends AWS cloud capabilities to local devices, allowing them to collect and analyze data closer to the source of information, react autonomously to local events, and communicate securely with each other on local networks1. AWS IoT Greengrass also supports ML inference at the edge, enabling devices to run ML models locally without requiring internet connectivity2. The other options are not correct because: Setting up an Amazon Kinesis video stream from each IP camera to AWS would not work if the factory's internet connectivity is down. It would also incur unnecessary costs and latency to stream video data to the cloud and back. Ordering an AWS Snowball device would not be a scalable or cost-effective solution for deploying the ML model. AWS Snowball is a service that provides physical devices for data transfer and edge computing, but it is not designed for continuous operation or frequent updates3. Deploying Amazon Monitron devices on each IP camera would not work because Amazon Monitron is a service that monitors the condition and performance of industrial equipment using sensors and machine learning, not cameras4. Reference: https://aws.amazon.com/greengrass/ https://docs.aws.amazon.com/greengrass/v2/developerguide/use-machine-learninginference.html https://aws.amazon.com/snowball/ https://aws.amazon.com/monitron/
QUESTION NO: 180 소매 회사는 비즈니스 파트너인 다른 회사에 일련의 데이터 파일을 제공해야 합니다. 이 파일은 소매 회사에 속한 계정 A의 Amazon S3 버킷에 저장됩니다. 비즈니스 파트너 회사는 오전 1시 사용자 중 한 명을 원합니다. User_DataProcessor. 자신의 AWS 계정(계정 B)에서 파일에 액세스합니다. User_DataProcessor가 S3 버킷에 성공적으로 액세스할 수 있도록 회사에서 수행해야 하는 단계 조합은 무엇입니까? (2개를 선택하세요.) A. 계정에서 S3 버킷에 대한 교차 출처 리소스 공유(CORS) 기능을 켭니다. B. 계정 A에서 S3 버킷 정책을 다음과 같이 설정합니다. C. 계정 A에서 S3 버킷 정책을 다음과 같이 설정합니다. D. 계정 B에서 User_DataProcessor의 권한을 다음과 같이 설정합니다. E. 계정 Bt에서 User_DataProcessor의 권한을 다음과 같이 설정합니다. Answer: C,D Explanation: https://aws.amazon.com/premiumsupport/knowledge-center/cross-account-access-s3/
QUESTION NO: 181 한 회사가 10개의 새 도메인 이름을 등록했습니다. 회사는 온라인 마케팅을 위해 도메인을 사용합니다. 회사는 온라인 방문자를 각 도메인의 특정 URL로 리디렉션하는 솔루션이 필요합니다. 모든 도메인 및 대상 URL은 JSON 문서에 정의됩니다. 모든 DNS 레코드는 Amazon Route 53에서 관리합니다. 솔루션 설계자는 HTTP 및 HTTPS 요청을 수락하는 리디렉션 서비스를 구현해야 합니다. 솔루션 아키텍트는 최소한의 운영 노력으로 이러한 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까? (3개를 선택하세요.) A. Amazon EC2 인스턴스에서 실행되는 동적 웹 페이지를 생성합니다. 리디렉션 URL을 조회하고 응답하기 위해 이벤트 메시지와 함께 JSON 문서를 사용하도록 웹 페이지를 구성합니다. B. HTTP 및 HTTPS 수신기를 포함하는 Application Load Balancer를 생성합니다. C. 이벤트 메시지와 함께 JSON 문서를 사용하여 리디렉션 URL을 조회하고 응답하는 AWS Lambda 함수를 생성합니다. D. 사용자 지정 도메인과 함께 Amazon API Gateway API를 사용하여 AWS Lambda 함수를 게시합니다. E. Amazon CloudFront 배포를 생성합니다. Lambda@Edge 함수를 배포합니다. F. AWS Certificate Manager(ACM)를 사용하여 SSL 인증서를 생성합니다. 도메인을 주체 대체 이름으로 포함합니다. Answer: C,E,F Explanation: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-edge-howit-works-tutorial.html
QUESTION NO: 182 회사는 개발용 AWS 계정이 몇 개 있고 프로덕션 애플리케이션을 AWS로 이동하려고 합니다. 회사는 유휴 상태의 Amazon Elastic Block Store(Amazon EBS) 암호화를 현재 프로덕션 계정과 향후 프로덕션 계정에만 적용해야 합니다. 이 회사는 빌트인 청사진과 가드레일을 포함하는 솔루션이 필요합니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (3개를 선택하세요.) A. AWS CloudFormation StackSets를 사용하여 프로덕션 계정에 AWS Config 규칙을 배포합니다. B. 기존 개발자 계정에서 새 AWS Control Tower 랜딩 존을 생성합니다. 계정에 대한 OU를 만듭니다. 프로덕션 및 개발 OU에 각각 프로덕션 및 개발 계정을 추가합니다. C. 회사의 마스터 계정에서 새로운 AWS Control Tower 랜딩 존을 생성합니다. 프로덕션 및 개발 OU에 프로덕션 및 개발 계정을 추가합니다. 각기. D. AWS Organizations에서 조직에 가입하도록 기존 계정을 초대합니다. 준수를 보장하기 위해 SCP를 생성합니다. E. 마스터 계정에서 가드레일을 생성하여 EBS 암호화를 감지합니다. F. 프로덕션 OU에 대한 가드레일을 생성하여 EBS 암호화를 감지합니다. Answer: C,D,F Explanation: https://docs.aws.amazon.com/controltower/latest/userguide/controls.html https://docs.aws.amazon.com/controltower/latest/userguide/strongly-recommendedcontrols.html#ebs-enable-encryption AWS is now transitioning the previous term 'guardrail' new term 'control'.
QUESTION NO: 183 한 회사는 최근 플랫폼 변경 전략을 사용하여 온프레미스 데이터 센터에서 AWS 클라우드로의 마이그레이션을 완료했습니다. 마이그레이션된 서버 중 하나는 중요한 애플리케이션이 의존하는 레거시 SMTP(Simple Mail Transfer Protocol) 서비스를 실행하고 있습니다. 애플리케이션은 회사 고객에게 아웃바운드 이메일 메시지를 보냅니다. 레거시 SMTP 서버는 TLS 암호화를 지원하지 않으며 TCP 포트 25를 사용합니다. 애플리케이션은 SMTP만 사용할 수 있습니다. 회사는 Amazon Simple Email Service(Amazon SES)를 사용하고 레거시 SMTP 서버를 폐기하기로 결정합니다. 회사는 SES 도메인을 생성하고 검증했습니다. 회사는 SES 한도를 해제했습니다. Amazon SES에서 이메일 메시지를 보내도록 애플리케이션을 수정하려면 회사는 어떻게 해야 합니까? A. TLS 래퍼를 사용하여 Amazon SES에 연결하도록 애플리케이션을 구성합니다. ses:SendEmail 및 ses:SendRawEmail 권한이 있는 IAM 역할을 생성합니다. Amazon EC2 인스턴스에 IAM 역할을 연결합니다. B. STARTTLS를 사용하여 Amazon SES에 연결하도록 애플리케이션을 구성합니다. Amazon SES SMTP 자격 증명을 얻습니다. 자격 증명을 사용하여 Amazon SES로 인증합니다. C. SES API를 사용하여 이메일 메시지를 보내도록 애플리케이션을 구성합니다. ses:SendEmail 및 ses:SendRawEmail 권한이 있는 IAM 역할을 생성합니다. Amazon SES에 대한 서비스 역할로 IAM 역할을 사용합니다. D. AWS SDK를 사용하여 이메일 메시지를 보내도록 애플리케이션을 구성합니다. Amazon SES에 대한 IAM 사용자를 생성합니다. API 액세스 키를 생성합니다. 액세스 키를 사용하여 Amazon SES로 인증합니다. Answer: B Explanation: To set up a STARTTLS connection, the SMTP client connects to the Amazon SES SMTP endpoint on port 25, 587, or 2587, issues an EHLO command, and waits for the server to announce that it supports the STARTTLS SMTP extension. The client then issues the STARTTLS command, initiating TLS negotiation. When negotiation is complete, the client issues an EHLO command over the new encrypted connection, and the SMTP session proceeds normally To set up a TLS Wrapper connection, the SMTP client connects to the Amazon SES SMTP endpoint on port 465 or 2465. The server presents its certificate, the client issues an EHLO command, and the SMTP session proceeds normally. https://docs.aws.amazon.com/ses/latest/dg/smtp-connect.html
QUESTION NO: 184 회사는 많은 AWS 계정을 보유하고 있으며 AWS Organizations를 사용하여 모든 계정을 관리합니다. 솔루션 설계자는 회사가 여러 계정에서 공통 네트워크를 공유하는 데 사용할 수 있는 솔루션을 구현해야 합니다. 회사의 인프라 팀에는 VPC가 있는 전용 인프라 계정이 있습니다. 인프라 팀은 이 계정을 사용하여 네트워크를 관리해야 합니다. 개별 계정은 자신의 네트워크를 관리할 수 없습니다. 그러나 개별 계정은 서브넷 내에서 AWS 리소스를 생성할 수 있어야 합니다. 이러한 요구 사항을 충족하기 위해 솔루션 설계자가 수행해야 하는 작업 조합은 무엇입니까? (2개를 선택하세요.) A. 인프라 계정에서 전송 게이트웨이를 생성합니다. B. AWS Organizations 마스터 계정에서 리소스 공유를 활성화합니다. C. AWS Organizations의 조직 내 각 AWS 계정에 VPC를 생성합니다. 인프라 계정의 VPC와 동일한 CIDR 범위 및 서브넷을 공유하도록 VPC를 구성합니다. 각 개별 계정의 VPC를 인프라 계정의 VPC와 피어링합니다. D. 인프라 계정의 AWS Resource Access Manager에서 리소스 공유를 생성합니다. 공유 네트워크를 사용할 특정 AWS Organizations OU를 선택합니다. 리소스 공유와 연결할 각 서브넷을 선택합니다. E. 인프라 계정의 AWS Resource Access Manager에서 리소스 공유를 생성합니다. 공유 네트워크를 사용할 특정 AWS Organizations OU를 선택합니다. 리소스 공유와 연결할 각 접두사 목록을 선택합니다. Answer: A,E Explanation: https://docs.aws.amazon.com/vpc/latest/userguide/sharing-managed-prefix-lists.html
QUESTION NO: 185 한 회사가 최근에 다른 여러 회사를 인수했습니다. 각 회사에는 청구 및 보고 방법이 다른 별도의 AWS 계정이 있습니다. 인수 회사는 모든 계정을 AWS Organizations의 하나의 조직으로 통합했습니다. 그러나 인수 회사는 모든 팀에 대해 의미 있는 그룹을 포함하는 비용 보고서를 생성하는 데 어려움을 겪었습니다. 인수 회사의 재무 팀은 자체 관리 애플리케이션을 통해 모든 회사의 비용을 보고할 수 있는 솔루션이 필요합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 조직에 대한 AWS 비용 및 사용 보고서를 생성합니다. 보고서에서 태그 및 비용 범주를 정의합니다. Amazon Athena에서 테이블을 생성합니다. Athena 테이블을 기반으로 Amazon QuickSight 데이터 세트를 생성합니다. 재무 팀과 데이터 세트를 공유합니다. B. 조직에 대한 AWS 비용 및 사용 보고서를 생성합니다. 보고서에서 태그 및 비용 범주를 정의합니다. 재무 부서에서 보고서 작성에 사용할 특수 템플릿을 AWS Cost Explorer에서 생성합니다. C. AWS Price List Query API에서 지출 정보를 수신하는 Amazon QuickSight 데이터 세트를 생성합니다. 재무 팀과 데이터 세트를 공유합니다. D. AWS Price List Query API를 사용하여 계정 지출 정보를 수집합니다. 재무 부서에서 보고서 작성에 사용할 특수 템플릿을 AWS Cost Explorer에서 생성합니다. Answer: A Explanation: Creating an AWS Cost and Usage Report for the organization and defining tags and cost categories in the report will allow for detailed cost reporting for the different companies that have been consolidated into one organization. By creating a table in Amazon Athena and an Amazon QuickSight dataset based on the Athena table, the finance team will be able to easily query and generate reports on the costs for all the companies. The dataset can then be shared with the finance team for them to use for their reporting needs.
QUESTION NO: 186 북미의 한 금융 서비스 회사는 AWS에서 고객에게 새로운 온라인 웹 애플리케이션을 출시할 계획입니다. 회사는 Amazon EC2 인스턴스의 us-east-1 지역에서 애플리케이션을 시작할 것입니다. 애플리케이션은 가용성이 높아야 하며 사용자 트래픽을 충족하도록 동적으로 확장되어야 합니다. 회사는 또한 활성-수동 장애 조치를 사용하여 us-west-1 지역의 애플리케이션에 대한 재해 복구 환경을 구현하려고 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. us-east-1에 VPC를 생성하고 us-west-1에 VPC를 생성합니다. us-east-1 VPC에서 VPC 피어링을 구성합니다. 두 VPC의 여러 가용 영역에 걸쳐 확장되는 Application Load Balancer(ALB) 생성 두 VPC의 여러 가용 영역에 걸쳐 EC2 인스턴스를 배포하는 Auto Scaling 그룹 생성 Auto Scaling 그룹을 ALB 뒤에 배치합니다. B. us-east-1에 VPC를 생성하고 us-west-1에 VPC를 생성합니다. us-east-1 VPC에서. 해당 VPC의 여러 가용 영역에 걸쳐 확장되는 Application Load Balancer(ALB)를 생성합니다. useast-1 VPC의 여러 가용 영역에 걸쳐 EC2 인스턴스를 배포하는 Auto Scaling 그룹을 생성합니다. ALB 뒤에 Auto Scaling 그룹을 배치합니다. us-west-1 VPC에서 동일한 구성을 설정합니다. Amazon Route 53 호스팅 영역 생성 각 ALB에 대해 별도의 레코드 생성 상태 확인을 활성화하여 리전 간 고가용성을 보장합니다. C. us-east-1에 VPC를 생성하고 us-west-1에 VPC를 생성합니다. 해당 VPC의 여러 가용 영역에 걸쳐 확장되는 Application Load Balancer(ALB) 생성 us-east-1 VPC의 여러 가용 영역에 걸쳐 EC2 인스턴스를 배포하는 Auto Scaling 그룹 생성 ALB 설정 뒤에 Auto Scaling 그룹 배치 us-west-1 VPC에서 동일한 구성 Amazon Route 53 호스팅 영역을 생성합니다. 각 ALB에 대해 별도의 레코드를 생성합니다. 상태 확인을 활성화하고 각 레코드에 대한 장애 조치 라우팅 정책을 구성합니다. D. us-east-1에 VPC를 생성하고 us-west-1에 VPC를 생성합니다. us-east-1 VPC에서 VPC 피어링을 구성합니다. 두 VPC의 여러 가용 영역에 걸쳐 EC2 인스턴스를 배포하는 Auto Scaling 그룹 생성에서 여러 가용 영역에 걸쳐 확장되는 Application Load Balancer(ALB) 생성 ALB 뒤에 Auto Scaling 그룹 배치 Amazon Route 53 호스트 생성. 생성 ALB에 대한 기록. Answer: C Explanation: it's the one that handles failover while B (the one shown as the answer today) it almost the same but does not handle failover.
QUESTION NO: 187 솔루션 설계자는 클라우드 엔지니어 팀이 AWS CLI를 사용하여 객체를 Amazon S3 버킷에 업로드할 수 있는 안전한 방법을 제공해야 합니다. 각 클라우드 엔지니어에게는 IAM 사용자가 있습니다. IAM 액세스 키 및 가상 MFA(Multi-Factor Authentication) 장치 클라우드 엔지니어의 IAM 사용자는 S3-access라는 그룹에 있습니다. 클라우드 엔지니어는 MFA를 사용하여 Amazon S3에서 작업을 수행해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? ? A. 정책을 S3 버킷에 연결하여 오전 1시 사용자가 S3 버킷에서 작업을 수행할 때 오전 1시 사용자에게 MFA 코드를 묻는 메시지를 표시합니다. AWS CLI와 함께 1AM 액세스 키를 사용하여 Amazon S3를 호출합니다. B. 보안 주체가 그룹을 맡을 때 보안 주체가 MFA를 사용하도록 요구하도록 S3 액세스 그룹에 대한 신뢰 정책 업데이트 AWS CLI와 함께 1AM 액세스 키를 사용하여 Amazon S3 호출 C. 정책을 S3 액세스 그룹에 연결하여 MFA가 없는 경우 모든 S3 작업을 거부합니다. AWS CLI와 함께 1AM 액세스 키를 사용하여 Amazon S3 호출 D. 정책을 S3 액세스 그룹에 연결하여 MFA가 없는 경우 모든 S3 작업을 거부합니다. AWS Security Token Service(AWS STS)에서 임시 자격 증명을 요청합니다. 사용자가 작업을 수행할 때 Amazon S3가 참조할 프로필에 임시 자격 증명을 연결합니다. Amazon S3의 작업 Answer: D Explanation: The company should attach a policy to the S3-access group to deny all S3 actions unless MFA is present. The company should request temporary credentials from AWS Security Token Service (AWS STS). The company should attach the temporary credentials in a profile that Amazon S3 will reference when the user performs actions in Amazon S3. This solution will meet the requirements because AWS STS is a service that enables you to request temporary, limited-privilege credentials for IAM users or for users that you authenticate (federated users). You can use MFA with AWS STS to provide an extra layer of security when requesting temporary credentials1. You can use the sts get-session-token AWS CLI command to request temporary credentials that include an MFA token2. You can then use these credentials with the AWS CLI to access Amazon S3 resources. To do this, you need to attach a policy to the IAM group that denies all S3 actions unless MFA is present3. You also need to create a profile in the AWS CLI configuration file that references the temporary credentials. The other options are not correct because: Attaching a policy to the S3 bucket to prompt the IAM user for an MFA code when the IAM user performs actions on the S3 bucket would not work because policies attached to S3 buckets cannot enforce MFA authentication. Policies attached to S3 buckets are resourcebased policies that define what actions can be performed on the bucket and by whom. They do not have any logic to prompt for an MFA code or verify it. Updating the trust policy for the S3-access group to require principals to use MFA when principals assume the group would not work because trust policies are used for roles, not groups. Trust policies are policies that define which principals can assume a role. They do not apply to groups, which are collections of IAM users that share permissions. Creating an Amazon Route 53 Resolver DNS Firewall domain list that contains the allowed domains and configuring a DNS Firewall rule group with rules to allow or block requests based on the domain list would not help with enforcing MFA authentication for Amazon S3 actions. Amazon Route 53 Resolver DNS Firewall is a feature that enables you to filter and regulate outbound DNS traffic for your VPC. You can create reusable collections of filtering rules in DNS Firewall rule groups and associate them with your VPCs. You can specify lists of domain names to allow or block, and you can customize the responses for the DNS queries that you block. This feature is useful for controlling access to sites and blocking DNSlevel threats, but not for requiring MFA authentication. Reference: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_temp.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_cliapi.html https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_sample-policies.html https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-profiles.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-dns-firewall.html
QUESTION NO: 188 회사에 AWS Lambda 함수로 호스팅되는 비동기 HTTP 애플리케이션이 있습니다. 퍼블릭 Amazon API Gateway 엔드포인트는 Lambda 함수를 호출합니다. Lambda 함수와 API 게이트웨이 엔드포인트는 us-east-1 리전에 상주합니다. 솔루션 설계자는 다른 AWS 리전에 대한 장애 조치를 지원하도록 애플리케이션을 재설계해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. us-west-2 리전에서 API 게이트웨이 엔드포인트를 생성하여 트래픽을 us-east-1의 Lambda 함수로 보냅니다. 장애 조치 라우팅 정책을 사용하여 두 API Gateway 엔드포인트에 대한 트래픽을 라우팅하도록 Amazon Route 53을 구성합니다. B. Amazon Simple Queue Service(Amazon SQS) 대기열을 생성합니다. Lambda 함수 대신 SQS 대기열로 트래픽을 보내도록 API Gateway를 구성합니다. 처리를 위해 대기열에서 메시지를 가져오도록 Lambda 함수를 구성합니다. C. us-west-2 리전에 Lambda 함수를 배포합니다. us-west-2에서 API 게이트웨이 엔드포인트를 생성하여 트래픽을 us-west-2의 Lambda 함수로 보냅니다. 두 API 게이트웨이 엔드포인트에서 트래픽을 관리하도록 AWS Global Accelerator 및 Application Load Balancer를 구성합니다. D. Lambda 함수와 API 게이트웨이 엔드포인트를 us-west-2 리전에 배포합니다. 장애 조치 라우팅 정책을 사용하여 두 API Gateway 엔드포인트에 대한 트래픽을 라우팅하도록 Amazon Route 53을 구성합니다. Answer: B Explanation: This solution allows for deploying the Lambda function and API Gateway endpoint to another region, providing a failover option in case of any issues in the primary region. Using Route 53's failover routing policy allows for automatic routing of traffic to the healthy endpoint, ensuring that the application is available even in case of issues in one region. This solution provides a cost-effective and simple way to implement failover while minimizing operational overhead.
QUESTION NO: 189 회사는 AWS 클라우드에서 loT 애플리케이션을 실행합니다. 이 회사는 미국의 주택에서 데이터를 수집하는 수백만 개의 센서를 보유하고 있습니다. 센서는 MOTT 프로토콜을 사용하여 사용자 지정 MQTT 브로커에 연결하고 데이터를 보냅니다. MQTT 브로커는 단일 Amazon EC2 인스턴스에 데이터를 저장합니다. 센서는 iot.example.com이라는 도메인을 통해 브로커에 연결됩니다. 이 회사는 Amazon Route 53을 DNS 서비스로 사용합니다. 회사는 데이터를 Amazon DynamoDB에 저장합니다. 여러 경우에 데이터 양이 MOTT 브로커에 과부하를 일으켜 센서 데이터 a가 손실되었습니다. 회사는 솔루션의 신뢰성을 향상시켜야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. MOTT 브로커를 위한 ALB(Application Load Balancer) 및 Auto Scaling 그룹을 생성합니다. Auto Scaling 그룹을 ALB의 대상으로 사용합니다. Route 53의 DNS 레코드를 별칭 레코드로 업데이트합니다. 별칭 레코드가 ALB를 가리키도록 합니다. MQTT 브로커를 사용하여 데이터를 저장합니다. B. 센서 데이터를 수신하도록 AWS loT Core를 설정합니다. AWS loT Core에 연결할 사용자 지정 도메인을 생성하고 구성합니다. AWS loT Core Data-ATS 엔드포인트를 가리키도록 Route 53의 DNS 레코드를 업데이트합니다. 데이터를 저장하도록 AWS loT 규칙을 구성합니다. C. NLB(Network Load Balancer)를 생성합니다. MQTT 브로커를 대상으로 설정합니다. AWS Global Accelerator 액셀러레이터를 생성합니다. NLB를 액셀러레이터의 끝점으로 설정합니다. Route 53의 DNS 레코드를 다중값 응답 레코드로 업데이트합니다. Global Accelerator IP 주소를 값으로 설정합니다. MQTT 브로커를 사용하여 데이터를 저장합니다. D. 센서 데이터를 수신하도록 AWS loT Greengrass를 설정합니다. AWS loT Greengrass 엔드포인트를 가리키도록 Route 53의 DNS 레코드를 업데이트합니다. 데이터를 저장하기 위해 AWS Lambda 함수를 호출하도록 AWS loT 규칙을 구성합니다. Answer: A Explanation: it describes a solution that uses an Application Load Balancer (ALB) and an Auto Scaling group for the MQTT broker. The ALB distributes incoming traffic across the instances in the Auto Scaling group and allows for automatic scaling based on incoming traffic. The use of an alias record in Route 53 allows for easy updates to the DNS record without changing the IP address. This solution improves the reliability of the MQTT broker by allowing it to automatically scale based on incoming traffic, reducing the likelihood of lost data due to broker overload. Reference: https://aws.amazon.com/elasticloadbalancing/applicationloadbalancer/ https://aws.amazon.com/autoscaling/ https://aws.amazon.com/route53/
QUESTION NO: 190 AWS 고객에게는 온프레미스에서 실행되는 웹 애플리케이션이 있습니다. 웹 애플리케이션은 방화벽 뒤에 있는 타사 API에서 데이터를 가져옵니다. 타사는 각 클라이언트의 허용 목록에서 하나의 공개 CIDR 블록만 허용합니다. 고객은 웹 애플리케이션을 AWS 클라우드로 마이그레이션하려고 합니다. 애플리케이션은 VPC의 Application Load Balancer(ALB) 뒤에 있는 Amazon EC2 인스턴스 세트에서 호스팅됩니다. ALB는 퍼블릭 서브넷에 있습니다. EC2 인스턴스는 프라이빗 서브넷에 있습니다. NAT 게이트웨이는 프라이빗 서브넷에 대한 인터넷 액세스를 제공합니다. 솔루션 설계자는 마이그레이션 후 웹 애플리케이션이 타사 API를 계속 호출할 수 있도록 어떻게 보장해야 합니까? A. 고객 소유 퍼블릭 IP 주소 블록을 VPC에 연결합니다. VPC의 퍼블릭 서브넷에 대한 퍼블릭 IP 주소 지정을 활성화합니다. B. 고객 소유의 공인 IP 주소 블록을 AWS 계정에 등록합니다. 주소 블록에서 탄력적 IP 주소를 생성하고 VPC의 NAT 게이트웨이에 할당합니다. C. 고객 소유 IP 주소 블록에서 탄력적 IP 주소를 생성합니다. 고정 탄력적 IP 주소를 ALB에 할당합니다. D. 고객 소유의 공인 IP 주소 블록을 AWS 계정에 등록합니다. 주소 블록의 탄력적 IP 주소를 사용하도록 AWS Global Accelerator를 설정합니다. ALB를 가속기 끝점으로 설정합니다. Answer: B Explanation: When EC2 instances reach third-party API through internet, their privates IP addresses will be masked by NAT Gateway public IP address. https://aws.amazon.com/blogs/networking-and-content-delivery/introducing-bring-your-ownip-byoip-for-amazon-vpc/
QUESTION NO: 191 회사는 소스라는 AWS 계정에 애플리케이션을 가지고 있습니다. 계정은 AWS Organizations의 조직에 있습니다. 애플리케이션 중 하나는 AWS Lambda 기능을 사용하고 인벤토리 데이터를 Amazon Aurora 데이터베이스에 저장합니다. 애플리케이션은 배포 패키지를 사용하여 Lambda 함수를 배포합니다. 회사는 Aurora에 대한 자동 백업을 구성했습니다. 회사는 Lambda 함수와 Aurora 데이터베이스를 Target이라는 새 AWS 계정으로 마이그레이션하려고 합니다. 애플리케이션은 중요한 데이터를 처리하므로 회사는 다운타임을 최소화해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 소스 계정에서 Lambda 함수 배포 패키지를 다운로드합니다. 배포 패키지를 사용하고 Target 계정에서 새 Lambda 함수를 생성합니다. 자동화된 Aurora DB 클러스터 스냅샷을 대상 계정과 공유합니다. B. 소스 계정에서 Lambda 함수 배포 패키지를 다운로드합니다. 배포 패키지를 사용하고 대상 계정에서 새 Lambda 함수를 생성합니다. AWS Resource Access Manager(AWS RAM)를 사용하여 Aurora DB 클러스터를 대상 계정과 공유합니다. 대상 계정에 Aurora DB 클러스터를 복제할 수 있는 권한을 부여합니다. C. AWS Resource Access Manager(AWS RAM)를 사용하여 대상 계정과 Lambda 함수 및 Aurora DB 클러스터를 공유합니다. 대상 계정에 Aurora DB 클러스터를 복제할 수 있는 권한을 부여합니다. D. AWS Resource Access Manager(AWS RAM)를 사용하여 대상 계정과 Lambda 함수를 공유합니다. 자동화된 Aurora DB 클러스터 스냅샷을 대상 계정과 공유합니다. Answer: C Explanation: This solution uses a combination of AWS Resource Access Manager (RAM) and automated backups to migrate the Lambda functions and the Aurora database to the Target account while minimizing downtime. In this solution, the Lambda function deployment package is downloaded from the Source account and used to create new Lambda functions in the Target account. The Aurora DB cluster is shared with the Target account using AWS RAM and the Target account is granted permission to clone the Aurora DB cluster, allowing for a new copy of the Aurora database to be created in the Target account. This approach allows for the data to be migrated to the Target account while minimizing downtime, as the Target account can use the cloned Aurora database while the original Aurora database continues to be used in the Source account.
QUESTION NO: 192 회사는 AWS Organizations의 조직에서 여러 계정을 포함하는 AWS 환경의 비용을 최적화해야 합니다. 이 회사는 3년 전에 비용 최적화 활동을 수행했으며 최근 만료된 Amazon EC2 표준 예약 인스턴스를 구입했습니다. 회사는 3년 더 EC2 인스턴스가 필요합니다. 또한 회사는 새로운 서버리스 워크로드를 배포했습니다. 회사에 가장 많은 비용을 절감할 수 있는 전략은 무엇입니까? A. 전체 선결제로 추가 3년 기간 동안 동일한 예약 인스턴스를 구매합니다. 추가 컴퓨팅 비용을 충당하기 위해 마스터 계정에서 전체 선결제로 3년 컴퓨팅 절약 계획을 구입하십시오. B. 각 멤버 계정에서 선결제가 없는 I-year Compute Savings Plan을 구입하십시오. AWS Cost Management 콘솔에서 Savings Plans 권장 사항을 사용하여 Compute Savings Plan을 선택합니다. C. 각 AWS 지역의 EC2 비용을 충당하기 위해 마스터 계정에서 선결제가 없는 3년 EC2 Instance Savings Plan을 구입합니다. 추가 컴퓨팅 비용을 충당하려면 마스터 계정에서 선결제가 없는 3년 컴퓨팅 절약 계획을 구입하십시오. D. 각 멤버 계정에서 전체 선불 결제가 포함된 3년 EC2 Instance Savings Plan을 구매합니다. AWS Cost Management 콘솔의 Savings Plans 권장 사항을 사용하여 EC2 Instance Savings Plan을 선택합니다. Answer: A Explanation: The company should purchase the same Reserved Instances for an additional 3-year term with All Upfront payment. The company should purchase a 3-year Compute Savings Plan with All Upfront payment in the management account to cover any additional compute costs. This solution will provide the company with the most cost savings because Reserved Instances and Savings Plans are both pricing models that offer significant discounts compared to On-Demand pricing. Reserved Instances are commitments to use a specific instance type and size in a single Region for a one- or three-year term. You can choose between three payment options: No Upfront, Partial Upfront, or All Upfront. The more you pay upfront, the greater the discount1. Savings Plans are flexible pricing models that offer low prices on EC2 instances, Fargate, and Lambda usage, in exchange for a commitment to a consistent amount of usage (measured in $/hour) for a one- or three-year term. You can choose between two types of Savings Plans: Compute Savings Plans and EC2 Instance Savings Plans. Compute Savings Plans apply to any EC2 instance regardless of Region, instance family, operating system, or tenancy, including those that are part of EMR, ECS, or EKS clusters, or launched by Fargate or Lambda. EC2 Instance Savings Plans apply to a specific instance family within a Region and provide the most savings2. By purchasing the same Reserved Instances for an additional 3-year term with All Upfront payment, the company can lock in the lowest possible price for its EC2 instances that run continuously for 3 years. By purchasing a 3-year Compute Savings Plan with All Upfront payment in the management account, the company can benefit from additional discounts on any other compute usage across its member accounts. The other options are not correct because: Purchasing a 1-year Compute Savings Plan with No Upfront payment in each member account would not provide as much cost savings as purchasing a 3-year Compute Savings Plan with All Upfront payment in the management account. A 1-year term offers lower discounts than a 3-year term, and a No Upfront payment option offers lower discounts than an All Upfront payment option. Also, purchasing a Savings Plan in each member account would not allow the company to share the benefits of unused Savings Plan discounts across its organization. Purchasing a 3-year EC2 Instance Savings Plan with No Upfront payment in the management account to cover EC2 costs in each AWS Region would not provide as much cost savings as purchasing Reserved Instances for an additional 3-year term with All Upfront payment. An EC2 Instance Savings Plan offers lower discounts than Reserved Instances for the same instance family and Region. Also, a No Upfront payment option offers lower discounts than an All Upfront payment option. Purchasing a 3-year EC2 Instance Savings Plan with All Upfront payment in each member account would not provide as much flexibility or cost savings as purchasing a 3-year Compute Savings Plan with All Upfront payment in the management account. An EC2 Instance Savings Plan applies only to a specific instance family within a Region and does not cover Fargate or Lambda usage. Also, purchasing a Savings Plan in each member account would not allow the company to share the benefits of unused Savings Plan discounts across its organization. Reference: https://aws.amazon.com/ec2/pricing/reserved-instances/ https://aws.amazon.com/savingsplans/
QUESTION NO: 193 대규모 모바일 게임 회사가 모든 온프레미스 인프라를 AWS 클라우드로 성공적으로 마이그레이션했습니다. 솔루션 설계자는 설계에 따라 구축되었고 Well-Architected 프레임워크에 맞춰 실행되고 있는지 확인하기 위해 환경을 검토하고 있습니다. 비용 탐색기에서 이전 월별 비용을 검토하는 동안 솔루션 설계자는 여러 대형 인스턴스 유형의 생성 및 후속 종료가 비용의 높은 비율을 차지한다는 사실을 알게 됩니다. 솔루션 설계자는 회사의 개발자가 테스트의 일부로 새로운 Amazon EC2 인스턴스를 시작하고 있으며 개발자가 적절한 인스턴스 유형을 사용하고 있지 않음을 알게 됩니다. 솔루션 설계자는 개발자만 시작할 수 있는 인스턴스 유형을 제한하는 제어 메커니즘을 구현해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS Config에서 원하는 인스턴스 유형의 관리형 규칙을 생성합니다. 허용되는 인스턴스 유형으로 규칙을 구성합니다. 새 EC2 인스턴스가 시작될 때마다 실행할 이벤트에 규칙을 연결합니다. B. EC2 콘솔에서 허용되는 인스턴스 유형을 지정하는 시작 템플릿을 생성합니다. 시작 템플릿을 개발자의 IAM 계정에 할당합니다. C. 새 IAM 정책을 만듭니다. 허용되는 인스턴스 유형을 지정합니다. 개발자의 IAM 계정이 포함된 IAM 그룹에 정책을 연결합니다. D. EC2 Image Builder를 사용하여 개발자를 위한 이미지 파이프라인을 생성하고 골든 이미지 생성을 지원합니다. Answer: C Explanation: This is doable with IAM policy creation to restrict users to specific instance types. Found the below article. https://blog.vizuri.com/limiting-allowed-aws-instance-type-with-iam-policy
QUESTION NO: 194 한 회사에서 리전 엔드포인트와 함께 Amazon API Gateway를 사용하는 API를 개발했습니다. API는 API Gateway 인증 메커니즘을 사용하는 AWS Lambda 함수를 호출합니다. 설계 검토 후 솔루션 설계자는 공용 액세스가 필요하지 않은 API 세트를 식별합니다. 솔루션 설계자는 VPC에서만 API 세트에 액세스할 수 있도록 솔루션을 설계해야 합니다. 모든 API는 인증된 사용자로 호출해야 합니다. 최소한의 노력으로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 내부 Application Load Balancer(ALB)를 생성합니다. 대상 그룹을 만듭니다. 호출할 Lambda 함수를 선택합니다. ALB DNS 이름을 사용하여 VPC에서 API를 호출합니다. B. API Gateway에서 API와 연결된 DNS 항목을 제거합니다. Amazon Route 53에서 호스팅 영역을 생성합니다. 호스팅 영역에서 CNAME 레코드를 생성합니다. CNAME 레코드로 API Gateway에서 API를 업데이트합니다. CNAME 레코드를 사용하여 VPC에서 API를 호출합니다. C. API Gateway에서 리전에서 프라이빗으로 API 엔드포인트를 업데이트합니다. VPC에서 인터페이스 VPC 엔드포인트를 생성합니다. 리소스 정책을 생성하고 API에 연결합니다. VPC 엔드포인트를 사용하여 VPC에서 API를 호출합니다. D. VPC 내부에 Lambda 함수를 배포합니다. EC2 인스턴스를 프로비저닝하고 Apache 서버를 설치합니다. Apache 서버에서 Lambda 함수를 호출합니다. EC2 인스턴스의 내부 CNAME 레코드를 사용하여 VPC에서 API를 호출합니다. Answer: C Explanation: This solution requires the least amount of effort as it only requires to update the API endpoint to private in API Gateway and create an interface VPC endpoint. Then create a resource policy and attach it to the API. This will make the API only accessible from the VPC and still keep the authentication mechanism intact. Reference: https://aws.amazon.com/premiumsupport/knowledge-center/private-api-gateway-vpcendpoint/ https://aws.amazon.com/api-gateway/features/
QUESTION NO: 195 건강 보험 회사는 Amazon S3 버킷에 개인 식별 정보(PII)를 저장합니다. 회사는 S3 관리형 암호화 키(SSE-S3)로 서버 측 암호화를 사용하여 개체를 암호화합니다. 새로운 요구 사항에 따라 S3 버킷의 모든 현재 및 향후 개체는 회사의 보안 팀이 관리하는 키로 암호화되어야 합니다. S3 버킷에 버전 관리가 활성화되어 있지 않습니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. S3 버킷 속성에서 기본 암호화를 고객 관리형 키를 사용하는 SSE-S3으로 변경합니다. AWS CLI를 사용하여 S3 버킷의 모든 객체를 다시 업로드합니다. 암호화되지 않은 PutObject 요청을 거부하도록 S3 버킷 정책을 설정합니다. B. S3 버킷 속성에서 기본 암호화를 AWS KMS 관리형 암호화 키(SSE-KMS)를 사용하는 서버 측 암호화로 변경합니다. 암호화되지 않은 PutObject 요청을 거부하도록 S3 버킷 정책을 설정합니다. AWS CLI를 사용하여 S3 버킷의 모든 객체를 다시 업로드합니다. C. S3 버킷 속성에서 기본 암호화를 AWS KMS 관리형 암호화 키(SSE-KMS)를 사용한 서버 측 암호화로 변경합니다. GetObject 및 PutObject 요청에서 객체를 자동으로 암호화하도록 S3 버킷 정책을 설정합니다. D. S3 버킷 속성에서 고객 관리형 키를 사용하여 기본 암호화를 AES-256으로 변경합니다. S3 버킷에 액세스하는 모든 엔터티에 대한 암호화되지 않은 PutObject 요청을 거부하는 정책을 연결합니다. AWS CLI를 사용하여 S3 버킷의 모든 객체를 다시 업로드합니다. Answer: D Explanation: https://docs.aws.amazon.com/AmazonS3/latest/userguide/ServerSideEncryptionCustomerKe ys.html Clearly says we need following header for SSE-C x-amz-server-side-encryptioncustomer-algorithm Use this header to specify the encryption algorithm. The header value must be AES256.
QUESTION NO: 196 회사가 AWS 클라우드에서 애플리케이션을 실행하고 있습니다. 회사의 보안 팀은 모든 새 IAM 사용자 생성을 승인해야 합니다. 새 IAM 사용자가 생성되면 해당 사용자에 대한 모든 액세스 권한이 자동으로 제거되어야 합니다. 그런 다음 보안 팀은 사용자를 승인하라는 알림을 받아야 합니다. 이 회사는 AWS 계정에 다중 리전 AWS CloudTrail 트레일이 있습니다. 이러한 요구 사항을 충족하는 단계 조합은 무엇입니까? (3개를 선택하세요.) A. Amazon EventBridge(Amazon CloudWatch Events) 규칙을 생성합니다. CloudTrail을 통해 AWS API 호출로 설정된 detail-type 값과 CreateUser의 eventName으로 패턴을 정의합니다. B. CreateUser 이벤트에 대한 알림을 Amazon Simple Notification Service(Amazon SNS) 주제로 보내도록 CloudTrail을 구성합니다. C. AWS Fargate 기술을 사용하여 Amazon Elastic Container Service(Amazon ECS)에서 실행되는 컨테이너를 호출하여 액세스 권한을 제거합니다. D. AWS Step Functions 상태 시스템을 호출하여 액세스 권한을 제거합니다. E. Amazon Simple Notification Service(Amazon SNS)를 사용하여 보안 팀에 알립니다. F. Amazon Pinpoint를 사용하여 보안 팀에 알립니다. Answer: A,D,E Explanation: https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/send-a-notification-whenan-iam-user-is-created.html
QUESTION NO: 197 소프트웨어 회사는 Amazon API Gateway를 사용하여 REST API를 사용하는 애플리케이션을 배포했습니다. AWS Lambda 함수 및 Amazon DynamoDB 테이블. 애플리케이션에서 PUT 요청 중 오류 수가 증가한 것으로 나타났습니다. 대부분의 PUT 호출은 특정 API 키로 인증된 소수의 클라이언트에서 발생합니다. 솔루션 설계자는 많은 수의 PUT 요청이 한 클라이언트에서 발생함을 확인했습니다. API는 중요하지 않으며 클라이언트는 실패한 호출의 재시도를 허용할 수 있습니다. 그러나 오류가 고객에게 표시되어 API 평판이 손상됩니다. 솔루션 설계자는 고객 경험을 개선하기 위해 무엇을 추천해야 합니까? A. 클라이언트 응용 프로그램에서 지수 백오프 및 불규칙한 변형으로 재시도 논리를 구현합니다. 설명 오류 메시지로 오류를 포착하고 처리하는지 확인하십시오. B. API 게이트웨이 수준에서 사용량 계획을 통해 API 조절을 구현합니다. 클라이언트 애플리케이션이 오류 없이 코드 429 응답을 처리하는지 확인하십시오. C. API 캐싱을 켜서 프로덕션 단계에 대한 응답성을 높입니다. 10분 부하 테스트를 실행합니다. 캐시 용량이 워크로드에 적합한지 확인합니다. D. Lambda 기능 수준에서 예약된 동시성을 구현하여 트래픽이 갑자기 증가하는 동안 필요한 리소스를 제공합니다. Answer: B Explanation: https://aws.amazon.com/premiumsupport/knowledge-center/aws-batch-requests-error/ https://aws.amazon.com/premiumsupport/knowledge-center/api-gateway-429-limit/
QUESTION NO: 198 회사에는 잠재적 임차인과 구매자에게 부동산 정보를 제공하는 온프레미스 웹 사이트 애플리케이션이 있습니다. 웹 사이트는 Java 백엔드와 NOSQL MongoDB 데이터베이스를 사용하여 구독자 데이터를 저장합니다. 회사는 전체 애플리케이션을 비슷한 구조의 AWS로 마이그레이션해야 합니다. 응용 프로그램은 고가용성을 위해 배포되어야 하며 회사는 응용 프로그램을 변경할 수 없습니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까? A. Amazon Aurora DB 클러스터를 구독자 데이터의 데이터베이스로 사용합니다. Java 백엔드 애플리케이션을 위해 여러 가용 영역에 걸쳐 Auto Scaling 그룹에 Amazon EC2 인스턴스를 배포합니다. B. Amazon EC2 인스턴스의 MongoDB를 구독자 데이터의 데이터베이스로 사용합니다. Java 백엔드 애플리케이션을 위한 단일 가용 영역의 Auto Scaling 그룹에 EC2 인스턴스를 배포합니다. C. 구독자 데이터용 데이터베이스로 여러 가용 영역에서 적절한 크기의 인스턴스로 Amazon DocumentD3(MongoDB 호환)를 구성합니다. Java 백엔드 애플리케이션을 위해 여러 가용 영역에 걸쳐 Auto Scaling 그룹에 Amazon EC2 인스턴스를 배포합니다. D. 여러 가용 영역에서 온디맨드 용량 모드로 Amazon DocumentDB(MongoDB와 호환)를 구독자 데이터용 데이터베이스로 구성합니다. Java 백엔드 애플리케이션을 위해 여러 가용 영역에 걸쳐 Auto Scaling 그룹에 Amazon EC2 인스턴스를 배포합니다. Answer: C Explanation: On-demand capacity mode is the function of Dynamodb. https://aws.amazon.com/blogs/news/running-spiky-workloads-and-optimizing-costs-by-morethan-90-using-amazon-dynamodb-on-demand-capacity-mode/ Amazon DocumentDB Elastic Clusters https://aws.amazon.com/blogs/news/announcing-amazon-documentdb-elasticclusters/ Deploy Amazon EC2 instances in an Auto Scaling group across multiple Availability Zones for the Java backend application. This will provide high availability and scalability, while allowing the company to retain the same database structure as the original application.
QUESTION NO: 199 회사는 개발 환경의 단일 Windows Amazon EC2 인스턴스에서 콘텐츠 관리 애플리케이션을 실행합니다. 애플리케이션은 인스턴스에 루트 디바이스로 연결된 2TB Amazon Elastic Block Store(Amazon EBS) 볼륨에 정적 콘텐츠를 읽고 씁니다. 이 회사는 이 애플리케이션을 여러 가용 영역에 걸쳐 최소 3개의 EC2 인스턴스에서 실행되는 고가용성 및 내결함성 솔루션으로 프로덕션 환경에 배포할 계획입니다. 솔루션 설계자는 응용 프로그램을 실행하는 모든 인스턴스를 Active Directory 도메인에 연결하는 솔루션을 설계해야 합니다. 솔루션은 또한 Windows ACL을 구현하여 파일 콘텐츠에 대한 액세스를 제어해야 합니다. 애플리케이션은 항상 특정 시점에 실행 중인 모든 인스턴스에서 정확히 동일한 콘텐츠를 유지해야 합니다. 최소한의 관리 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. Amazon Elastic File System(Amazon EFS) 파일 공유를 생성합니다. 3개의 가용 영역에 걸쳐 확장되고 최소 3개의 인스턴스 크기를 유지하는 Auto Scaling 그룹을 생성합니다. 사용자 데이터 스크립트를 구현하여 애플리케이션을 설치하고, 인스턴스를 AD 도메인에 가입하고, EFS 파일 공유를 마운트합니다. B. 실행 중인 현재 EC2 인스턴스에서 새 AMI를 생성합니다. Lustre용 Amazon FSx 파일 시스템을 생성합니다. 3개의 가용 영역에 걸쳐 확장되고 최소 3개의 인스턴스 크기를 유지하는 Auto Scaling 그룹을 생성합니다. 사용자 데이터 스크립트를 구현하여 인스턴스를 AD 도메인에 가입시키고 FSx for Lustre 파일 시스템을 마운트합니다. C. Windows 파일 서버용 Amazon FSx 파일 시스템을 생성합니다. 3개의 가용 영역에 걸쳐 확장되고 최소 3개의 인스턴스 크기를 유지하는 Auto Scaling 그룹을 생성합니다. 사용자 데이터 스크립트를 구현하여 애플리케이션을 설치하고 FSx for Windows File Server 파일 시스템을 마운트합니다. 원활한 도메인 가입을 수행하여 인스턴스를 AD 도메인에 가입합니다. D. 실행 중인 현재 EC2 인스턴스에서 새 AMI를 생성합니다. Amazon Elastic File System(Amazon EFS) 파일 시스템을 생성합니다. 3개의 가용 영역에 걸쳐 확장되고 최소 3개의 인스턴스 크기를 유지하는 Auto Scaling 그룹을 생성합니다. 원활한 도메인 가입을 수행하여 인스턴스를 AD 도메인에 가입합니다. Answer: C Explanation: https://docs.aws.amazon.com/fsx/latest/WindowsGuide/what-is.html https://docs.aws.amazon.com/directoryservice/latest/admin-guide/ms_ad_join_instance.html
QUESTION NO: 200 금융 서비스 회사는 신용 카드 서비스 파트너로부터 정기적인 데이터 피드를 받습니다. 약 5,000개의 레코드가 15분마다 일반 텍스트로 전송되고 HTTPS를 통해 서버 측 암호화를 통해 Amazon S3 버킷으로 직접 전달됩니다. 이 피드에는 민감한 신용 카드 기본 계정 번호(PAN) 데이터가 포함되어 있습니다. 회사는 추가 내부 처리를 위해 데이터를 다른 S3 버킷으로 보내기 전에 자동으로 PAN을 마스킹해야 합니다. 회사는 또한 특정 필드를 제거 및 병합한 다음 레코드를 JSON 형식으로 변환해야 합니다. 또한 향후 추가 피드가 추가될 가능성이 있으므로 모든 디자인을 쉽게 확장할 수 있어야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 파일 전송 시 각 레코드를 추출하여 Amazon SQS 대기열에 쓰는 AWS Lambda 함수를 트리거합니다. 레코드를 처리하기 위해 새 메시지가 SQS 대기열에 도착하면 다른 Lambda 함수를 트리거하여 결과를 Amazon S3의 임시 위치에 기록합니다. SQS 대기열이 비어 있으면 최종 Lambda 함수를 트리거하여 레코드를 JSON 형식으로 변환하고 내부 처리를 위해 결과를 다른 S3 버킷으로 보냅니다. B. 파일 전송 시 각 레코드를 추출하여 Amazon SQS 대기열에 쓰는 AWS Lambda 함수를 트리거합니다. SQS 대기열에 메시지가 포함된 경우 단일 인스턴스로 자동 확장되도록 AWS Fargate 컨테이너 애플리케이션을 구성합니다. 애플리케이션이 각 레코드를 처리하고 레코드를 JSON 형식으로 변환하도록 합니다. 대기열이 비어 있으면 내부 처리를 위해 결과를 다른 S3 버킷으로 보내고 AWS Fargate 인스턴스를 축소합니다. C. 데이터 피드 형식을 기반으로 AWS Glue 크롤러 및 사용자 지정 분류자를 생성하고 일치시킬 테이블 정의를 구축합니다. 파일 전송 시 AWS Lambda 함수를 트리거하여 AWS Glue ETL 작업을 시작하여 처리 및 변환 요구 사항에 따라 전체 레코드를 변환합니다. 출력 형식을 JSON으로 정의합니다. 완료되면 ETL 작업이 내부 처리를 위해 결과를 다른 S3 버킷으로 보내도록 합니다. D. 데이터 피드 형식을 기반으로 AWS Glue 크롤러 및 사용자 지정 분류자를 생성하고 일치시킬 테이블 정의를 구축합니다. 파일 전송에 대해 Amazon Athena 쿼리를 수행하여 처리 및 변환 요구 사항에 따라 전체 레코드를 변환하는 Amazon EMR ETL 작업을 시작합니다. 출력 형식을 JSON으로 정의합니다. 완료되면 내부 처리를 위해 결과를 다른 S3 버킷으로 보내고 EMR 클러스터를 축소합니다. Answer: C Explanation: You can use a Glue crawler to populate the AWS Glue Data Catalog with tables. The Lambda function can be triggered using S3 event notifications when object create events occur. The Lambda function will then trigger the Glue ETL job to transform the records masking the sensitive data and modifying the output format to JSON. This solution meets all requirements.
QUESTION NO: 201 회사는 AWS CodeCommit 리포지토리를 사용합니다. 회사는 두 번째 AWS 리전에 있는 리포지토리에 있는 데이터의 백업 복사본을 저장해야 합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까? A. CodeCommit 리포지토리 데이터를 두 번째 리전에 복제하도록 AWS Elastic Disaster Recovery 구성 B. AWS Backup을 사용하여 매시간 일정에 따라 CodeCommit 리포지토리를 백업합니다. 두 번째 리전에서 교차 리전 복사본을 생성합니다. C. 회사가 코드를 리포지토리로 푸시할 때 AWS CodeBuild를 호출하는 Amazon EventBridge 규칙을 생성합니다. CodeBuild를 사용하여 리포지토리를 복제합니다. 콘텐츠의 zip 파일을 생성합니다. 파일을 두 번째 리전의 S3 버킷에 복사합니다. D. 시간별 일정에 따라 AWS Step Functions 워크플로를 생성하여 CodeCommit 리포지토리의 스냅샷을 생성합니다. 두 번째 리전의 S3 버킷에 스냅샷을 복사하도록 워크플로를 구성합니다. Answer: B Explanation: AWS Backup is a fully managed service that makes it easy to centralize and automate the creation, retention, and restoration of backups across AWS services. It provides a way to schedule automatic backups for CodeCommit repositories on an hourly basis. Additionally, it also supports cross-Region replication, which allows you to copy the backups to a second Region for disaster recovery. By using AWS Backup, the company can set up an automatic and regular backup schedule for the CodeCommit repository, ensuring that the data is regularly backed up and stored in a second Region. This can provide a way to recover quickly from any disaster event that might occur. Reference: AWS Backup documentation: https://aws.amazon.com/backup/ AWS Backup for AWS CodeCommit documentation: https://aws.amazon.com/aboutaws/ whats-new/2020/07/aws-backup-now-supports-aws-codecommit-repositories/
QUESTION NO: 202 회사에서 비즈니스 크리티컬 애플리케이션을 온프레미스 데이터 센터에서 AWS로 마이그레이션할 계획입니다. 회사에는 Microsoft SQL Server Always On 클러스터의 온프레미스 설치가 있습니다. 회사는 AWS 관리형 데이터베이스 서비스로 마이그레이션하려고 합니다. 솔루션 설계자는 AWS에서 이기종 데이터베이스 마이그레이션을 설계해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 백업 및 복원 유틸리티를 사용하여 SQL Server 데이터베이스를 MySQL용 Amazon RDS로 마이그레이션합니다. B. AWS Snowball Edge Storage Optimized 장치를 사용하여 데이터를 Amazon S3로 전송합니다. MySQL용 Amazon RDS를 설정합니다. BULK INSERT와 같은 SQL Server 기능과 S3 통합을 사용합니다. C. AWS Schema Conversion Tool을 사용하여 데이터베이스 스키마를 MeSQL용 Amazon RDS로 변환합니다. 그런 다음 AWS Database Migration Service(AWS DMS)를 사용하여 온프레미스 데이터베이스에서 Amazon RDS로 데이터를 마이그레이션합니다. D. AWS DataSync를 사용하여 온프레미스 스토리지와 Amazon S3 간에 네트워크를 통해 데이터를 마이그레이션합니다. MySQL용 Amazon RDS를 설정합니다. BULK INSERT와 같은 SQL Server 기능과 S3 통합을 사용합니다. Answer: C Explanation: https://aws.amazon.com/dms/schema-conversion-tool/ AWS Schema Conversion Tool (SCT) can automatically convert the database schema from Microsoft SQL Server to Amazon RDS for MySQL. This allows for a smooth transition of the database schema without any manual intervention. AWS DMS can then be used to migrate the data from the on-premises databases to the newly created Amazon RDS for MySQL instance. This service can perform a one-time migration of the data or can set up ongoing replication of data changes to keep the on-premises and AWS databases in sync.
QUESTION NO: 203 회사의 대화형 웹 애플리케이션은 Amazon CloudFront 배포를 사용하여 Amazon S3 버킷의 이미지를 제공합니다. 경우에 따라 타사 도구가 손상된 이미지를 S3 버킷으로 수집합니다. 이 이미지 손상으로 인해 나중에 애플리케이션에서 사용자 경험이 저하됩니다. 이 회사는 손상된 이미지를 감지하기 위해 Python 논리를 성공적으로 구현하고 테스트했습니다. 솔루션 설계자는 수집과 제공 사이의 대기 시간을 최소화하면서 탐지 논리를 통합하는 솔루션을 추천해야 합니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 뷰어 응답 이벤트에 의해 호출되는 Lambda@Edge 함수를 사용합니다. B. 원본 응답 이벤트에 의해 호출되는 Lambda@Edge 함수를 사용합니다. C. AWS Lambda 함수를 호출하는 S3 이벤트 알림을 사용합니다. D. AWS Step Functions 상태 시스템을 호출하는 S3 이벤트 알림을 사용합니다. Answer: B Explanation: This solution will allow the detection logic to be run as soon as the image is uploaded to the S3 bucket, before it is served to users via the CloudFront distribution. This way, the detection logic can quickly identify any corrupted images and prevent them from being served to users, minimizing latency between ingestion and serving.
QUESTION NO: 204 회사는 온프레미스 데이터 센터에서 애플리케이션을 실행합니다. 이 애플리케이션은 사용자에게 미디어 파일을 업로드할 수 있는 기능을 제공합니다. 파일은 파일 서버에 유지됩니다. 웹 애플리케이션에는 많은 사용자가 있습니다. 응용 프로그램 서버가 과도하게 사용되어 때때로 데이터 업로드가 실패합니다. 회사는 파일 서버에 새 스토리지를 자주 추가합니다. 회사는 애플리케이션을 AWS로 마이그레이션하여 이러한 문제를 해결하고자 합니다. 미국과 캐나다 전역의 사용자가 애플리케이션에 액세스합니다. 인증된 사용자만 애플리케이션에 액세스하여 파일을 업로드할 수 있어야 합니다. 회사는 애플리케이션을 리팩터링하는 솔루션을 고려할 것이며 회사는 애플리케이션 개발을 가속화해야 합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS Application Migration Service를 사용하여 애플리케이션 서버를 Amazon EC2 인스턴스로 마이그레이션합니다. EC2 인스턴스에 대한 Auto Scaling 그룹을 생성합니다. Application Load Balancer를 사용하여 요청을 분산합니다. Amazon S3를 사용하여 파일을 유지하도록 애플리케이션을 수정합니다. Amazon Cognito를 사용하여 사용자를 인증합니다. B. AWS Application Migration Service를 사용하여 애플리케이션 서버를 Amazon EC2 인스턴스로 마이그레이션합니다. EC2 인스턴스에 대한 Auto Scaling 그룹을 생성합니다. Application Load Balancer를 사용하여 요청을 분산합니다. 사용자가 애플리케이션에 로그인할 수 있도록 AWS IAM Identity Center(AWS Single Sign-On)를 설정합니다. Amazon S3를 사용하여 파일을 유지하도록 애플리케이션을 수정합니다. C. 미디어 파일 업로드를 위한 정적 웹사이트를 만듭니다. 정적 자산을 Amazon S3에 저장합니다. AWS AppSync를 사용하여 API를 생성합니다. AWS Lambda 확인자를 사용하여 미디어 파일을 Amazon S3에 업로드합니다. Amazon Cognito를 사용하여 사용자를 인증합니다. D. AWS Amplify를 사용하여 미디어 파일 업로드를 위한 정적 웹사이트를 생성합니다. Amplify Hosting을 사용하여 Amazon CloudFront를 통해 웹 사이트를 제공합니다. Amazon S3를 사용하여 업로드된 미디어 파일을 저장합니다. Amazon Cognito를 사용하여 사용자를 인증합니다. Answer: D Explanation: The company should use AWS Amplify to create a static website for uploads of media files. The company should use Amplify Hosting to serve the website through Amazon CloudFront. The company should use Amazon S3 to store the uploaded media files. The company should use Amazon Cognito to authenticate users. This solution will meet the requirements with the least operational overhead because AWS Amplify is a complete solution that lets frontend web and mobile developers easily build, ship, and host full-stack applications on AWS, with the flexibility to leverage the breadth of AWS services as use cases evolve. No cloud expertise needed1. By using AWS Amplify, the company can refactor the application to a serverless architecture that reduces operational complexity and costs. AWS Amplify offers the following features and benefits: Amplify Studio: A visual interface that enables you to build and deploy a full-stack app quickly, including frontend UI and backend. Amplify CLI: A local toolchain that enables you to configure and manage an app backend with just a few commands. Amplify Libraries: Open-source client libraries that enable you to build cloud-powered mobile and web apps. Amplify UI Components: Open-source design system with cloud-connected components for building feature-rich apps fast. Amplify Hosting: Fully managed CI/CD and hosting for fast, secure, and reliable static and server-side rendered apps. By using AWS Amplify to create a static website for uploads of media files, the company can leverage Amplify Studio to visually build a pixel-perfect UI and connect it to a cloud backend in clicks. By using Amplify Hosting to serve the website through Amazon CloudFront, the company can easily deploy its web app or website to the fast, secure, and reliable AWS content delivery network (CDN), with hundreds of points of presence globally. By using Amazon S3 to store the uploaded media files, the company can benefit from a highly scalable, durable, and cost-effective object storage service that can handle any amount of data2. By using Amazon Cognito to authenticate users, the company can add user sign-up, sign-in, and access control to its web app with a fully managed service that scales to support millions of users3. The other options are not correct because: Using AWS Application Migration Service to migrate the application server to Amazon EC2 instances would not refactor the application or accelerate development. AWS Application Migration Service (AWS MGN) is a service that enables you to migrate physical servers, virtual machines (VMs), or cloud servers from any source infrastructure to AWS without requiring agents or specialized tools. However, this would not address the challenges of overutilization and data uploads failures. It would also not reduce operational overhead or costs compared to a serverless architecture. Creating a static website for uploads of media files and using AWS AppSync to create an API would not be as simple or fast as using AWS Amplify. AWS AppSync is a service that enables you to create flexible APIs for securely accessing, manipulating, and combining data from one or more data sources. However, this would require more configuration and management than using Amplify Studio and Amplify Hosting. It would also not provide authentication features like Amazon Cognito. Setting up AWS IAM Identity Center (AWS Single Sign-On) to give users the ability to sign in to the application would not be as suitable as using Amazon Cognito. AWS Single Sign-On (AWS SSO) is a service that enables you to centrally manage SSO access and user permissions across multiple AWS accounts and business applications. However, this service is designed for enterprise customers who need to manage access for employees or partners across multiple resources. It is not intended for authenticating end users of web or mobile apps. Reference: https://aws.amazon.com/amplify/ https://aws.amazon.com/s3/ https://aws.amazon.com/cognito/ https://aws.amazon.com/mgn/ https://aws.amazon.com/appsync/ https://aws.amazon.com/single-sign-on/
QUESTION NO: 205 솔루션 설계자는 회사가 온프레미스에서 호스팅하는 3계층 애플리케이션을 재설계하고 있습니다. 이 응용 프로그램은 사용자 프로필을 기반으로 개인화 된 권장 사항을 제공합니다. 회사에는 이미 AWS 계정이 있으며 애플리케이션을 호스팅하도록 VPC를 구성했습니다. 프런트엔드는 온프레미스 VM에서 실행되는 Java 기반 애플리케이션입니다. 회사는 물리적 애플리케이션 서버에서 개인화 모델을 호스팅하고 TensorFlow를 사용하여 모델을 구현합니다. 개인화 모델은 인공 지능 및 기계 학습(AI/ML)을 사용합니다. 회사는 사용자 정보를 Microsoft SQL Server 데이터베이스에 저장합니다. 웹 애플리케이션은 데이터베이스에서 사용자 프로필을 읽고 권장 사항을 제공하는 개인화 모델을 호출합니다. 회사는 재설계된 애플리케이션을 AWS로 마이그레이션하려고 합니다. 최소한의 운영 오버헤드로 이 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS Server Migration Service(AWS SMS)를 사용하여 온프레미스 물리적 애플리케이션 서버와 웹 애플리케이션 VM을 AWS로 마이그레이션합니다. AWS Database Migration Service(AWS DMS)를 사용하여 SQL Server 데이터베이스를 SQL Server용 Amazon RDS로 마이그레이션합니다. B. 개인화 모델을 내보냅니다. 모델 아티팩트를 Amazon S3에 저장합니다. 모델을 Amazon SageMaker에 배포하고 엔드포인트를 생성합니다. AWS Elastic Beanstalk에서 Java 애플리케이션을 호스팅합니다. AWS Database Migration Service(AWS DMS)를 사용하여 SQL Server 데이터베이스를 SQL Server용 Amazon RDS로 마이그레이션합니다. C. AWS Application Migration Service를 사용하여 온프레미스 개인화 모델 및 VM을 Auto Scaling 그룹의 Amazon EC2 인스턴스로 마이그레이션합니다. AWS Database Migration Service(AWS DMS)를 사용하여 SQL Server 데이터베이스를 EC2 인스턴스로 마이그레이션합니다. D. 개인화 모델 및 Java 애플리케이션을 컨테이너화합니다. Amazon Elastic Kubernetes Service(Amazon EKS) 관리형 노드 그룹을 사용하여 모델 및 애플리케이션을 Amazon EKS에 배포합니다. VPC에서 노드 그룹을 호스팅합니다. AWS Database Migration Service(AWS DMS)를 사용하여 SQL Server 데이터베이스를 SQL Server용 Amazon RDS로 마이그레이션합니다. Answer: B Explanation: Amazon SageMaker is a fully managed machine learning service that allows users to build, train, and deploy machine learning models quickly and easily1. Users can export their existing TensorFlow models and store the model artifacts in Amazon S3, a highly scalable and durable object storage service2. Users can then deploy the model to Amazon SageMaker and create an endpoint that can be invoked by the web application to provide recommendations3. This way, the solution can leverage the AI/ML capabilities of Amazon SageMaker without having to rewrite the personalization model. AWS Elastic Beanstalk is a service that allows users to deploy and manage web applications without worrying about the infrastructure that runs those applications. Users can host their Java application in AWS Elastic Beanstalk and configure it to communicate with the Amazon SageMaker endpoint. This way, the solution can reduce the operational overhead of managing servers, load balancers, scaling, and application health monitoring. AWS Database Migration Service (AWS DMS) is a service that helps users migrate databases to AWS quickly and securely. Users can use AWS DMS to migrate their SQL Server database to Amazon RDS for SQL Server, a fully managed relational database service that offers high availability, scalability, security, and compatibility. This way, the solution can reduce the operational overhead of managing database servers, backups, patches, and upgrades. Option A is incorrect because using AWS Server Migration Service (AWS SMS) to migrate the on-premises physical application server and the web application VMs to AWS is not costeffective or scalable. AWS SMS is a service that helps users migrate on-premises workloads to AWS. However, for this use case, migrating the physical application server and the web application VMs to AWS will not take advantage of the AI/ML capabilities of Amazon SageMaker or the managed services of AWS Elastic Beanstalk and Amazon RDS. Option C is incorrect because using AWS Application Migration Service to migrate the onpremises personalization model and VMs to Amazon EC2 instances in Auto Scaling groups is not cost-effective or scalable. AWS Application Migration Service is a service that helps users migrate applications from on-premises or other clouds to AWS without making any changes to their applications. However, for this use case, migrating the personalization model and VMs to EC2 instances will not take advantage of the AI/ML capabilities of Amazon SageMaker or the managed services of AWS Elastic Beanstalk and Amazon RDS. Option D is incorrect because containerizing the personalization model and the Java application and using Amazon Elastic Kubernetes Service (Amazon EKS) managed node groups to deploy them to Amazon EKS is not necessary or cost-effective. Amazon EKS is a service that allows users to run Kubernetes on AWS without needing to install, operate, and maintain their own Kubernetes control plane or nodes. However, for this use case, containerizing and deploying the personalization model and the Java application will not take advantage of the AI/ML capabilities of Amazon SageMaker or the managed services of AWS Elastic Beanstalk. Moreover, using S3 Glacier Deep Archive as a storage class for images will incur a high retrieval fee and latency for accessing them.
QUESTION NO: 206 회사는 Amazon EC2 인스턴스 플릿에서 프록시 서버를 운영합니다. 다른 국가의 파트너는 프록시 서버를 사용하여 회사의 기능을 테스트합니다. EC2 인스턴스는 VPC에서 실행 중입니다. 인스턴스는 인터넷에 액세스할 수 있습니다. 회사의 보안 정책에 따라 파트너는 회사가 소유한 도메인에서만 리소스에 액세스할 수 있습니다. 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 허용된 도메인이 포함된 Amazon Route 53 Resolver DNS 방화벽 도메인 목록을 생성합니다. 모든 요청을 차단하는 높은 숫자 값을 가진 규칙으로 DNS 방화벽 규칙 그룹을 구성합니다. 허용 목록의 도메인에 대한 요청을 허용하는 낮은 숫자 값을 갖는 규칙을 구성합니다. 규칙 그룹을 VPC와 연결합니다. B. 허용된 도메인이 포함된 Amazon Route 53 Resolver DNS 방화벽 도메인 목록을 생성합니다. Route 53 아웃바운드 엔드포인트를 구성합니다. 아웃바운드 엔드포인트를 VPC와 연결합니다. 도메인 목록을 아웃바운드 엔드포인트와 연결합니다. C. 허용된 도메인과 일치하도록 Amazon Route 53 트래픽 흐름 정책을 생성합니다. Route 53 Resolver와 일치하는 요청을 전달하도록 트래픽 흐름 정책을 구성합니다. 트래픽 흐름 정책을 VPC와 연결합니다. D. Amazon Route 53 아웃바운드 엔드포인트를 생성합니다. 아웃바운드 엔드포인트를 VPC와 연결합니다. 허용된 도메인에 대한 요청을 아웃바운드 엔드포인트로 전달하도록 Route 53 트래픽 흐름 정책을 구성합니다. 트래픽 흐름 정책을 VPC와 연결합니다. Answer: A Explanation: The company should create an Amazon Route 53 Resolver DNS Firewall domain list that contains the allowed domains. The company should configure a DNS Firewall rule group with a rule that has a high numeric value that blocks all requests. The company should configure a rule that has a low numeric value that allows requests for domains in the allowed list. The company should associate the rule group with the VPC. This solution will meet the requirements because Amazon Route 53 Resolver DNS Firewall is a feature that enables you to filter and regulate outbound DNS traffic for your VPC. You can create reusable collections of filtering rules in DNS Firewall rule groups and associate them with your VPCs. You can specify lists of domain names to allow or block, and you can customize the responses for the DNS queries that you block1. By creating a domain list with the allowed domains and a rule group with rules to allow or block requests based on the domain list, the company can enforce its security policy and control access to sites. The other options are not correct because: Configuring a Route 53 outbound endpoint and associating it with the VPC would not help with filtering outbound DNS traffic. A Route 53 outbound endpoint is a resource that enables you to forward DNS queries from your VPC to your network over AWS Direct Connect or VPN connections2. It does not provide any filtering capabilities. Creating a Route 53 traffic flow policy to match the allowed domains would not help with filtering outbound DNS traffic. A Route 53 traffic flow policy is a resource that enables you to route traffic based on multiple criteria, such as endpoint health, geographic location, and latency3. It does not provide any filtering capabilities. Creating a Gateway Load Balancer (GWLB) would not help with filtering outbound DNS traffic. A GWLB is a service that enables you to deploy, scale, and manage third-party virtual appliances such as firewalls, intrusion detection and prevention systems, and deep packet inspection systems in the cloud4. It does not provide any filtering capabilities. Reference: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-dns-firewall.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/resolver-outboundendpoints.html https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/traffic-flow.html https://docs.aws.amazon.com/elasticloadbalancing/latest/gateway/introduction.html
QUESTION NO: 207 회사의 솔루션 아키텍트가 AWS에서 실행되는 웹 애플리케이션을 검토하고 있습니다. 애플리케이션은 us-east-1 리전의 Amazon S3 버킷에 있는 정적 자산을 참조합니다. 회사는 여러 AWS 리전에서 복원력이 필요합니다. 회사는 이미 두 번째 리전에 S3 버킷을 생성했습니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 각 객체를 두 S3 버킷에 쓰도록 애플리케이션을 구성합니다. 각 S3 버킷에 대한 가중 라우팅 정책을 사용하여 레코드 세트로 Amazon Route 53 퍼블릭 호스팅 영역을 설정합니다. Route 53 DNS 이름을 사용하여 객체를 참조하도록 애플리케이션을 구성합니다. B. us-east-1의 S3 버킷에서 두 번째 리전의 S3 버킷으로 객체를 복사하는 AWS Lambda 함수를 생성합니다. 객체가 us-east-1의 S3 버킷에 기록될 때마다 Lambda 함수를 호출합니다. 두 개의 S3 버킷을 오리진으로 포함하는 오리진 그룹으로 Amazon CloudFront 배포를 설정합니다. C. 객체를 두 번째 리전의 S3 버킷에 복제하도록 us-east-1의 S3 버킷에서 복제를 구성합니다. 2개의 S3 버킷을 오리진으로 포함하는 오리진 그룹으로 Amazon CloudFront 배포를 설정합니다. D. 두 번째 리전의 S3 버킷에 개체를 복제하도록 us-east-1의 S3 버킷에서 복제를 구성합니다. 장애 조치가 필요한 경우 두 번째 리전의 S3 버킷에서 S3 객체를 로드하도록 애플리케이션 코드를 업데이트합니다. Answer: C Explanation: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/high_availability_ori gin_failover.html
QUESTION NO: 208 날씨 서비스는 eu-west-1 리전의 AWS에서 호스팅되는 웹 애플리케이션의 고해상도 날씨 지도를 제공합니다. 날씨 지도는 자주 업데이트되며 정적 HTML 콘텐츠와 함께 Amazon S3에 저장됩니다. 웹 애플리케이션은 Amazon CloudFront가 전면에 있습니다. 이 회사는 최근 us-east-1 지역의 사용자에게 서비스를 제공하도록 확장했으며 이러한 신규 사용자는 각각의 날씨 지도 보기가 때때로 느리다고 보고합니다. us-east-1 성능 문제를 해결하는 단계 조합은 무엇입니까? (2개 선택하세요.) A. eu-west-1에서 S3 버킷에 대한 AWS Global Accelerator 엔드포인트를 구성합니다. useast-1에서 TCP 포트 80 및 443에 대한 엔드포인트 그룹을 구성합니다. B. us-east-1에 새 S3 버킷을 생성합니다. eu-west-1의 S3 버킷에서 동기화하도록 S3 교차 리전 복제를 구성합니다. C. Lambda@Edge를 사용하여 us-east-1에서 S3 Transfer Acceleration 엔드포인트를 사용하도록 북미에서 요청을 수정합니다. D. Lambda@Edge를 사용하여 북미에서 us-east-1의 S3 버킷을 사용하도록 요청을 수정합니다. E. us-east-1에 대한 AWS Global Accelerator 엔드포인트를 CloudFront 배포의 오리진으로 구성합니다. Lambda@Edge를 사용하여 북미에서 새 오리진을 사용하도록 요청을 수정합니다. Answer: B,D Explanation: https://aws.amazon.com/about-aws/whats-new/2016/04/transfer-files-into-amazon-s3-up-to-300-percent-faster/
QUESTION NO: 209 팀이 전체 회사의 행동 데이터를 수집하고 라우팅합니다. 회사는 퍼블릭 서브넷, 프라이빗 서브넷 및 인터넷 게이트웨이가 있는 다중 AZ VPC 환경을 실행합니다. 각 퍼블릭 서브넷에는 NAT 게이트웨이도 포함되어 있습니다. 대부분의 회사 애플리케이션은 Amazon에서 읽고 씁니다. Kinesis 데이터 스트림. 대부분의 워크로드는 프라이빗 서브넷에 있습니다. 솔루션 설계자는 인프라를 검토해야 합니다. 솔루션 설계자는 비용을 절감하고 애플리케이션의 기능을 유지해야 합니다. 솔루션 설계자는 Cost Explorer를 사용하고 EC2- Other 범주의 비용이 지속적으로 높다는 것을 알게 됩니다. 추가 검토를 통해 NatGateway-Bytes가 요금을 부과한다는 것을 알 수 있습니다. EC2-기타 범주의 비용이 증가하고 있습니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 무엇을 해야 합니까? A. VPC 흐름 로그를 활성화합니다. Amazon Athena를 사용하여 제거할 수 있는 트래픽에 대한 로그를 분석합니다. 보안 그룹이 높은 비용을 초래하는 모킹 트래픽인지 확인합니다. B. VPC에 Kinesis Data Streams용 인터페이스 VPC 엔드포인트를 추가합니다. 애플리케이션에 인터페이스 VPC 엔드포인트를 사용할 수 있는 올바른 IAM 권한이 있는지 확인하십시오. C. Kinesis Data Streams와 관련되지 않은 트래픽에 대해 VPC 흐름 로그 및 Amazon Detective Review Detective 결과를 활성화합니다. 해당 트래픽을 차단하도록 보안 그룹을 구성합니다. D. Kinesis Data Streams용 인터페이스 VPC 엔드포인트를 VPC에 추가합니다. VPC 엔드포인트 정책이 애플리케이션의 트래픽을 허용하는지 확인하십시오. Answer: D Explanation: https://docs.aws.amazon.com/vpc/latest/privatelink/vpc-endpoints-access.html https://aws.amazon.com/premiumsupport/knowledge-center/vpc-reduce-nat-gatewaytransfer-costs/ VPC endpoint policies enable you to control access by either attaching a policy to a VPC endpoint or by using additional fields in a policy that is attached to an IAM user, group, or role to restrict access to only occur via the specified VPC endpoint
QUESTION NO: 210 회사는 서버에 대한 패치 프로세스를 구현해야 합니다. 온프레미스 서버와 Amazon EC2 인스턴스는 다양한 도구를 사용하여 패치를 수행합니다. 관리에는 모든 서버 및 인스턴스의 패치 상태를 보여주는 단일 보고서가 필요합니다. 이러한 요구 사항을 충족하기 위해 솔루션 설계자는 어떤 조치를 취해야 합니까? A. AWS Systems Manager를 사용하여 온프레미스 서버 및 EC2 인스턴스에서 패치를 관리합니다. Systems Manager를 사용하여 패치 규정 준수 보고서를 생성합니다. B. AWS OpsWorks를 사용하여 온프레미스 서버 및 EC2 인스턴스에서 패치를 관리합니다. OpsWorks와 Amazon OuickSight 통합을 사용하여 패치 규정 준수 보고서를 생성합니다. C. Amazon EventBridge(Amazon CloudWatch Events) 규칙을 사용하여 AWS Systems Manager 패치 수정 작업을 예약하여 패치를 적용합니다. Amazon Inspector를 사용하여 패치 규정 준수 보고서를 생성하십시오. D. AWS OpsWorks를 사용하여 온프레미스 서버 및 EC2 인스턴스에서 패치를 관리합니다. AWS X-Ray를 사용하여 패치 상태를 AWS Systems Manager OpsCenter에 게시하여 패치 규정 준수 보고서를 생성합니다. Answer: A Explanation: https://docs.aws.amazon.com/systems-manager/latest/userguide/systems-managerpatch.html
QUESTION NO: 211 회사는 인터넷 연결 Application Load Balancer(ALB) 뒤에 있는 프라이빗 서브넷에 있는 Amazon EC2 인스턴스 플릿에서 애플리케이션을 실행합니다. ALB는 Amazon CloudFront 배포의 오리진입니다. 다양한 AWS 관리형 규칙을 포함하는 AWS WAF 웹 ACL은 CloudFront 배포와 연결됩니다. 회사는 인터넷 트래픽이 ALB에 직접 액세스하지 못하도록 하는 솔루션이 필요합니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 기존 웹 ACL에 포함된 것과 동일한 규칙을 포함하는 새 웹 ACL을 생성합니다. 새 웹 ACL을 ALB와 연결합니다. B. 기존 웹 ACL을 ALB와 연결합니다. C. ALB에 보안 그룹 규칙을 추가하여 CloudFront에 대해서만 AWS 관리형 접두사 목록의 트래픽을 허용합니다. D. 다양한 CloudFront IP 주소 범위만 허용하도록 보안 그룹 규칙을 ALB에 추가합니다. Answer: C Explanation: https://aws.amazon.com/about-aws/whats-new/2022/02/amazon-cloudfront-managed-prefix-list/
QUESTION NO: 212 솔루션 설계자는 회사의 Amazon EC2 인스턴스 및 Amazon Elastic Block Store(Amazon EBS) 볼륨을 분석하여 회사가 리소스를 효율적으로 사용하고 있는지 확인해야 합니다. /passive 구성 이러한 EC2 인스턴스의 활용도는 데이터베이스를 사용하는 애플리케이션에 따라 다르며 회사는 패턴을 식별하지 않았습니다. 솔루션 설계자는 환경을 분석하고 결과에 따라 조치를 취해야 합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까? A. AWS Systems Manager OpsConter를 사용하여 대시보드 생성 EC2 인스턴스 및 해당 EBS 볼륨과 연결된 Amazon CloudWatch 지표에 대한 시각화 구성 대시보드를 주기적으로 검토하고 사용 패턴 식별 지표의 최고점을 기준으로 EC2 인스턴스 크기 조정 B. EC2 인스턴스 및 해당 EBS 볼륨에 대한 Amazon CloudWatch 세부 모니터링을 켭니다. 메트릭을 기반으로 하는 대시보드 생성 및 검토 사용 패턴 식별 FC 크기 조정? 메트릭의 피크를 기반으로 하는 인스턴스 C. 각 EC2 인스턴스에 Amazon CloudWatch 에이전트를 설치합니다. AWS Compute Optimizer를 켜고 최소 12시간 동안 실행합니다. Compute Optimizer의 권장 사항을 검토하고 지시에 따라 EC2 인스턴스의 크기를 조정합니다. D. AWS Enterprise Support 플랜에 가입합니다. AWS Trusted Advisor를 켭니다. 12시간 동안 대기합니다. Trusted Advisor의 권장 사항을 검토하고 지시에 따라 EC2 인스턴스의 크기를 조정합니다. Answer: C Explanation: (https://aws.amazon.com/compute-optimizer/pricing/ , https://aws.amazon.com/systemsmanager/ pricing/ ). https://aws.amazon.com/compute-optimizer/
QUESTION NO: 213 회사에서 여러 Amazon DynamoDB 테이블에 데이터를 저장하고 있습니다. 솔루션 설계자는 서버리스 아키텍처를 사용하여 HTTPS를 통한 간단한 API를 통해 공개적으로 데이터에 액세스할 수 있도록 해야 합니다. 솔루션은 수요에 따라 자동으로 확장되어야 합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까? (2개 선택하세요.) A. Amazon API Gateway REST API를 생성합니다. API Gateway의 AWS 통합 유형을 사용하여 DynamoDB에 대한 직접 통합으로 이 API를 구성합니다. B. Amazon API Gateway HTTP API를 생성합니다. API Gateway의 AWS 통합 유형을 사용하여 Dynamo DB에 대한 직접 통합으로 이 API를 구성합니다. C. Amazon API Gateway HTTP API를 생성합니다. DynamoDB 테이블에서 데이터를 반환하는 AWS Lambda 함수와의 통합으로 이 API를 구성합니다. D. AWS Global Accelerator에서 액셀러레이터를 생성합니다. DynamoDB 테이블에서 데이터를 반환하는 AWS Lambda@Edge 함수 통합으로 이 액셀러레이터를 구성합니다. E. Network Load Balancer를 생성합니다. 요청을 적절한 AWS Lambda 함수로 전달하도록 리스너 규칙 구성 Answer: A,C Explanation: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-overviewdeveloper-experience.html
QUESTION NO: 214 회사에는 단일 AWS 계정이 있는 환경이 있습니다. 솔루션 아키텍트는 AWS Management Console에 대한 액세스와 관련하여 특히 개선할 수 있는 부분을 추천하기 위해 환경을 검토하고 있습니다. 회사의 IT 지원 작업자는 현재 관리 작업을 위해 콘솔에 액세스하여 작업 역할에 매핑된 명명된 IAM 사용자로 인증합니다. IT 지원 작업자는 더 이상 Active Directory와 IAM 사용자 계정을 모두 유지하기를 원하지 않습니다. 기존 Active Directory 자격 증명을 사용하여 콘솔에 액세스할 수 있기를 원합니다. 솔루션 설계자는 AWS Single Sign-On(AWS SSO)을 사용하여 이 기능을 구현합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 솔루션은 무엇입니까? A. AWS Organizations에서 조직을 생성합니다. 조직에서 AWS SSO 기능을 켭니다. 회사의 온프레미스 Active Directory에 대한 양방향 트러스트를 사용하여 AWS Directory Service for Microsoft Active Directory(AWS Managed Microsoft AD)에서 디렉터리를 생성하고 구성합니다. AWS SSO를 구성하고 AWS Managed Microsoft AD 디렉터리를 자격 증명 소스로 설정합니다. 권한 집합을 생성하고 AWS Managed Microsoft AD 디렉터리 내의 기존 그룹에 매핑합니다. B. AWS Organizations에서 조직을 생성합니다. Organizations Create에서 AWS SSO 기능을 켜고 회사의 온프레미스 Active Directory에 연결할 AD Connector를 구성하고 구성합니다. AWS SSO를 구성하고 AD 커넥터를 자격 증명 소스로 선택합니다. 권한 집합을 만들고 회사의 Active Directory 내의 기존 그룹에 매핑합니다. C. AWS Organizations에서 조직을 생성합니다. 조직의 모든 기능을 켭니다. 회사의 온프레미스 Active Directory에 대한 양방향 트러스트를 통해 AWS Directory Service for Microsoft Active Directory(AWS Managed Microsoft AD)에서 디렉터리를 생성하고 구성합니다. AWS SSO를 구성하고 AWS Managed Microsoft AD 디렉터리를 자격 증명 소스로 선택합니다. 권한 집합을 생성하고 AWS Managed Microsoft AD 디렉터리 내의 기존 그룹에 매핑합니다. D. AWS Organizations에서 조직을 생성합니다. 조직의 모든 기능을 켭니다. 회사의 온프레미스 Active Directory에 연결할 AD 커넥터를 만들고 구성합니다. AWS SSO를 구성하고 AD 커넥터를 자격 증명 소스로 선택합니다. 권한 집합을 만들고 회사의 Active Directory 내의 기존 그룹에 매핑합니다. Answer: D Explanation: https://docs.aws.amazon.com/organizations/latest/userguide/orgs_manage_org_support-allfeatures.html https://docs.aws.amazon.com/singlesignon/latest/userguide/get-started-prereqsconsiderations.html
QUESTION NO: 215 회사가 AWS 클라우드에서 애플리케이션을 실행하고 있습니다. 애플리케이션은 Amazon Elastic Container Service(Amazon ECS) 클러스터의 컨테이너에서 실행됩니다. ECS 작업은 Fargate 시작 유형을 사용합니다. 애플리케이션의 데이터는 관계형이며 Amazon Aurora MySQL에 저장됩니다. 규제 요구 사항을 충족하려면 애플리케이션 장애 시 애플리케이션이 별도의 AWS 리전으로 복구할 수 있어야 합니다. 장애가 발생해도 데이터가 손실되지 않습니다. 최소한의 운영 오버헤드로 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 다른 지역에서 Aurora 복제본을 프로비저닝합니다. B. 데이터를 다른 지역으로 지속적으로 복제하도록 AWS DataSync를 설정합니다. C. 다른 리전으로 데이터를 지속적으로 복제하도록 AWS DMS(AWS Database Migration Service)를 설정합니다. D. Amazon Data Lifecycle Manager(Amazon DLM)를 사용하여 5분마다 스냅샷을 예약합니다. Answer: A Explanation: Provision an Aurora Replica in a different Region will meet the requirement of the application being able to recover to a separate AWS Region in the event of an application failure, and no data can be lost, with the least amount of operational overhead.
QUESTION NO: 216 회사는 정적 포트에서 TCP를 사용하여 액세스할 새 서비스를 개발하고 있습니다. 솔루션 설계자는 서비스의 가용성이 높고 가용성 영역 전체에 중복성이 있으며 공개적으로 액세스할 수 있는 DNS 이름 myservice.com을 사용하여 액세스할 수 있는지 확인해야 합니다. 다른 회사에서 허용 목록에 주소를 추가할 수 있도록 서비스에서 고정 주소 할당을 사용해야 합니다. 리소스가 단일 리전의 여러 가용 영역에 배포된다고 가정할 때 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. 각 인스턴스에 대한 탄력적 IP 주소로 Amazon EC2 인스턴스 생성 NLB(Network Load Balancer) 생성 및 정적 TCP 포트 노출 NLB에 EC2 인스턴스 등록 my service com이라는 이름의 새 이름 서버 레코드 세트를 생성하고 할당 레코드 세트에 대한 EC2 인스턴스의 탄력적 IP 주소 허용 목록에 추가할 다른 회사에 EC2 인스턴스의 탄력적 IP 주소 제공 B. 애플리케이션에 대한 Amazon ECS 클러스터 및 서비스 정의 생성 ECS 클러스터에 대한 퍼블릭 IP 주소 생성 및 할당 NLB(Network Load Balancer) 생성 및 TCP 포트 노출 대상 그룹 생성 및 ECS 클러스터 이름 할당 NLB는 my service com이라는 새 A 레코드 세트를 생성하고 레코드 세트에 ECS 클러스터의 공용 IP 주소를 할당합니다. ECS 클러스터의 공용 IP 주소를 다른 회사에 제공하여 허용 목록에 추가합니다. C. 서비스에 대한 Amazon EC2 인스턴스 생성 각 가용 영역에 대해 하나의 탄력적 IP 주소 생성 NLB(Network Load Balancer) 생성 및 할당된 TCP 포트 노출 각 가용 영역에 대한 NLB에 탄력적 IP 주소 할당 대상 그룹 생성 NLB에 EC2 인스턴스를 등록합니다. my service com이라는 새 A(별칭) 레코드 세트를 생성하고 레코드 세트에 NLB DNS 이름을 할당합니다. D. 애플리케이션에 대한 Amazon ECS 클러스터 및 서비스 정의 생성 클러스터의 각 호스트에 대한 퍼블릭 IP 주소 생성 및 할당 ALB(Application Load Balancer) 생성 및 정적 TCP 포트 노출 대상 그룹 생성 및 ECS 할당 ALB에 대한 서비스 정의 이름 새 CNAME 레코드 세트 생성 및 퍼블릭 IP 주소를 레코드 세트에 연결 허용 목록에 추가할 다른 회사에 Amazon EC2 인스턴스의 탄력적 IP 주소 제공 Answer: C Explanation: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/routing-to-elb-loadbalancer.html Create a Network Load Balancer (NLB) and expose the assigned TCP port. Assign the Elastic IP addresses to the NLB for each Availability Zone. Create a target group and register the EC2 instances with the NLB. Create a new A (alias) record set named my.service.com, and assign the NLB DNS name to the record set. As it uses the NLB as the resource in the A-record, traffic will be routed through the NLB, and it will automatically route the traffic to the healthy instances based on the health checks and also it provides the fixed address assignments as the other companies can add the NLB's Elastic IP addresses to their allow lists.
QUESTION NO: 217 회사에서 Amazon S3 버킷에 민감한 데이터를 저장하고 있습니다. 회사는 S3 버킷의 객체에 대한 모든 활동을 기록하고 5년 동안 로그를 유지해야 합니다. 또한 회사의 보안 팀은 S3 버킷에서 데이터를 삭제하려는 시도가 있을 때마다 이메일 알림을 받아야 합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 단계 조합은 무엇입니까? (3개를 선택하세요.) A. S3 데이터 이벤트를 기록하도록 AWS CloudTrail을 구성합니다. B. S3 버킷에 대한 S3 서버 액세스 로깅을 구성합니다. C. 객체 삭제 이벤트를 Amazon Simple Email Service(Amazon SES)로 보내도록 Amazon S3를 구성합니다. D. Amazon Simple Notification Service(Amazon SNS) 주제에 게시하는 Amazon EventBridge 이벤트 버스로 객체 삭제 이벤트를 보내도록 Amazon S3를 구성합니다. E. 데이터 스토리지 계층화를 사용하여 로그를 Amazon Timestream으로 보내도록 Amazon S3를 구성합니다. F. S3 수명 주기 정책으로 로그를 저장하도록 새 S3 버킷을 구성합니다. Answer: A,D,F Explanation: Configuring AWS CloudTrail to log S3 data events will enable logging all activities for objects in the S3 bucket1. Data events are object-level API operations such as GetObject, DeleteObject, and PutObject1. Configuring Amazon S3 to send object deletion events to an Amazon EventBridge event bus that publishes to an Amazon Simple Notification Service (Amazon SNS) topic will enable sending email notifications every time there is an attempt to delete data in the S3 bucket2. EventBridge can route events from S3 to SNS, which can send emails to subscribers2. Configuring a new S3 bucket to store the logs with an S3 Lifecycle policy will enable keeping the logs for 5 years in a cost-effective way3. A lifecycle policy can transition the logs to a cheaper storage class such as Glacier or delete them after a specified period of time3.
QUESTION NO: 218 회사에서 AWS WAF 솔루션을 배포하여 여러 AWS 계정에서 AWS WAF 규칙을 관리하려고 합니다. 계정은 AWS Organizations의 서로 다른 OU에서 관리됩니다. 관리자는 필요에 따라 관리형 AWS WAF 규칙 세트에서 계정 또는 OU를 추가하거나 제거할 수 있어야 합니다. 또한 관리자는 모든 계정에서 비준수 AWS WAF 규칙을 자동으로 업데이트하고 수정할 수 있어야 합니다. 운영 오버헤드를 최소화하면서 이러한 요구 사항을 충족하는 솔루션은 무엇입니까? A. AWS Firewall Manager를 사용하여 조직의 여러 계정에서 AWS WAF 규칙을 관리합니다. AWS Systems Manager Parameter Store 매개변수를 사용하여 관리할 계정 번호 및 OU 저장 필요에 따라 매개변수를 업데이트하여 계정 또는 OU 추가 또는 제거 Amazon EventBridge(Amazon CloudWatch Events) 규칙을 사용하여 매개변수 변경 사항을 식별하고 AWS Firewall Manager 관리 계정에서 보안 정책을 업데이트하는 Lambda 함수 B. 선택한 OU의 모든 리소스가 AWS WAF 규칙을 연결해야 하는 조직 전체 AWS Config 규칙을 배포합니다. 비준수 리소스를 수정하기 위해 AWS Lambda를 사용하여 자동화된 수정 작업을 배포합니다. AWS Config 규칙이 적용되는 동일한 OU를 대상으로 하는 AWS CloudFormation 스택 세트를 사용하여 AWS WAF 규칙을 배포합니다. C. 조직의 마스터 계정에 AWS WAF 규칙 생성 AWS Lambda 환경 변수를 사용하여 계정 번호 및 OU를 관리 관리 계정 또는 OU를 추가 또는 제거하기 위해 필요에 따라 환경 변수 업데이트 회원 계정에서 교차 계정 IAM 역할 생성 가정 Lambda 함수에서 AWS Security Token Service(AWS STS)를 사용하여 구성원 계정에서 AWS WAF 규칙을 생성하고 업데이트합니다. D. AWS Control Tower를 사용하여 조직의 계정 전체에서 AWS WAF 규칙 관리 AWS KMS(AWS Key Management Service)를 사용하여 계정 번호 및 OU를 관리 관리 필요에 따라 AWS KMS를 업데이트하여 계정 또는 OU를 추가 또는 제거 IAM 사용자 생성 회원 계정에서 허용 마스터 계정의 AWS Control Tower가 액세스 키 및 보안 액세스 키를 사용하여 회원 계정에서 AWS WAF 규칙을 생성 및 업데이트하도록 허용 Answer: A Explanation: https://aws.amazon.com/solutions/implementations/automations-for-aws-firewall-manager/ In this solution, AWS Firewall Manager is used to manage AWS WAF rules across accounts in the organization. An AWS Systems Manager Parameter Store parameter is used to store account numbers and OUs to manage. This parameter can be updated as needed to add or remove accounts or OUs. An Amazon EventBridge rule is used to identify any changes to the parameter and to invoke an AWS Lambda function to update the security policy in the Firewall Manager administrative account. This solution allows for easy management of AWS WAF rules across multiple accounts with minimal operational overhead
QUESTION NO: 219 회사에서 Redis용 Amazon ElastiCache 클러스터를 캐싱 계층으로 사용하는 애플리케이션을 실행하고 있습니다. 최근 보안 감사에서 회사가 ElastiCache에 대해 유휴 암호화를 구성한 것으로 나타났습니다. 그러나 회사는 전송 중 암호화를 사용하도록 ElastiCache를 구성하지 않았습니다. 또한 사용자는 액세스할 수 있습니다. 인증 없는 캐시 솔루션 설계자는 사용자 인증을 요구하고 회사가 종단 간 암호화를 사용하도록 변경해야 합니다. 어떤 솔루션이 이러한 요구 사항을 충족합니까? A. AUTH 토큰 생성 토큰을 암호화된 파라미터로 AWS System Manager Parameter Store에 저장 AUTH로 새 클러스터를 생성하고 전송 중 암호화 구성 필요한 경우 Parameter Store에서 AUTH 토큰을 검색하고 인증을 위한 AUTH 토큰 B. AUTH 토큰을 생성합니다. AWS Secrets Manager에 토큰을 저장합니다. AUTH 토큰을 사용하도록 기존 클러스터를 구성하고 전송 중 암호화를 구성합니다. 필요한 경우 Secrets Manager에서 AUTH 토큰을 검색하고 인증에 AUTH 토큰을 사용하도록 애플리케이션을 업데이트합니다. . C. SSL 인증서 생성 AWS Secrets Manager에 인증서 저장 새 클러스터 생성 및 전송 중 암호화 구성 필요할 때 Secrets Manager에서 SSL 인증서를 검색하고 인증에 인증서를 사용하도록 애플리케이션을 업데이트합니다. D. SSL 인증서 생성 암호화된 고급 파라미터로 AWS Systems Manager Parameter Store에 인증서 저장 기존 클러스터를 업데이트하여 전송 중 암호화 구성 필요한 경우 Parameter Store에서 SSL 인증서를 검색하고 인증서를 사용하도록 애플리케이션 업데이트 인증을 위해 Answer: B Explanation: Creating an AUTH token and storing it in AWS Secrets Manager and configuring the existing cluster to use the AUTH token and configure encryption in transit, and updating the application to retrieve the AUTH token from Secrets Manager when necessary and to use the AUTH token for authentication, would meet the requirements for user authentication and endto-end encryption. AWS Secrets Manager is a service that enables you to easily rotate, manage, and retrieve database credentials, API keys, and other secrets throughout their lifecycle. Secrets Manager also enables you to encrypt the data and ensure that only authorized users and applications can access it. By configuring the existing cluster to use the AUTH token and encryption in transit, all data will be encrypted as it is sent over the network, providing additional security for the data stored in ElastiCache. Additionally, by updating the application to retrieve the AUTH token from Secrets Manager when necessary and to use the AUTH token for authentication, it ensures that only authorized users and applications can access the cache. Reference: AWS Secrets Manager documentation: https://aws.amazon.com/secrets-manager/ Encryption in transit for ElastiCache: https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/encryption.html Authentication and Authorization for ElastiCache: https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/accessing-elasticache.html
QUESTION NO: 220 회사에서 고객이 온라인 주문에 사용하는 애플리케이션을 업데이트하고 있습니다. 최근 악성 행위자의 애플리케이션 공격 횟수가 증가했습니다. 회사는 Amazon Elastic Container Service(Amazon ECS) 클러스터에서 업데이트된 애플리케이션을 호스팅합니다. 회사는 Amazon DynamoDB를 사용하여 애플리케이션 데이터를 저장합니다. 공용 Application Load Balancer(ALB)는 최종 사용자에게 애플리케이션에 대한 액세스 권한을 제공합니다. 회사는 공격을 예방하고 공격이 진행되는 동안 최소한의 서비스 중단으로 비즈니스 연속성을 보장해야 합니다. 이러한 요구 사항을 가장 비용 효율적으로 충족하는 단계 조합은 무엇입니까? (2개를 선택하세요.) A. ALB를 오리진으로 사용하여 Amazon CloudFront 배포를 생성합니다. CloudFront 도메인에 사용자 지정 헤더와 임의 값을 추가합니다. 헤더와 값이 일치하는 경우 트래픽을 조건부로 전달하도록 ALB를 구성합니다. B. 두 AWS 지역에 애플리케이션을 배포합니다. 동일한 가중치로 두 리전으로 라우팅하도록 Amazon Route 53을 구성합니다. C. Amazon ECS 작업에 대한 자동 조정을 구성합니다. DynamoDB Accelerator(DAX) 클러스터를 생성합니다. D. DynamoDB의 오버헤드를 줄이도록 Amazon ElastiCache를 구성합니다. E. 적절한 규칙 그룹을 포함하는 AWS WAF 웹 ACL을 배포합니다. 웹 ACL을 Amazon CloudFront 배포와 연결합니다. Answer: A,E Explanation: The company should create an Amazon CloudFront distribution with the ALB as the origin. The company should add a custom header and random value on the CloudFront domain. The company should configure the ALB to conditionally forward traffic if the header and value match. The company should also deploy an AWS WAF web ACL that includes an appropriate rule group. The company should associate the web ACL with the Amazon CloudFront distribution. This solution will meet the requirements most cost-effectively because Amazon CloudFront is a fast content delivery network (CDN) service that securely delivers data, videos, applications, and APIs to customers globally with low latency, high transfer speeds, all within a developer-friendly environment1. By creating an Amazon CloudFront distribution with the ALB as the origin, the company can improve the performance and availability of its application by caching static content at edge locations closer to end users. By adding a custom header and random value on the CloudFront domain, the company can prevent direct access to the ALB and ensure that only requests from CloudFront are forwarded to the ECS tasks. By configuring the ALB to conditionally forward traffic if the header and value match, the company can implement origin access identity (OAI) for its ALB origin. OAI is a feature that enables you to restrict access to your content by requiring users to access your content through CloudFront URLs2. By deploying an AWS WAF web ACL that includes an appropriate rule group, the company can prevent attacks and ensure business continuity with minimal service interruptions during an ongoing attack. AWS WAF is a web application firewall that lets you monitor and control web requests that are forwarded to your web applications. You can use AWS WAF to define customizable web security rules that control which traffic can access your web applications and which traffic should be blocked3. By associating the web ACL with the Amazon CloudFront distribution, the company can apply the web security rules to all requests that are forwarded by CloudFront. The other options are not correct because: Deploying the application in two AWS Regions and configuring Amazon Route 53 to route to both Regions with equal weight would not prevent attacks or ensure business continuity. Amazon Route 53 is a highly available and scalable cloud Domain Name System (DNS) web service that routes end users to Internet applications by translating names like www.example.com into numeric IP addresses4. However, routing traffic to multiple Regions would not protect against attacks or provide failover in case of an outage. It would also increase operational complexity and costs compared to using CloudFront and AWS WAF. Configuring auto scaling for Amazon ECS tasks and creating a DynamoDB Accelerator (DAX) cluster would not prevent attacks or ensure business continuity. Auto scaling is a feature that enables you to automatically adjust your ECS tasks based on demand or a schedule. DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that delivers up to a 10x performance improvement. However, these features would not protect against attacks or provide failover in case of an outage. They would also increase operational complexity and costs compared to using CloudFront and AWS WAF. Configuring Amazon ElastiCache to reduce overhead on DynamoDB would not prevent attacks or ensure business continuity. Amazon ElastiCache is a fully managed in-memory data store service that makes it easy to deploy, operate, and scale popular open-source compatible in-memory data stores. However, this service would not protect against attacks or provide failover in case of an outage. It would also increase operational complexity and costs compared to using CloudFront and AWS WAF. Reference: https://aws.amazon.com/cloudfront/ https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/private-contentrestricting-access-to-s3.html https://aws.amazon.com/waf/ https://aws.amazon.com/route53/ https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html https://aws.amazon.com/dynamodb/dax/ https://aws.amazon.com/elasticache/
QUESTION NO: 221 비디오 처리 회사에는 Amazon S3 버킷에서 이미지를 다운로드하고, 이미지를 처리하고, 변환된 이미지를 두 번째 S3 버킷에 저장하고, Amazon DynamoDB 테이블에서 이미지에 대한 메타데이터를 업데이트하는 애플리케이션이 있습니다. 애플리케이션은 Node.js로 작성되고 AWS Lambda 함수를 사용하여 실행됩니다. 새 이미지가 Amazon S3에 업로드되면 Lambda 함수가 호출됩니다. 애플리케이션은 한동안 아무 문제 없이 실행되었습니다. 그러나 이미지의 크기가 크게 커졌습니다. 이제 Lambda 함수가 시간 초과 오류로 인해 자주 실패합니다. 기능 제한 시간은 최대값으로 설정됩니다. 솔루션 설계자는 호출 실패를 방지하기 위해 애플리케이션의 아키텍처를 리팩터링해야 합니다. 회사는 기본 인프라를 관리하기를 원하지 않습니다. 솔루션 설계자는 이러한 요구 사항을 충족하기 위해 어떤 단계 조합을 수행해야 합니까? (2개 선택하세요.) A. 애플리케이션 코드가 포함된 Docker 이미지를 빌드하여 애플리케이션 배포를 수정합니다. 이미지를 Amazon Elastic Container Registry(Amazon ECR)에 게시합니다. B. 호환 유형이 AWS Fargate인 새 Amazon Elastic Container Service(Amazon ECS) 작업 정의를 생성합니다. Amazon Elastic Container Registry(Amazon ECR)에서 새 이미지를 사용하도록 작업 정의를 구성합니다. 새 파일이 Amazon S3에 도착하면 ECS 작업 정의를 사용하여 ECS 작업을 호출하도록 Lambda 함수를 조정합니다. C. Parallel 상태로 AWS Step Functions 상태 시스템을 생성하여 Lambda 함수를 호출합니다. Lambda 함수의 프로비저닝된 동시성을 높입니다. D. 호환성 유형이 Amazon EC2인 새 Amazon Elastic Container Service(Amazon ECS) 작업 정의를 생성합니다. Amazon Elastic Container Registry(Amazon ECR)에서 새 이미지를 사용하도록 작업 정의를 구성합니다. 새 파일이 Amazon S3에 도착하면 ECS 작업 정의를 사용하여 ECS 작업을 호출하도록 Lambda 함수를 조정합니다. E. Amazon Elastic File System(Amazon EFS)에 이미지를 저장하고 Amazon RDS DB 인스턴스에 메타데이터를 저장하도록 애플리케이션을 수정합니다. Lambda 함수를 조정하여 EFS 파일 공유를 탑재합니다. Answer: A,B Explanation: 1. Modify the application deployment by building a Docker image that contains the application code. Publish the image to Amazon Elastic Container Registry (Amazon ECR). - This step is necessary to package the application code in a container and make it available for running on ECS. B. Create a new Amazon Elastic Container Service (Amazon ECS) task definition with a compatibility type of AWS Fargate. Configure the task definition to use the new image in Amazon Elastic Container Registry (Amazon ECR). Adjust the Lambda function to invoke an ECS task by using the ECS task definition when a new file arrives in Amazon S3. 